{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import io\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import csv\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample notebook from ECG deep learning project",
    "\n",
    "from sklearn.model_selection import GroupKFold, KFold\n",
    "\n",
    "import tensorflow as tf  \n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Flatten, Reshape\n",
    "from tensorflow.keras.layers import Dropout, Lambda\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, Conv2DTranspose,Convolution2D, Conv3D, ConvLSTM2D, Bidirectional\n",
    "from tensorflow.keras.layers import ConvLSTM2D\n",
    "from tensorflow.keras.layers import MaxPooling1D, MaxPooling2D, MaxPooling3D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from tensorflow.keras.layers import Activation, SpatialDropout1D, SpatialDropout2D, LayerNormalization, GlobalAvgPool1D, GlobalMaxPool1D\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow_addons.layers import WeightNormalization\n",
    "from tensorflow_addons.optimizers import AdamW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening numpy arrays for sex    \n",
    "suffix = 'corrected_60k_sex'           #non-null- 56,665 ECGs; will use 53,665 in training set\n",
    "\n",
    "ecg_X_k = np.load('MGH_60k_500hz_ecgs_age_18_100_trainval_X.npy')\n",
    "ecg_y_k = np.load('MGH_60k_500hz_ecgs_age_18_100_trainval_yfemale.npy')\n",
    "\n",
    "ecg_X_k = ecg_X_k[ecg_y_k>=0]\n",
    "ecg_y_k = ecg_y_k[ecg_y_k>=0]\n",
    "\n",
    "print(ecg_X_k.shape)\n",
    "print(ecg_y_k.shape)\n",
    "\n",
    "np.unique(ecg_y_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening numpy arrays for sex test set\n",
    "\n",
    "ecg_X_k = np.load('MGH_60k_500hz_ecgs_age_18_100_test_X.npy')\n",
    "ecg_y_k = np.load('MGH_60k_500hz_ecgs_age_18_100_test_y.npy')\n",
    "\n",
    "ecg_X_k = ecg_X_k[ecg_y_k>=0]\n",
    "ecg_y_k = ecg_y_k[ecg_y_k>=0]\n",
    "\n",
    "print(ecg_X_k.shape)\n",
    "print(ecg_y_k.shape)\n",
    "\n",
    "np.unique(ecg_y_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening numpy arrays for age    #57,000; 54,150 in training with 20-fold\n",
    "suffix = 'corrected_60k_age'\n",
    "\n",
    "ecg_X_k = np.load('MGH_60k_500hz_ecgs_age_18_100_trainval_X.npy')\n",
    "ecg_y_k = np.load('MGH_60k_500hz_ecgs_age_18_100_trainval_yage.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_X_k = ecg_X_k[ecg_y_k>0]\n",
    "ecg_y_k = ecg_y_k[ecg_y_k>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ecg_X_k.shape)\n",
    "print(ecg_y_k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting only X and y elements in 60k dataset where rhythm is sinus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainval_sinus = np.load('MGH_60k_500hz_ecgs_age_18_100_trainval_sinus.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_filename_k = np.load('MGH_60k_500hz_ecgs_age_18_100_trainval_filename.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_metadata = []\n",
    "\n",
    "for file in ecg_filename_k:\n",
    "    \n",
    "    \n",
    "    with open('/mnt/obi0/phi/ecg/convertedData/MGH/' + file[0:5] + '/' + file + '.dict', 'rb') as f:\n",
    "        ecg_dict = pickle.load(f)\n",
    "\n",
    "    try:\n",
    "        ecg_dict['metadata']['StmtText'] = ' '.join([str(elem) for elem in ecg_dict['metadata']['StmtText']])\n",
    "        ecg_dict['metadata']['Filename'] = file\n",
    "        ecg_metadata.append(ecg_dict['metadata'])\n",
    "        \n",
    "    except:\n",
    "        ecg_metadata.append({'Filename':file})\n",
    "\n",
    "meta_df = pd.DataFrame.from_dict(ecg_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = meta_df[ecg_y_k>0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = meta_df[trainval_sinus==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_X_k = ecg_X_k[trainval_sinus==True]\n",
    "ecg_y_k = ecg_y_k[trainval_sinus==True]\n",
    "\n",
    "print(ecg_X_k.shape)          #27,215 in trainval set; 15 folds used -> 25,400 for training set\n",
    "print(ecg_y_k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df[['VentricularRate', 'AtrialRate', 'PRInterval', 'QRSDuration', 'QTInterval', 'QTCorrected', 'PAxis', 'RAxis', 'TAxis', 'QRSCount', 'QOnset', 'QOffset', 'POnset', 'POffset', 'TOffset']] = meta_df[['VentricularRate', 'AtrialRate', 'PRInterval', 'QRSDuration', 'QTInterval', 'QTCorrected', 'PAxis', 'RAxis', 'TAxis', 'QRSCount', 'QOnset', 'QOffset', 'POnset', 'POffset', 'TOffset']].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df[['QOnset', 'QOffset', 'POnset', 'POffset', 'TOffset']] = meta_df[['QOnset', 'QOffset', 'POnset', 'POffset', 'TOffset']] *2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df.iloc[0:5, 10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df.isna().sum()    #some missing atrial rate (52), p axis (182), qrs count (52), p onset (125), p offset (125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df_nonull = meta_df[pd.notnull(meta_df['POnset'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df_nonull.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_X_k = ecg_X_k[pd.notnull(meta_df['POnset'])]   #27,090, will use 24,090 for training\n",
    "ecg_y_k = ecg_y_k[pd.notnull(meta_df['POnset'])]\n",
    "\n",
    "print(ecg_X_k.shape)         \n",
    "print(ecg_y_k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df_set = meta_df_nonull[['VentricularRate', 'PRInterval', 'QRSDuration', 'QTInterval', 'QTCorrected', 'RAxis', 'TAxis', 'QOnset', 'QOffset', 'TOffset']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df_set.isna().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get only lead I\n",
    "ecg_X_k = ecg_X_k[:, 0:5000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get 2D array for 12-lead ecgs\n",
    "ecg_X_k = ecg_X_k.reshape(ecg_X_k.shape[0], 12, int(ecg_X_k.shape[1]/12), ecg_X_k.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_X_k = ecg_X_k.reshape(ecg_X_k.shape[0], ecg_X_k.shape[1])   #removes last dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_X_rescaled = np.zeros((ecg_X_k.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ecgnum in range(ecg_X_k.shape[0]):            #normalizes with mean 0 and max 1\n",
    "    ecg_X_rescaled[ecgnum] = ecg_X_k[ecgnum]-np.mean(ecg_X_k[ecgnum])\n",
    "    ecg_X_rescaled[ecgnum] = ecg_X_rescaled[ecgnum]/np.max(ecg_X_rescaled[ecgnum])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipped1 = []\n",
    "skipped2 = []\n",
    "skipped3 = []\n",
    "ecg_X_full_intervals = np.zeros((0, 6))\n",
    "ecg_X_intervals = np.zeros((0, 6))\n",
    "\n",
    "for ecg_num in range(ecg_X_rescaled.shape[0]):\n",
    "    peaks, _ = find_peaks(ecg_X_rescaled[ecg_num], distance = 150, prominence=0.6)\n",
    "    \n",
    "    if (len(peaks)>=7):\n",
    "        if (peaks[6]-peaks[0]>= 1500) & (peaks[6]-peaks[0]<=4500):  #selects mean HR 40-120 for the 7 beats\n",
    "        \n",
    "            #finds R-R intervals, as the percentage of the 7 beat group for each R-R interval\n",
    "            rr_ints = [(t - s)/(peaks[6]-peaks[0]) for s, t in zip(peaks[0:6], peaks[1:7])] \n",
    "            rr_full = [(t - s) for s, t in zip(peaks[0:6], peaks[1:7])] \n",
    "        \n",
    "            ecg_X_full_intervals = np.concatenate((ecg_X_full_intervals, np.array([rr_full])))\n",
    "            ecg_X_intervals = np.concatenate((ecg_X_intervals, np.array([rr_ints])))\n",
    "        else: \n",
    "            if (peaks[6]-peaks[0]< 1500):\n",
    "                skipped1.append(ecg_num)\n",
    "            if (peaks[6]-peaks[0]>4500):\n",
    "                skipped2.append(ecg_num)\n",
    "    else:\n",
    "        skipped3.append(ecg_num)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(skipped1)       # 614 skipped for detected mean HR for first 7 beats >120 (oversensing t-wave?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(skipped2)       # 11 skipped for detected mean HR for first 7 beats <40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(skipped3)       # 284 skipped for not having at least 7 beats detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipped = skipped1 + skipped2 + skipped3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(skipped)        # 909"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_X_intervals.shape   #26,306 kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_X_full_intervals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_X_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_X_full_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.ones_like(ecg_y_k, bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[skipped]=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_y_intervals = ecg_y_k[mask]       #26,306 in y numpy array\n",
    "print(ecg_y_intervals.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(ecg_X_intervals)  #0.167"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(ecg_X_intervals)   #0.509"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(ecg_X_intervals)   #0.042"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_X_intervals_all = ecg_X_intervals.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ecg_X_intervals_all)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#will set min and max R-R intervals to avoid PACs and PVCs\n",
    "#mean R-R: 0.1667\n",
    "#max- 0.200 : 20% increase\n",
    "#min- 0.133 : 20% decrease\n",
    "#difference between min and max- 50%\n",
    "ecg_y_intervals = ecg_y_intervals[(np.min(ecg_X_intervals, axis=1)>=0.133) & (np.max(ecg_X_intervals, axis=1)<=0.2)]\n",
    "ecg_X_intervals = ecg_X_intervals[(np.min(ecg_X_intervals, axis=1)>=0.133) & (np.max(ecg_X_intervals, axis=1)<=0.2),:]\n",
    "\n",
    "print(ecg_X_intervals.shape)    #23,509 of 26,306 remaining; will use 2,000 for val set\n",
    "print(ecg_y_intervals.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_X_full_intervals = ecg_X_full_intervals[(np.min(ecg_X_intervals, axis=1)>=0.133) & (np.max(ecg_X_intervals, axis=1)<=0.2),:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_X_full_intervals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full_list = [0]\n",
    "X_train_full_list[0] = ecg_X_full_intervals[0:21509]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternative to k-fold \n",
    "\n",
    "X_train_list = [0]\n",
    "y_train_list = [0]\n",
    "X_val_list = [0]\n",
    "y_val_list = [0]\n",
    "\n",
    "X_train_list[0] = ecg_X_intervals[0:24090]\n",
    "y_train_list[0] = ecg_y_intervals[0:24090]\n",
    "\n",
    "X_val_list[0] = ecg_X_intervals[24090:]\n",
    "y_val_list[0] = ecg_y_intervals[24090:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to visualize ECGs\n",
    "ecg_num= 108\n",
    "\n",
    "peaks, _ = find_peaks(ecg_X_rescaled[ecg_num], distance = 150, prominence=0.6) \n",
    "\n",
    "plt.figure(figsize=(14,10))\n",
    "plt.plot(ecg_X_rescaled[ecg_num])\n",
    "plt.plot(peaks, ecg_X_rescaled[ecg_num][peaks], 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list = [0]\n",
    "y_train_list = [0]\n",
    "X_val_list = [0]\n",
    "y_val_list = [0]\n",
    "\n",
    "X_train_list[0] = ecg_X_k[0:21509]\n",
    "y_train_list[0] = ecg_y_k[0:21509]\n",
    "\n",
    "X_val_list[0] = ecg_X_k[21509:]\n",
    "y_val_list[0] = ecg_y_k[21509:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_list = [0]\n",
    "y_train_list = [0]\n",
    "X_val_list = [0]\n",
    "y_val_list = [0]\n",
    "meta_df_train_list = [0]\n",
    "meta_df_val_list = [0]\n",
    "\n",
    "X_train_list[0] = ecg_X_k[0:24090]\n",
    "y_train_list[0] = ecg_y_k[0:24090]\n",
    "meta_df_train_list[0] = meta_df_set[0:24090]\n",
    "\n",
    "X_val_list[0] = ecg_X_k[24090:]\n",
    "y_val_list[0] = ecg_y_k[24090:]\n",
    "meta_df_val_list[0] = meta_df_set[24090:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list[0] = X_train_list[0][:, 0:5000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_list[0] = X_val_list[0][:, 0:5000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_val_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if reshaping needed\n",
    "X_train_list[0] = X_train_list[0].reshape(X_train_list[0].shape[0], 12, 5000, 1)\n",
    "X_val_list[0] = X_val_list[0].reshape(X_val_list[0].shape[0], 12, 5000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_list[0][0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(X_val_list[0][45][0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://www.jeremyjordan.me/nn-learning-rate/\n",
    "\n",
    "class LRFinder(keras.callbacks.Callback):\n",
    "    \n",
    "    '''\n",
    "    A simple callback for finding the optimal learning rate range for your model + dataset. \n",
    "    \n",
    "    # Usage\n",
    "        ```python\n",
    "            lr_finder = LRFinder(min_lr=1e-5, \n",
    "                                 max_lr=1e-2, \n",
    "                                 steps_per_epoch=np.ceil(epoch_size/batch_size), \n",
    "                                 epochs=3)\n",
    "            model.fit(X_train, Y_train, callbacks=[lr_finder])\n",
    "            \n",
    "            lr_finder.plot_loss()\n",
    "        ```\n",
    "    \n",
    "    # Arguments\n",
    "        min_lr: The lower bound of the learning rate range for the experiment.\n",
    "        max_lr: The upper bound of the learning rate range for the experiment.\n",
    "        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n",
    "        epochs: Number of epochs to run experiment. Usually between 2 and 4 epochs is sufficient. \n",
    "        \n",
    "    # References\n",
    "        Blog post: jeremyjordan.me/nn-learning-rate\n",
    "        Original paper: https://arxiv.org/abs/1506.01186\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, min_lr=1e-5, max_lr=1e-2, steps_per_epoch=None, epochs=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.total_iterations = steps_per_epoch * epochs\n",
    "        self.iteration = 0\n",
    "        self.history = {}\n",
    "        \n",
    "    def clr(self):\n",
    "        '''Calculate the learning rate.'''\n",
    "        x = self.iteration / self.total_iterations \n",
    "        return self.min_lr + (self.max_lr-self.min_lr) * x\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
    "        logs = logs or {}\n",
    "        K.set_value(self.model.optimizer.lr, self.min_lr)\n",
    "        \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        '''Record previous batch statistics and update the learning rate.'''\n",
    "        logs = logs or {}\n",
    "        self.iteration += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.iteration)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "            \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    " \n",
    "    def plot_lr(self):\n",
    "        '''Helper function to quickly inspect the learning rate schedule.'''\n",
    "        plt.plot(self.history['iterations'], self.history['lr'])\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Learning rate')\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_loss(self):\n",
    "        '''Helper function to quickly observe the learning rate experiment results.'''\n",
    "        plt.plot(self.history['lr'], self.history['loss'])\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel('Learning rate')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://github.com/psklight/keras_one_cycle_clr/blob/master/keras_one_cycle_clr/one_cycle.py\n",
    "\n",
    "class OneCycle(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    A callback class for one-cycle policy training.\n",
    "    :param lr_range: a tuple of starting (usually minimum) lr value and maximum (peak) lr value.\n",
    "    :param momentum_range: a tuple of momentum values.\n",
    "    :param phase_one_fraction: a fraction for phase I (increasing lr) in one cycle. Must between 0 to 1.\n",
    "    :param reset_on_train_begin: True or False to reset counters when training begins.\n",
    "    :param record_frq: integer > 0, a frequency in batches to record training loss.\n",
    "    :param verbose: True or False to print progress.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            lr_range,\n",
    "            momentum_range=None,\n",
    "            phase_one_fraction=0.3,\n",
    "            reset_on_train_begin=True,\n",
    "            record_frq=10,\n",
    "            verbose=False):\n",
    "\n",
    "        super(OneCycle, self).__init__()\n",
    "\n",
    "        self.lr_range = lr_range\n",
    "\n",
    "        self.momentum_range = momentum_range\n",
    "        if momentum_range is not None:\n",
    "            err_msg = \"momentum_range must be a 2-numeric tuple (m1, m2).\"\n",
    "            if not isinstance(momentum_range, (tuple,)) or len(momentum_range) != 2:\n",
    "                raise ValueError(err_msg)\n",
    "\n",
    "        self.phase_one_fraction = phase_one_fraction\n",
    "        self.reset_on_train_begin = reset_on_train_begin\n",
    "        self.record_frq = record_frq\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # helper tracker\n",
    "        self.log = {}  # history in iterations\n",
    "        self.log_ep = {}  # history in epochs\n",
    "        self.stop_training = False\n",
    "\n",
    "        # counter\n",
    "        self.current_iter = 0\n",
    "\n",
    "    def get_current_lr(self, n_iter=None):\n",
    "        \"\"\"\n",
    "        A helper function to calculate a current learning rate based on current iteration number.\n",
    "        :return lr: a current learning rate.\n",
    "        \"\"\"\n",
    "        if n_iter is None:\n",
    "            n_iter = self.n_iter\n",
    "\n",
    "        x = float(self.current_iter) / n_iter\n",
    "        if x < self.phase_one_fraction:\n",
    "            amp = self.lr_range[1] - self.lr_range[0]\n",
    "            lr = (np.cos(x * np.pi/self.phase_one_fraction - np.pi) + 1) * amp / 2.0 + self.lr_range[0]\n",
    "        if x >= self.phase_one_fraction:\n",
    "            amp = self.lr_range[1]\n",
    "            lr = (np.cos((x - self.phase_one_fraction) * np.pi/ (1-self.phase_one_fraction)) + 1) / 2.0 * amp\n",
    "        return lr\n",
    "\n",
    "    def get_current_momentum(self, n_iter=None):\n",
    "        \"\"\"\n",
    "        A helper function to calculate a current momentum based on current iteration number.\n",
    "        :return momentum: a current momentum.\n",
    "        \"\"\"\n",
    "        if n_iter is None:\n",
    "            n_iter = self.n_iter\n",
    "        amp = self.momentum_range[1] - self.momentum_range[0]\n",
    "        # delta = (1 - np.abs(np.mod(self.current_iter, n_iter) * 2.0 / n_iter - 1)) * amplitude\n",
    "        x = float(self.current_iter) / n_iter\n",
    "        if x < self.phase_one_fraction:\n",
    "            delta = (np.cos(x * np.pi / self.phase_one_fraction - np.pi) + 1) * amp / 2.0\n",
    "        if x >= self.phase_one_fraction:\n",
    "            delta = (np.cos((x - self.phase_one_fraction) * np.pi / (1 - self.phase_one_fraction)) + 1) / 2.0 * amp\n",
    "        return delta + self.momentum_range[0]\n",
    "\n",
    "\n",
    "    @property\n",
    "    def cycle_momentum(self):\n",
    "        return self.momentum_range is not None\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.n_epoch = self.params['epochs']\n",
    "\n",
    "        # find number of batches per epoch\n",
    "        if self.params['batch_size'] is not None:  # model.fit\n",
    "            self.n_bpe = int(np.ceil(self.params['samples'] / self.params['batch_size']))\n",
    "        if self.params['batch_size'] is None:  # model.fit_generator\n",
    "            self.n_bpe = self.params['samples']\n",
    "\n",
    "        self.n_iter = self.n_epoch * self.n_bpe\n",
    "        # this is a number of iteration in one cycle\n",
    "\n",
    "        self.current_iter = 0\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs={}):\n",
    "        set_lr(self.model.optimizer, self.get_current_lr())\n",
    "        if self.cycle_momentum:\n",
    "            set_momentum(self.model.optimizer, self.get_current_momentum())\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs={}):\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"lr={:.2e}\".format(self.get_current_lr()), \",\", \"m={:.2e}\".format(self.get_current_momentum()))\n",
    "\n",
    "        # record according to record_frq\n",
    "        if np.mod(int(self.current_iter), self.record_frq) == 0:\n",
    "            self.log.setdefault('lr', []).append(self.get_current_lr())\n",
    "            if self.cycle_momentum:\n",
    "                self.log.setdefault('momentum', []).append(self.get_current_momentum())\n",
    "\n",
    "            for k, v in logs.items():\n",
    "                self.log.setdefault(k, []).append(v)\n",
    "\n",
    "            self.log.setdefault('iter', []).append(self.current_iter)\n",
    "\n",
    "        # update current iteration\n",
    "        self.current_iter += 1\n",
    "\n",
    "        # consider termination\n",
    "        if self.current_iter == self.n_iter:\n",
    "            self.stop_training = True\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.log_ep.setdefault('epoch', []).append(epoch)\n",
    "        self.log_ep.setdefault('lr', []).append(\n",
    "            K.get_value(self.model.optimizer.lr))\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.log_ep.setdefault(k, []).append(v)\n",
    "\n",
    "    def test_run(self, n_iter=None):\n",
    "        \"\"\"\n",
    "        Visualize values of learning rate (and momentum) as a function of iteration (batch).\n",
    "        :param n_iter: a number of cycles. If None, 1000 is used.\n",
    "        \"\"\"\n",
    "\n",
    "        if hasattr(self, 'current_iter'):\n",
    "            original_it = self.current_iter\n",
    "\n",
    "        if n_iter is None:\n",
    "            if hasattr(self, 'n_iter'):\n",
    "                n_iter = self.n_iter\n",
    "            else:\n",
    "                n_iter = 1000\n",
    "        n_iter = int(n_iter)\n",
    "\n",
    "        lrs = np.zeros(shape=(n_iter,))\n",
    "        if self.momentum_range is not None:\n",
    "            moms = np.zeros_like(lrs)\n",
    "\n",
    "        for i in range(int(n_iter)):\n",
    "            self.current_iter = i\n",
    "            lrs[i] = self.get_current_lr(n_iter)\n",
    "            if self.cycle_momentum:\n",
    "                moms[i] = self.get_current_momentum(n_iter)\n",
    "        if not self.cycle_momentum:\n",
    "            plt.plot(lrs)\n",
    "            plt.xlabel('iterations')\n",
    "            plt.ylabel('lr')\n",
    "        else:\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(lrs)\n",
    "            plt.xlabel('iterations')\n",
    "            plt.ylabel('lr')\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(moms)\n",
    "            plt.xlabel('iterations')\n",
    "            plt.ylabel('momentum')\n",
    "\n",
    "        if hasattr(self, 'current_iter'):\n",
    "            self.current_iter = original_it\n",
    "\n",
    "def set_momentum(optimizer, mom_val):\n",
    "    \"\"\"\n",
    "    Helper to set momentum of Keras optimizers.\n",
    "    :param optimizer: Keras optimizer\n",
    "    :param mom_val: value of momentum.\n",
    "    \"\"\"\n",
    "    keys = dir(optimizer)\n",
    "    if \"momentum\" in keys:\n",
    "        K.set_value(optimizer.momentum, mom_val)\n",
    "    if \"rho\" in keys:\n",
    "        K.set_value(optimizer.rho, mom_val)\n",
    "    if \"beta_1\" in keys:\n",
    "        K.set_value(optimizer.beta_1, mom_val)\n",
    "\n",
    "\n",
    "def set_lr(optimizer, lr):\n",
    "    \"\"\"\n",
    "    Helper to set learning rate of Keras optimizers.\n",
    "    :param optimizer: Keras optimizer\n",
    "    :param lr: value of learning rate.\n",
    "    \"\"\"\n",
    "    K.set_value(optimizer.lr, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list[0] = ecg_X_k[0:24090]\n",
    "y_train_list[0] = ecg_y_k[0:24090]\n",
    "meta_df_train_list[0] = meta_df_set[0:24090]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df_train_list[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df_train = meta_df_train_list[0].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df_val = meta_df_val_list[0].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression (binary classification separate)\n",
    "#suffix = 'sex_8k'\n",
    "#suffix = 'pr_12lead_2d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suffix = 'corrected_60k_age_12lead'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "filename_base = 'trained_' + suffix + '_1'\n",
    "print(filename_base)\n",
    "\n",
    "in_neurons=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12-lead model\n",
    "inputs = Input((12, in_neurons, 1))\n",
    "\n",
    "c1 = Conv2D(16,(1,4), padding='same', strides=(1,2), use_bias = False, kernel_initializer= 'he_normal') (inputs)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Activation('relu') (c1)\n",
    "c1 = Dropout(0.1) (c1)\n",
    "\n",
    "c1 = Conv2D(32,(1,8), padding='same', strides=(1,2), use_bias = False, kernel_initializer= 'he_normal') (c1)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Activation('relu') (c1)\n",
    "c1 = Dropout(0.1) (c1)\n",
    "\n",
    "s1 = Conv2D(16, (1,8), padding='same', use_bias=False, kernel_initializer= 'he_normal') (c1)\n",
    "s1 = BatchNormalization() (s1)\n",
    "\n",
    "c2 = WeightNormalization(Conv2D(32,(1,8), padding='same', dilation_rate=(1,2), kernel_initializer= 'he_normal')) (c1)\n",
    "c2 = Activation('relu') (c2)\n",
    "c2 = SpatialDropout2D(0.1) (c2)\n",
    "c2 = WeightNormalization(Conv2D(32,(1,8), padding='same', dilation_rate=(1,4), kernel_initializer= 'he_normal')) (c2)\n",
    "#c2 = Activation('relu') (c2)\n",
    "c2 = Activation('relu') (c1+c2)\n",
    "c2 = SpatialDropout2D(0.1) (c2)\n",
    "#c2 = MaxPooling1D(pool_size=1, strides=4) (c2)\n",
    "\n",
    "c3 = WeightNormalization(Conv2D(16,(1,8), padding='same', dilation_rate=(1,8), kernel_initializer= 'he_normal')) (c2)\n",
    "c3 = Activation('relu') (c3)\n",
    "c3 = SpatialDropout2D(0.1) (c3)\n",
    "c3 = WeightNormalization(Conv2D(16,(1,8), padding='same', dilation_rate=(1,16), kernel_initializer= 'he_normal')) (c3)\n",
    "#c3 = Activation('relu') (c3)\n",
    "c3 = Activation('relu') (s1+c3)\n",
    "c3 = SpatialDropout2D(0.1) (c3)\n",
    "c3 = MaxPooling2D(pool_size=1, strides=(1,8)) (c3)\n",
    "\n",
    "c7 = Conv2D(8, (1,3), use_bias = False, kernel_initializer= 'he_normal') (c3)\n",
    "c7 = BatchNormalization() (c7)\n",
    "c7 = Activation('relu') (c7)\n",
    "c7 = Dropout(0.3) (c7)\n",
    "\n",
    "c7 = Conv2D(16, (12,1), use_bias=False, kernel_initializer='he_normal') (c7)\n",
    "c7 = BatchNormalization() (c7)\n",
    "c7 = Activation('relu') (c7)\n",
    "c7 = Dropout(0.3) (c7)\n",
    "\n",
    "final = Flatten() (c7)\n",
    "\n",
    "outputs = Dense(1) (final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changed to Adam, changed batch size\n",
    "\n",
    "set_loss = 'mean_squared_error'\n",
    "set_metrics = ['mean_squared_error'] \n",
    "\n",
    "num_batch_size=512   #180\n",
    "num_epochs=100\n",
    "num_patience=30\n",
    "stopping_min_epochs=80\n",
    "weight_d = 0 #0.000001  \n",
    "\n",
    "#adamw = AdamW(weight_decay=weight_d)\n",
    "adam = optimizers.Adam()\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[outputs])\n",
    "model.compile(optimizer= adam, loss = set_loss, metrics= set_metrics)\n",
    "model.save_weights('temp4.h5')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ocp = OneCycle(lr_range=(0.0007, 0.007), momentum_range=(0.94,0.85))\n",
    "#ocp.test_run(128)\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "#y_train_list_scaled = y_train_list[i]\n",
    "#y_val_list_scaled = y_val_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removed age scaling\n",
    "\n",
    "#X_train_list_scaled = np.multiply(np.sign(X_train_list[0]), np.log(np.abs(X_train_list[0])+1)) #log-modulus transform- https://blogs.sas.com/content/iml/2014/07/14/log-transformation-of-pos-neg.html\n",
    "#X_val_list_scaled = np.multiply(np.sign(X_val_list[0]), np.log(np.abs(X_val_list[0])+1))\n",
    "\n",
    "#12-lead- too large\n",
    "#X_train_list_scaled_a = np.divide(X_train_list[i][0:27000],4000)\n",
    "#X_train_list_scaled_b = np.divide(X_train_list[i][27000:],4000)\n",
    "#X_train_list_scaled = np.vstack((X_train_list_scaled_a, X_train_list_scaled_b))\n",
    "\n",
    "\n",
    "X_train_list_scaled = np.divide(X_train_list[i], 4000)\n",
    "X_val_list_scaled = np.divide(X_val_list[i], 4000)\n",
    "\n",
    "y_train_list_scaled = np.multiply((y_train_list[i]), 4)   #for age\n",
    "y_val_list_scaled = np.multiply((y_val_list[i]), 4)\n",
    "\n",
    "#padding to replicate Mayo Clinic paper\n",
    "#X_train_list_scaled = np.pad(X_train_list_scaled, pad_width=((0,0),(0,0),(60,60),(0,0)))\n",
    "#X_val_list_scaled = np.pad(X_val_list_scaled, pad_width=((0,0),(0,0),(60,60),(0,0)))\n",
    "\n",
    "\n",
    "#sns.distplot(X_train_list_scaled)\n",
    "\n",
    "#Normalization\n",
    "#X_train_list_scaled = np.log(X_train_list[0])\n",
    "#print(np.mean(X_train_list_scaled))\n",
    "#print(np.std(X_train_list_scaled))\n",
    "\n",
    "#X_val_list_scaled = (X_val_list[0]-np.mean(X_val_list[0]))/370\n",
    "#print(np.mean(X_val_list_scaled))\n",
    "#print(np.std(X_val_list_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_list_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_list_lr = X_train_list_scaled[0:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train_list_lr = y_train_list_scaled[0:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_finder = LRFinder(min_lr=1e-4, max_lr=1e-1, steps_per_epoch=np.ceil(X_train_list_scaled.shape[0]/num_batch_size), epochs=1)\n",
    "\n",
    "#model.fit(X_train_list_scaled, y_train_list_scaled, callbacks=[lr_finder])\n",
    "#lr_finder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#changed to Adam\n",
    "\n",
    "best_epochs = []\n",
    "mses = []\n",
    "val_mses = []\n",
    "mse_hists = []\n",
    "val_mse_hists = []\n",
    "spearmans = []\n",
    "pearsons = []\n",
    "mean_aes = []\n",
    "median_aes = []\n",
    "\n",
    "#for train_index, val_index in group_kfold.split(ecg_X_k, ecg_y_k, ecg_id_k):  #for full 10-fold cross validation\n",
    "\n",
    "\n",
    "for j in range(i, (i+1)):\n",
    "    \n",
    "    K.clear_session()\n",
    "\n",
    "    adamw = AdamW(weight_decay=weight_d)\n",
    "    adam = optimizers.Adam()  #, clipnorm=1.)\n",
    "    #rmsprop = optimizers.RMSprop(learning_rate= set_learning_rate, clipnorm=1.)\n",
    "    #sgdopt = optimizers.SGD(learning_rate= set_learning_rate, clipnorm=1.)\n",
    "    #reduce_lr = tf.keras.callbacks.ReduceLROnPlateau()\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    #model.compile(optimizer= adamw, loss = set_loss, metrics= set_metrics)\n",
    "    model.compile(optimizer= adam, loss = set_loss, metrics= set_metrics)\n",
    "    model.load_weights('temp4.h5')\n",
    "    #model = load_model(filename_base + '_fold_' + str(i) + '.h5', custom_objects={'AdamW':adamw})\n",
    "    \n",
    "    \n",
    "    #corr_check = corr_checkpointer(X_val_list_scaled, y_val_list_scaled, filepath= (filename_base + '_fold_' + str(i) + '.h5'))\n",
    "    #early_stopper = corr_early_stopping(min_epochs=stopping_min_epochs, mode='max', patience=num_patience, verbose=1)\n",
    "    \n",
    "    onecyc = OneCycle(lr_range=(0.001, 0.01), momentum_range=(0.95,0.85))\n",
    "    \n",
    "    earlystopper = EarlyStopping(monitor = 'val_mean_squared_error', mode='min', patience=num_patience, verbose=1)  #restore_best_weights not available\n",
    "    checkpointer = ModelCheckpoint(filename_base + '_fold_' + str(i) + '.h5', monitor = 'val_mean_squared_error', mode='min', verbose=1, save_best_only = True)\n",
    "\n",
    "\n",
    "    \n",
    "    #reduce_lr = tf.keras.callbacks.ReduceLROnPlateau()\n",
    "\n",
    "\n",
    "    history = model.fit(X_train_list_scaled, y_train_list_scaled, validation_data = (X_val_list_scaled, y_val_list_scaled), batch_size = num_batch_size, epochs=num_epochs, callbacks=[checkpointer, onecyc, earlystopper])   #[corr_check, onecyc, early_stopper]) #tensorboard_callback])\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "current_time2 = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Start time: \", current_time)\n",
    "print(\"End time: \", current_time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#should be in above loop\n",
    "#filename_base = 'trained_' + suffix + '_12'\n",
    "#adamw = AdamW(weight_decay=weight_d)\n",
    "\n",
    "#best_epoch = len(history.history['val_mean_squared_error']) - num_patience\n",
    "#mse_model = history.history['mean_squared_error'][-(num_patience + 1)]\n",
    "#val_mse_model = history.history['val_mean_squared_error'][-(num_patience + 1)]\n",
    "#mse_hist = history.history['mean_squared_error']\n",
    "#val_mse_hist = history.history['val_mean_squared_error']\n",
    "\n",
    "\n",
    "#for round 2 model:\n",
    "#model = load_model(filename_base + '_fold_' + str(i) + '_round2.h5')\n",
    "model = load_model(filename_base + '_fold_' + str(i) + '.h5')  #, custom_objects={'AdamW':adamw})\n",
    "\n",
    "preds_val_initial = model.predict(X_val_list_scaled, verbose=1, batch_size=256)\n",
    "preds_val_initial = preds_val_initial.reshape(preds_val_initial.shape[0])\n",
    "    \n",
    "    #****\n",
    "#y_val_list_scaled = y_val_list[0]\n",
    "#preds_val = preds_val_initial\n",
    "\n",
    "y_val_list_scaled = y_val_list[i] #np.divide((y_val_list_scaled), 4)\n",
    "preds_val = np.divide((preds_val_initial), 4)\n",
    "    \n",
    "    #y_train_list_scaled = np.divide((y_train_list[0]-18),41) - 1\n",
    "    \n",
    "    #preds_val = np.multipy((preds_val_initial), 82) + 18\n",
    "    #y_train_list_scaled = np.multipiy((y_train_list_scaled), 82) +18\n",
    "\n",
    "spearman = stats.spearmanr(preds_val, y_val_list_scaled)\n",
    "pearson = stats.pearsonr(preds_val, y_val_list_scaled)\n",
    "    \n",
    "mean_ae = np.mean(abs(preds_val - y_val_list_scaled))\n",
    "median_ae = np.median(abs(preds_val - y_val_list_scaled))\n",
    "    \n",
    "print(pearson)\n",
    "print(mean_ae)\n",
    "print(median_ae)\n",
    "\n",
    "\n",
    "##best_epochs.append(best_epoch)\n",
    "#mses.append(mse_model)\n",
    "##val_mses.append(val_mse_model)\n",
    "#mse_hists.append(mse_hist)\n",
    "#val_mse_hists.append(val_mse_hist)\n",
    "#spearmans.append(spearman)\n",
    "#pearsons.append(pearson)\n",
    "#mean_aes.append(mean_ae)\n",
    "#median_aes.append(median_ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#y_val_list_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best epoch: ', best_epochs)\n",
    "print('MSE: ', mses)\n",
    "print('Val MSE: ', val_mses)\n",
    "print('Spearman: ', spearmans)\n",
    "print('Pearson: ', pearsons)\n",
    "print('Mean abs error: ', mean_aes)\n",
    "print('Median abs error: ', median_aes)\n",
    "\n",
    "\n",
    "#print('Mean MSE: ', np.mean(mses))\n",
    "#print('Mean val_MSE: ', np.mean(val_mses))\n",
    "#print('Mean Spearman r: ', [np.mean(i) for i in zip(*spearmans)])\n",
    "#print('Mean Pearson r: ', [np.mean(i) for i in zip(*pearsons)])\n",
    "#print('Mean of Mean abs error: ', np.mean(mean_aes))\n",
    "#print('Mean of Median abs error: ', np.mean(median_aes))\n",
    "\n",
    "\n",
    "mean_mse = [np.mean(i) for i in zip(*mse_hists)]\n",
    "mean_val_mse = [np.mean(i) for i in zip(*val_mse_hists)]\n",
    "plt.plot(mean_mse[3:])\n",
    "plt.plot(mean_val_mse[3:])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#for j in range(len(mse_hists)):\n",
    "#    plt.plot(mse_hists[j])\n",
    "#    plt.plot(val_mse_hists[j])\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mean_mse[50:1000])\n",
    "plt.plot(mean_val_mse[50:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for PR, max limit 420, for age 105\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,12)) \n",
    "plt.scatter(y_val_list_scaled, preds_val, alpha=0.3)\n",
    "    \n",
    "ax.set_title('Age Using MGH 12-Lead ECGs', fontsize=28)\n",
    "\n",
    "ax.set(xlim = [0, 105], ylim = [0, 105])\n",
    "plt.xticks(range(0, 105, 10), fontsize=20)\n",
    "plt.yticks(range(0, 105, 10), fontsize=20)\n",
    "plt.xlabel('Actual Age', fontsize=26)\n",
    "plt.ylabel('Predicted Age', fontsize=26)\n",
    "\n",
    "ax.plot([0, 105], [0,105], alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_list_scaled = np.multiply((y_val_list[i]), 4)\n",
    "model = load_model(filename_base + '_fold_' + str(i) + '.h5')\n",
    "\n",
    "onecyc = OneCycle(lr_range=(0.0005, 0.005), momentum_range=(0.95,0.85))\n",
    "    \n",
    "earlystopper = EarlyStopping(monitor = 'val_mean_squared_error', mode='min', patience=num_patience, verbose=1)  #restore_best_weights not available\n",
    "checkpointer = ModelCheckpoint(filename_base + '_fold_' + str(i) + '_round2.h5', monitor = 'val_mean_squared_error', mode='min', verbose=1, save_best_only = True)\n",
    "\n",
    "history = model.fit(X_train_list_scaled, y_train_list_scaled, validation_data = (X_val_list_scaled, y_val_list_scaled), batch_size = num_batch_size, epochs=num_epochs, callbacks=[checkpointer, onecyc, earlystopper])   #[corr_check, onecyc, early_stopper]) #tensorboard_callback])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
