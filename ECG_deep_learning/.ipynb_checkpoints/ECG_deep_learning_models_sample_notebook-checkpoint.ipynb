{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import io\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import csv\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold, KFold\n",
    "\n",
    "import tensorflow as tf  \n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Flatten, Reshape\n",
    "from tensorflow.keras.layers import Dropout, Lambda\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, Conv2DTranspose,Convolution2D, Conv3D, ConvLSTM2D, Bidirectional\n",
    "from tensorflow.keras.layers import ConvLSTM2D\n",
    "from tensorflow.keras.layers import MaxPooling1D, MaxPooling2D, MaxPooling3D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from tensorflow.keras.layers import Activation, SpatialDropout1D, SpatialDropout2D, LayerNormalization, GlobalAvgPool1D, GlobalMaxPool1D\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow_addons.layers import WeightNormalization\n",
    "from tensorflow_addons.optimizers import AdamW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening numpy arrays for sex    \n",
    "suffix = 'corrected_60k_sex'           #non-null- 56,665 ECGs; will use 53,665 in training set\n",
    "\n",
    "ecg_X_k = np.load('MGH_60k_500hz_ecgs_age_18_100_trainval_X.npy')\n",
    "ecg_y_k = np.load('MGH_60k_500hz_ecgs_age_18_100_trainval_yfemale.npy')\n",
    "\n",
    "ecg_X_k = ecg_X_k[ecg_y_k>=0]\n",
    "ecg_y_k = ecg_y_k[ecg_y_k>=0]\n",
    "\n",
    "print(ecg_X_k.shape)\n",
    "print(ecg_y_k.shape)\n",
    "\n",
    "np.unique(ecg_y_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening numpy arrays for sex test set\n",
    "\n",
    "ecg_X_k = np.load('MGH_60k_500hz_ecgs_age_18_100_test_X.npy')\n",
    "ecg_y_k = np.load('MGH_60k_500hz_ecgs_age_18_100_test_y.npy')\n",
    "\n",
    "ecg_X_k = ecg_X_k[ecg_y_k>=0]\n",
    "ecg_y_k = ecg_y_k[ecg_y_k>=0]\n",
    "\n",
    "print(ecg_X_k.shape)\n",
    "print(ecg_y_k.shape)\n",
    "\n",
    "np.unique(ecg_y_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening numpy arrays for age    #57,000; 54,150 in training with 20-fold\n",
    "suffix = 'corrected_60k_age'\n",
    "\n",
    "ecg_X_k = np.load('MGH_60k_500hz_ecgs_age_18_100_trainval_X.npy')\n",
    "ecg_y_k = np.load('MGH_60k_500hz_ecgs_age_18_100_trainval_yage.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_X_k = ecg_X_k[ecg_y_k>0]\n",
    "ecg_y_k = ecg_y_k[ecg_y_k>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ecg_X_k.shape)\n",
    "print(ecg_y_k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting only X and y elements in 60k dataset where rhythm is sinus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainval_sinus = np.load('MGH_60k_500hz_ecgs_age_18_100_trainval_sinus.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_filename_k = np.load('MGH_60k_500hz_ecgs_age_18_100_trainval_filename.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_metadata = []\n",
    "\n",
    "for file in ecg_filename_k:\n",
    "    \n",
    "    \n",
    "    with open('/mnt/obi0/phi/ecg/convertedData/MGH/' + file[0:5] + '/' + file + '.dict', 'rb') as f:\n",
    "        ecg_dict = pickle.load(f)\n",
    "\n",
    "    try:\n",
    "        ecg_dict['metadata']['StmtText'] = ' '.join([str(elem) for elem in ecg_dict['metadata']['StmtText']])\n",
    "        ecg_dict['metadata']['Filename'] = file\n",
    "        ecg_metadata.append(ecg_dict['metadata'])\n",
    "        \n",
    "    except:\n",
    "        ecg_metadata.append({'Filename':file})\n",
    "\n",
    "meta_df = pd.DataFrame.from_dict(ecg_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = meta_df[ecg_y_k>0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = meta_df[trainval_sinus==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_X_k = ecg_X_k[trainval_sinus==True]\n",
    "ecg_y_k = ecg_y_k[trainval_sinus==True]\n",
    "\n",
    "print(ecg_X_k.shape)          #27,215 in trainval set; 15 folds used -> 25,400 for training set\n",
    "print(ecg_y_k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df[['VentricularRate', 'AtrialRate', 'PRInterval', 'QRSDuration', 'QTInterval', 'QTCorrected', 'PAxis', 'RAxis', 'TAxis', 'QRSCount', 'QOnset', 'QOffset', 'POnset', 'POffset', 'TOffset']] = meta_df[['VentricularRate', 'AtrialRate', 'PRInterval', 'QRSDuration', 'QTInterval', 'QTCorrected', 'PAxis', 'RAxis', 'TAxis', 'QRSCount', 'QOnset', 'QOffset', 'POnset', 'POffset', 'TOffset']].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df[['QOnset', 'QOffset', 'POnset', 'POffset', 'TOffset']] = meta_df[['QOnset', 'QOffset', 'POnset', 'POffset', 'TOffset']] *2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df.iloc[0:5, 10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df.isna().sum()    #some missing atrial rate (52), p axis (182), qrs count (52), p onset (125), p offset (125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df_nonull = meta_df[pd.notnull(meta_df['POnset'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df_nonull.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_X_k = ecg_X_k[pd.notnull(meta_df['POnset'])]   #27,090, will use 24,090 for training\n",
    "ecg_y_k = ecg_y_k[pd.notnull(meta_df['POnset'])]\n",
    "\n",
    "print(ecg_X_k.shape)         \n",
    "print(ecg_y_k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df_set = meta_df_nonull[['VentricularRate', 'PRInterval', 'QRSDuration', 'QTInterval', 'QTCorrected', 'RAxis', 'TAxis', 'QOnset', 'QOffset', 'TOffset']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df_set.isna().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get only lead I\n",
    "ecg_X_k = ecg_X_k[:, 0:5000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get 2D array for 12-lead ecgs\n",
    "ecg_X_k = ecg_X_k.reshape(ecg_X_k.shape[0], 12, int(ecg_X_k.shape[1]/12), ecg_X_k.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_X_k = ecg_X_k.reshape(ecg_X_k.shape[0], ecg_X_k.shape[1])   #removes last dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_X_rescaled = np.zeros((ecg_X_k.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ecgnum in range(ecg_X_k.shape[0]):            #normalizes with mean 0 and max 1\n",
    "    ecg_X_rescaled[ecgnum] = ecg_X_k[ecgnum]-np.mean(ecg_X_k[ecgnum])\n",
    "    ecg_X_rescaled[ecgnum] = ecg_X_rescaled[ecgnum]/np.max(ecg_X_rescaled[ecgnum])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipped1 = []\n",
    "skipped2 = []\n",
    "skipped3 = []\n",
    "ecg_X_full_intervals = np.zeros((0, 6))\n",
    "ecg_X_intervals = np.zeros((0, 6))\n",
    "\n",
    "for ecg_num in range(ecg_X_rescaled.shape[0]):\n",
    "    peaks, _ = find_peaks(ecg_X_rescaled[ecg_num], distance = 150, prominence=0.6)\n",
    "    \n",
    "    if (len(peaks)>=7):\n",
    "        if (peaks[6]-peaks[0]>= 1500) & (peaks[6]-peaks[0]<=4500):  #selects mean HR 40-120 for the 7 beats\n",
    "        \n",
    "            #finds R-R intervals, as the percentage of the 7 beat group for each R-R interval\n",
    "            rr_ints = [(t - s)/(peaks[6]-peaks[0]) for s, t in zip(peaks[0:6], peaks[1:7])] \n",
    "            rr_full = [(t - s) for s, t in zip(peaks[0:6], peaks[1:7])] \n",
    "        \n",
    "            ecg_X_full_intervals = np.concatenate((ecg_X_full_intervals, np.array([rr_full])))\n",
    "            ecg_X_intervals = np.concatenate((ecg_X_intervals, np.array([rr_ints])))\n",
    "        else: \n",
    "            if (peaks[6]-peaks[0]< 1500):\n",
    "                skipped1.append(ecg_num)\n",
    "            if (peaks[6]-peaks[0]>4500):\n",
    "                skipped2.append(ecg_num)\n",
    "    else:\n",
    "        skipped3.append(ecg_num)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(skipped1)       # 614 skipped for detected mean HR for first 7 beats >120 (oversensing t-wave?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(skipped2)       # 11 skipped for detected mean HR for first 7 beats <40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(skipped3)       # 284 skipped for not having at least 7 beats detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipped = skipped1 + skipped2 + skipped3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(skipped)        # 909"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_X_intervals.shape   #26,306 kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_X_full_intervals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_X_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_X_full_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.ones_like(ecg_y_k, bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[skipped]=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_y_intervals = ecg_y_k[mask]       #26,306 in y numpy array\n",
    "print(ecg_y_intervals.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(ecg_X_intervals)  #0.167"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(ecg_X_intervals)   #0.509"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(ecg_X_intervals)   #0.042"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_X_intervals_all = ecg_X_intervals.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ecg_X_intervals_all)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#will set min and max R-R intervals to avoid PACs and PVCs\n",
    "#mean R-R: 0.1667\n",
    "#max- 0.200 : 20% increase\n",
    "#min- 0.133 : 20% decrease\n",
    "#difference between min and max- 50%\n",
    "ecg_y_intervals = ecg_y_intervals[(np.min(ecg_X_intervals, axis=1)>=0.133) & (np.max(ecg_X_intervals, axis=1)<=0.2)]\n",
    "ecg_X_intervals = ecg_X_intervals[(np.min(ecg_X_intervals, axis=1)>=0.133) & (np.max(ecg_X_intervals, axis=1)<=0.2),:]\n",
    "\n",
    "print(ecg_X_intervals.shape)    #23,509 of 26,306 remaining; will use 2,000 for val set\n",
    "print(ecg_y_intervals.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_X_full_intervals = ecg_X_full_intervals[(np.min(ecg_X_intervals, axis=1)>=0.133) & (np.max(ecg_X_intervals, axis=1)<=0.2),:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_X_full_intervals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full_list = [0]\n",
    "X_train_full_list[0] = ecg_X_full_intervals[0:21509]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternative to k-fold \n",
    "\n",
    "X_train_list = [0]\n",
    "y_train_list = [0]\n",
    "X_val_list = [0]\n",
    "y_val_list = [0]\n",
    "\n",
    "X_train_list[0] = ecg_X_intervals[0:24090]\n",
    "y_train_list[0] = ecg_y_intervals[0:24090]\n",
    "\n",
    "X_val_list[0] = ecg_X_intervals[24090:]\n",
    "y_val_list[0] = ecg_y_intervals[24090:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to visualize ECGs\n",
    "ecg_num= 108\n",
    "\n",
    "peaks, _ = find_peaks(ecg_X_rescaled[ecg_num], distance = 150, prominence=0.6) \n",
    "\n",
    "plt.figure(figsize=(14,10))\n",
    "plt.plot(ecg_X_rescaled[ecg_num])\n",
    "plt.plot(peaks, ecg_X_rescaled[ecg_num][peaks], 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list = [0]\n",
    "y_train_list = [0]\n",
    "X_val_list = [0]\n",
    "y_val_list = [0]\n",
    "\n",
    "X_train_list[0] = ecg_X_k[0:21509]\n",
    "y_train_list[0] = ecg_y_k[0:21509]\n",
    "\n",
    "X_val_list[0] = ecg_X_k[21509:]\n",
    "y_val_list[0] = ecg_y_k[21509:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_list = [0]\n",
    "y_train_list = [0]\n",
    "X_val_list = [0]\n",
    "y_val_list = [0]\n",
    "meta_df_train_list = [0]\n",
    "meta_df_val_list = [0]\n",
    "\n",
    "X_train_list[0] = ecg_X_k[0:24090]\n",
    "y_train_list[0] = ecg_y_k[0:24090]\n",
    "meta_df_train_list[0] = meta_df_set[0:24090]\n",
    "\n",
    "X_val_list[0] = ecg_X_k[24090:]\n",
    "y_val_list[0] = ecg_y_k[24090:]\n",
    "meta_df_val_list[0] = meta_df_set[24090:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list[0] = X_train_list[0][:, 0:5000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_list[0] = X_val_list[0][:, 0:5000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_val_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if reshaping needed\n",
    "X_train_list[0] = X_train_list[0].reshape(X_train_list[0].shape[0], 12, 5000, 1)\n",
    "X_val_list[0] = X_val_list[0].reshape(X_val_list[0].shape[0], 12, 5000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_list[0][0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(X_val_list[0][45][0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class corr_checkpointer(keras.callbacks.Callback):\n",
    "    def __init__(self, x_val, y_val, filepath, batch_size=512, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.x_val = x_val\n",
    "        self.y_val = y_val\n",
    "        self.best_corr = -1.\n",
    "        self.curr_corr = 0.0\n",
    "        self.best_epoch = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.filepath = filepath\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        print('\\nBest Pearson: %.3f, from epoch %d' % (self.best_corr, self.best_epoch))\n",
    "        pred_val = self.model.predict(self.x_val, batch_size=self.batch_size)\n",
    "        pred_val = pred_val.reshape(pred_val.shape[0])\n",
    "        pearson_val = stats.pearsonr(pred_val, self.y_val)\n",
    "        print('Current Pearson: %.3f' % pearson_val[0])\n",
    "        \n",
    "        if(pearson_val[0] > self.best_corr):\n",
    "            self.best_corr = pearson_val[0]\n",
    "            self.best_epoch = epoch + 1\n",
    "            self.model.save(self.filepath, overwrite=True)\n",
    "            print('Saved current epoch model')       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class corr_early_stopping(keras.callbacks.EarlyStopping):\n",
    "    def __init__(self, min_epochs=200, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.min_epochs = min_epochs\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):   #parent function with alterations as below\n",
    "        current = corr_check.best_corr          #change value assigned to current\n",
    "        if current is None:\n",
    "            return\n",
    "        if self.monitor_op(current - self.min_delta, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            if self.restore_best_weights:\n",
    "                self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if (self.wait >= self.patience) and (epoch > self.min_epochs):   #add min_epochs\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                if self.restore_best_weights:\n",
    "                    if self.verbose > 0:\n",
    "                        print('Restoring model weights from the end of the best epoch.')\n",
    "                    self.model.set_weights(self.best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://www.jeremyjordan.me/nn-learning-rate/\n",
    "\n",
    "class LRFinder(keras.callbacks.Callback):\n",
    "    \n",
    "    '''\n",
    "    A simple callback for finding the optimal learning rate range for your model + dataset. \n",
    "    \n",
    "    # Usage\n",
    "        ```python\n",
    "            lr_finder = LRFinder(min_lr=1e-5, \n",
    "                                 max_lr=1e-2, \n",
    "                                 steps_per_epoch=np.ceil(epoch_size/batch_size), \n",
    "                                 epochs=3)\n",
    "            model.fit(X_train, Y_train, callbacks=[lr_finder])\n",
    "            \n",
    "            lr_finder.plot_loss()\n",
    "        ```\n",
    "    \n",
    "    # Arguments\n",
    "        min_lr: The lower bound of the learning rate range for the experiment.\n",
    "        max_lr: The upper bound of the learning rate range for the experiment.\n",
    "        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n",
    "        epochs: Number of epochs to run experiment. Usually between 2 and 4 epochs is sufficient. \n",
    "        \n",
    "    # References\n",
    "        Blog post: jeremyjordan.me/nn-learning-rate\n",
    "        Original paper: https://arxiv.org/abs/1506.01186\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, min_lr=1e-5, max_lr=1e-2, steps_per_epoch=None, epochs=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.total_iterations = steps_per_epoch * epochs\n",
    "        self.iteration = 0\n",
    "        self.history = {}\n",
    "        \n",
    "    def clr(self):\n",
    "        '''Calculate the learning rate.'''\n",
    "        x = self.iteration / self.total_iterations \n",
    "        return self.min_lr + (self.max_lr-self.min_lr) * x\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
    "        logs = logs or {}\n",
    "        K.set_value(self.model.optimizer.lr, self.min_lr)\n",
    "        \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        '''Record previous batch statistics and update the learning rate.'''\n",
    "        logs = logs or {}\n",
    "        self.iteration += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.iteration)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "            \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    " \n",
    "    def plot_lr(self):\n",
    "        '''Helper function to quickly inspect the learning rate schedule.'''\n",
    "        plt.plot(self.history['iterations'], self.history['lr'])\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Learning rate')\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_loss(self):\n",
    "        '''Helper function to quickly observe the learning rate experiment results.'''\n",
    "        plt.plot(self.history['lr'], self.history['loss'])\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel('Learning rate')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://github.com/psklight/keras_one_cycle_clr/blob/master/keras_one_cycle_clr/one_cycle.py\n",
    "\n",
    "class OneCycle(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    A callback class for one-cycle policy training.\n",
    "    :param lr_range: a tuple of starting (usually minimum) lr value and maximum (peak) lr value.\n",
    "    :param momentum_range: a tuple of momentum values.\n",
    "    :param phase_one_fraction: a fraction for phase I (increasing lr) in one cycle. Must between 0 to 1.\n",
    "    :param reset_on_train_begin: True or False to reset counters when training begins.\n",
    "    :param record_frq: integer > 0, a frequency in batches to record training loss.\n",
    "    :param verbose: True or False to print progress.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            lr_range,\n",
    "            momentum_range=None,\n",
    "            phase_one_fraction=0.3,\n",
    "            reset_on_train_begin=True,\n",
    "            record_frq=10,\n",
    "            verbose=False):\n",
    "\n",
    "        super(OneCycle, self).__init__()\n",
    "\n",
    "        self.lr_range = lr_range\n",
    "\n",
    "        self.momentum_range = momentum_range\n",
    "        if momentum_range is not None:\n",
    "            err_msg = \"momentum_range must be a 2-numeric tuple (m1, m2).\"\n",
    "            if not isinstance(momentum_range, (tuple,)) or len(momentum_range) != 2:\n",
    "                raise ValueError(err_msg)\n",
    "\n",
    "        self.phase_one_fraction = phase_one_fraction\n",
    "        self.reset_on_train_begin = reset_on_train_begin\n",
    "        self.record_frq = record_frq\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # helper tracker\n",
    "        self.log = {}  # history in iterations\n",
    "        self.log_ep = {}  # history in epochs\n",
    "        self.stop_training = False\n",
    "\n",
    "        # counter\n",
    "        self.current_iter = 0\n",
    "\n",
    "    def get_current_lr(self, n_iter=None):\n",
    "        \"\"\"\n",
    "        A helper function to calculate a current learning rate based on current iteration number.\n",
    "        :return lr: a current learning rate.\n",
    "        \"\"\"\n",
    "        if n_iter is None:\n",
    "            n_iter = self.n_iter\n",
    "\n",
    "        x = float(self.current_iter) / n_iter\n",
    "        if x < self.phase_one_fraction:\n",
    "            amp = self.lr_range[1] - self.lr_range[0]\n",
    "            lr = (np.cos(x * np.pi/self.phase_one_fraction - np.pi) + 1) * amp / 2.0 + self.lr_range[0]\n",
    "        if x >= self.phase_one_fraction:\n",
    "            amp = self.lr_range[1]\n",
    "            lr = (np.cos((x - self.phase_one_fraction) * np.pi/ (1-self.phase_one_fraction)) + 1) / 2.0 * amp\n",
    "        return lr\n",
    "\n",
    "    def get_current_momentum(self, n_iter=None):\n",
    "        \"\"\"\n",
    "        A helper function to calculate a current momentum based on current iteration number.\n",
    "        :return momentum: a current momentum.\n",
    "        \"\"\"\n",
    "        if n_iter is None:\n",
    "            n_iter = self.n_iter\n",
    "        amp = self.momentum_range[1] - self.momentum_range[0]\n",
    "        # delta = (1 - np.abs(np.mod(self.current_iter, n_iter) * 2.0 / n_iter - 1)) * amplitude\n",
    "        x = float(self.current_iter) / n_iter\n",
    "        if x < self.phase_one_fraction:\n",
    "            delta = (np.cos(x * np.pi / self.phase_one_fraction - np.pi) + 1) * amp / 2.0\n",
    "        if x >= self.phase_one_fraction:\n",
    "            delta = (np.cos((x - self.phase_one_fraction) * np.pi / (1 - self.phase_one_fraction)) + 1) / 2.0 * amp\n",
    "        return delta + self.momentum_range[0]\n",
    "\n",
    "\n",
    "    @property\n",
    "    def cycle_momentum(self):\n",
    "        return self.momentum_range is not None\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.n_epoch = self.params['epochs']\n",
    "\n",
    "        # find number of batches per epoch\n",
    "        if self.params['batch_size'] is not None:  # model.fit\n",
    "            self.n_bpe = int(np.ceil(self.params['samples'] / self.params['batch_size']))\n",
    "        if self.params['batch_size'] is None:  # model.fit_generator\n",
    "            self.n_bpe = self.params['samples']\n",
    "\n",
    "        self.n_iter = self.n_epoch * self.n_bpe\n",
    "        # this is a number of iteration in one cycle\n",
    "\n",
    "        self.current_iter = 0\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs={}):\n",
    "        set_lr(self.model.optimizer, self.get_current_lr())\n",
    "        if self.cycle_momentum:\n",
    "            set_momentum(self.model.optimizer, self.get_current_momentum())\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs={}):\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"lr={:.2e}\".format(self.get_current_lr()), \",\", \"m={:.2e}\".format(self.get_current_momentum()))\n",
    "\n",
    "        # record according to record_frq\n",
    "        if np.mod(int(self.current_iter), self.record_frq) == 0:\n",
    "            self.log.setdefault('lr', []).append(self.get_current_lr())\n",
    "            if self.cycle_momentum:\n",
    "                self.log.setdefault('momentum', []).append(self.get_current_momentum())\n",
    "\n",
    "            for k, v in logs.items():\n",
    "                self.log.setdefault(k, []).append(v)\n",
    "\n",
    "            self.log.setdefault('iter', []).append(self.current_iter)\n",
    "\n",
    "        # update current iteration\n",
    "        self.current_iter += 1\n",
    "\n",
    "        # consider termination\n",
    "        if self.current_iter == self.n_iter:\n",
    "            self.stop_training = True\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.log_ep.setdefault('epoch', []).append(epoch)\n",
    "        self.log_ep.setdefault('lr', []).append(\n",
    "            K.get_value(self.model.optimizer.lr))\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.log_ep.setdefault(k, []).append(v)\n",
    "\n",
    "    def test_run(self, n_iter=None):\n",
    "        \"\"\"\n",
    "        Visualize values of learning rate (and momentum) as a function of iteration (batch).\n",
    "        :param n_iter: a number of cycles. If None, 1000 is used.\n",
    "        \"\"\"\n",
    "\n",
    "        if hasattr(self, 'current_iter'):\n",
    "            original_it = self.current_iter\n",
    "\n",
    "        if n_iter is None:\n",
    "            if hasattr(self, 'n_iter'):\n",
    "                n_iter = self.n_iter\n",
    "            else:\n",
    "                n_iter = 1000\n",
    "        n_iter = int(n_iter)\n",
    "\n",
    "        lrs = np.zeros(shape=(n_iter,))\n",
    "        if self.momentum_range is not None:\n",
    "            moms = np.zeros_like(lrs)\n",
    "\n",
    "        for i in range(int(n_iter)):\n",
    "            self.current_iter = i\n",
    "            lrs[i] = self.get_current_lr(n_iter)\n",
    "            if self.cycle_momentum:\n",
    "                moms[i] = self.get_current_momentum(n_iter)\n",
    "        if not self.cycle_momentum:\n",
    "            plt.plot(lrs)\n",
    "            plt.xlabel('iterations')\n",
    "            plt.ylabel('lr')\n",
    "        else:\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(lrs)\n",
    "            plt.xlabel('iterations')\n",
    "            plt.ylabel('lr')\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(moms)\n",
    "            plt.xlabel('iterations')\n",
    "            plt.ylabel('momentum')\n",
    "\n",
    "        if hasattr(self, 'current_iter'):\n",
    "            self.current_iter = original_it\n",
    "\n",
    "def set_momentum(optimizer, mom_val):\n",
    "    \"\"\"\n",
    "    Helper to set momentum of Keras optimizers.\n",
    "    :param optimizer: Keras optimizer\n",
    "    :param mom_val: value of momentum.\n",
    "    \"\"\"\n",
    "    keys = dir(optimizer)\n",
    "    if \"momentum\" in keys:\n",
    "        K.set_value(optimizer.momentum, mom_val)\n",
    "    if \"rho\" in keys:\n",
    "        K.set_value(optimizer.rho, mom_val)\n",
    "    if \"beta_1\" in keys:\n",
    "        K.set_value(optimizer.beta_1, mom_val)\n",
    "\n",
    "\n",
    "def set_lr(optimizer, lr):\n",
    "    \"\"\"\n",
    "    Helper to set learning rate of Keras optimizers.\n",
    "    :param optimizer: Keras optimizer\n",
    "    :param lr: value of learning rate.\n",
    "    \"\"\"\n",
    "    K.set_value(optimizer.lr, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted version of https://github.com/josesho/bland_altman/blob/master/bland_altman.py\n",
    "\n",
    "def bland_altman_plot(m1, m2,\n",
    "                      sd_limit=1.96,\n",
    "                      ax=None,\n",
    "                      scatter_kwds=None,\n",
    "                      mean_line_kwds=None,\n",
    "                      limit_lines_kwds=None,\n",
    "                      title=None):\n",
    "\n",
    "\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    if len(m1) != len(m2):\n",
    "        raise ValueError('m1 does not have the same length as m2.')\n",
    "    if sd_limit < 0:\n",
    "        raise ValueError('sd_limit ({}) is less than 0.'.format(sd_limit))\n",
    "\n",
    "    means = np.mean([m1, m2], axis=0)\n",
    "    diffs = m1 - m2\n",
    "    mean_diff = np.mean(diffs)\n",
    "    std_diff = np.std(diffs, axis=0)\n",
    "\n",
    "    if ax is None:\n",
    "        plt.figure(figsize=(10,5))\n",
    "        ax = plt.gca()\n",
    "        ax.set_title(title, fontsize=14)\n",
    "        \n",
    "\n",
    "    scatter_kwds = scatter_kwds or {}\n",
    "    if 's' not in scatter_kwds:\n",
    "        scatter_kwds['s'] = 20\n",
    "    mean_line_kwds = mean_line_kwds or {}\n",
    "    limit_lines_kwds = limit_lines_kwds or {}\n",
    "    for kwds in [mean_line_kwds, limit_lines_kwds]:\n",
    "        if 'color' not in kwds:\n",
    "            kwds['color'] = 'gray'\n",
    "        if 'linewidth' not in kwds:\n",
    "            kwds['linewidth'] = 1\n",
    "    if 'linestyle' not in mean_line_kwds:\n",
    "        kwds['linestyle'] = '--'\n",
    "    if 'linestyle' not in limit_lines_kwds:\n",
    "        kwds['linestyle'] = ':'\n",
    "\n",
    "    ax.scatter(means, diffs, **scatter_kwds, alpha=0.3)\n",
    "    ax.axhline(mean_diff, **mean_line_kwds)  # draw mean line.\n",
    "\n",
    "    # Annotate mean line with mean difference.\n",
    "    ax.annotate('mean diff:\\n{}'.format(np.round(mean_diff, 2)),\n",
    "                xy=(0.99, 0.49),\n",
    "                horizontalalignment='right',\n",
    "                verticalalignment='center',\n",
    "                fontsize=12,\n",
    "                xycoords='axes fraction')\n",
    "\n",
    "    if sd_limit > 0:\n",
    "        plt.axis('scaled')\n",
    "        left, right = plt.xlim()\n",
    "        plt.xlim(left*0.7, right*1.2)\n",
    "        half_ylim = (1.5 * sd_limit) * std_diff\n",
    "        ax.set_ylim(mean_diff - (half_ylim*2),\n",
    "                    mean_diff + (half_ylim*2))\n",
    "        \n",
    "        limit_of_agreement = sd_limit * std_diff\n",
    "        lower = mean_diff - limit_of_agreement\n",
    "        upper = mean_diff + limit_of_agreement\n",
    "        for j, lim in enumerate([lower, upper]):\n",
    "            ax.axhline(lim, **limit_lines_kwds)\n",
    "        ax.annotate('-SD{}: {}'.format(sd_limit, np.round(lower, 2)),\n",
    "                    xy=(0.99, 0.25),\n",
    "                    horizontalalignment='right',\n",
    "                    verticalalignment='bottom',\n",
    "                    fontsize=12,\n",
    "                    xycoords='axes fraction')\n",
    "        ax.annotate('+SD{}: {}'.format(sd_limit, np.round(upper, 2)),\n",
    "                    xy=(0.99, 0.70),\n",
    "                    horizontalalignment='right',\n",
    "                    fontsize=12,\n",
    "                    xycoords='axes fraction')\n",
    "\n",
    "    elif sd_limit == 0:\n",
    "        half_ylim = 3 * std_diff\n",
    "        ax.set_ylim(mean_diff - half_ylim,\n",
    "                    mean_diff + half_ylim)\n",
    "\n",
    "    ax.set_ylabel('Difference', fontsize=12)\n",
    "    ax.set_xlabel('Means', fontsize=12)\n",
    "    ax.tick_params(labelsize=12)\n",
    "    plt.tight_layout()\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list[0] = ecg_X_k[0:24090]\n",
    "y_train_list[0] = ecg_y_k[0:24090]\n",
    "meta_df_train_list[0] = meta_df_set[0:24090]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df_train_list[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df_train = meta_df_train_list[0].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df_val = meta_df_val_list[0].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = 'ecg_meaurements_age_05_27_2020'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "filename_base = 'trained_' + suffix + '_1'\n",
    "print(filename_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on Wavenet\n",
    "\n",
    "inputs = Input((5000, 1))\n",
    "\n",
    "c1 = Conv1D(16,4, padding='same', strides=2, use_bias = False, kernel_initializer= 'he_normal') (inputs)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Activation('relu') (c1)\n",
    "c1 = Dropout(0.1) (c1)\n",
    "\n",
    "c1 = Conv1D(32,8, padding='same', strides=2, use_bias = False, kernel_initializer= 'he_normal') (inputs)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Activation('relu') (c1)\n",
    "c1 = Dropout(0.1) (c1)\n",
    "\n",
    "s1 = Conv1D(16, 8, padding='same', use_bias=False, kernel_initializer= 'he_normal') (c1)\n",
    "s1 = BatchNormalization() (s1)\n",
    "\n",
    "c2 = WeightNormalization(Conv1D(32,8, padding='same', dilation_rate=2, kernel_initializer= 'he_normal')) (c1)\n",
    "c2 = Activation('relu') (c2)\n",
    "c2 = SpatialDropout1D(0.1) (c2)\n",
    "c2 = WeightNormalization(Conv1D(32,8, padding='same', dilation_rate=4, kernel_initializer= 'he_normal')) (c2)\n",
    "#c2 = Activation('relu') (c2)\n",
    "c2 = Activation('relu') (c1+c2)\n",
    "c2 = SpatialDropout1D(0.1) (c2)\n",
    "#c2 = MaxPooling1D(pool_size=1, strides=4) (c2)\n",
    "\n",
    "c3 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=8, kernel_initializer= 'he_normal')) (c2)\n",
    "c3 = Activation('relu') (c3)\n",
    "c3 = SpatialDropout1D(0.1) (c3)\n",
    "c3 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=16, kernel_initializer= 'he_normal')) (c3)\n",
    "#c3 = Activation('relu') (c3)\n",
    "c3 = Activation('relu') (s1+c3)\n",
    "c3 = SpatialDropout1D(0.1) (c3)\n",
    "c3 = MaxPooling1D(pool_size=1, strides=8) (c3)\n",
    "\n",
    "c7 = Conv1D(1, 3, use_bias = False, kernel_initializer= 'he_normal') (c3)\n",
    "c7 = BatchNormalization() (c7)\n",
    "c7 = Activation('relu') (c7)\n",
    "c7 = Dropout(0.3) (c7)\n",
    "\n",
    "final = Flatten() (c7)\n",
    "\n",
    "outputs = Dense(10) (final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only Dense layers\n",
    "\n",
    "inputs = Input((5000, 1))\n",
    "\n",
    "c1 = Dense(5, kernel_initializer = 'he_normal', activation = 'relu') (inputs)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Dropout(0.5) (c1)\n",
    "\n",
    "c1 = Dense(5, kernel_initializer = 'he_normal', activation = 'relu') (c1)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Dropout(0.5) (c1)\n",
    "\n",
    "c1 = Dense(5, kernel_initializer = 'he_normal', activation = 'relu') (c1)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Dropout(0.5) (c1)\n",
    "\n",
    "c1 = Dense(5, kernel_initializer = 'he_normal', activation = 'relu') (c1)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Dropout(0.5) (c1)\n",
    "\n",
    "c1 = Dense(5, kernel_initializer = 'he_normal', activation = 'relu') (c1)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Dropout(0.5) (c1)\n",
    "\n",
    "c1 = Dense(5, kernel_initializer = 'he_normal', activation = 'relu') (c1)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Dropout(0.5) (c1)\n",
    "\n",
    "c1 = Dense(5, kernel_initializer = 'he_normal', activation = 'relu') (c1)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Dropout(0.5) (c1)\n",
    "\n",
    "c1 = Dense(5, kernel_initializer = 'he_normal', activation = 'relu') (c1)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Dropout(0.5) (c1)\n",
    "\n",
    "c1 = Dense(5, kernel_initializer = 'he_normal', activation = 'relu') (c1)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Dropout(0.5) (c1)\n",
    "\n",
    "outputs = Dense(10) (c1)\n",
    "\n",
    "#c1 = Conv2D(32,(1,8), padding='same', strides=(1,2), use_bias = False, kernel_initializer= 'he_normal') (c1)\n",
    "#c1 = BatchNormalization() (c1)\n",
    "#c1 = Activation('relu') (c1)\n",
    "#c1 = Dropout(0.1) (c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changed to Adam, changed batch size\n",
    "\n",
    "set_loss = 'mean_squared_error'       # 'binary_crossentropy'    #'mean_squared_error'\n",
    "set_metrics = ['mean_squared_error']  # ['accuracy']             #['mean_squared_error']\n",
    "\n",
    "num_batch_size=256   #180\n",
    "num_epochs=10000\n",
    "num_patience=300\n",
    "stopping_min_epochs=80\n",
    "weight_d = 0 #0.000001  \n",
    "\n",
    "adamw = AdamW(weight_decay=weight_d)\n",
    "#adam = optimizers.Adam()\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[outputs])\n",
    "model.compile(optimizer= adamw, loss = set_loss, metrics= set_metrics)\n",
    "model.save_weights('temp.h5')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_finder = LRFinder(min_lr=1e-4, max_lr=1e-1, steps_per_epoch=np.ceil(X_train_list[0].shape[0]/num_batch_size), epochs=1)\n",
    "\n",
    "#model.fit(X_train_list[0], meta_df_train, callbacks=[lr_finder])\n",
    "#lr_finder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "#corr_check = corr_checkpointer(X_val_list_scaled, y_val_list_scaled, filepath= (filename_base + '_fold_' + str(i) + '.h5'))\n",
    "#early_stopper = corr_early_stopping(min_epochs=stopping_min_epochs, mode='max', patience=num_patience, verbose=1)\n",
    "    \n",
    "onecyc = OneCycle(lr_range=(0.001, 0.01), momentum_range=(0.95,0.85))\n",
    "    \n",
    "earlystopper = EarlyStopping(monitor = 'val_mean_squared_error', mode='min', patience=num_patience, verbose=1)  #restore_best_weights not available\n",
    "checkpointer = ModelCheckpoint(filename_base + '_fold_' + str(i) + '.h5', monitor = 'val_mean_squared_error', mode='min', verbose=1, save_best_only = True)\n",
    "\n",
    "history = model.fit(X_train_list[0], meta_df_train, validation_data = (X_val_list[0], meta_df_val), batch_size = num_batch_size, epochs=num_epochs, callbacks=[checkpointer, onecyc, earlystopper])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(filename_base + '_fold_' + str(i) + '.h5', custom_objects={'AdamW':adamw})\n",
    "\n",
    "preds_val = model.predict(X_val_list[0], verbose=1, batch_size=256)\n",
    "preds_val = preds_val.reshape(preds_val.shape[0])\n",
    "    \n",
    "    #****\n",
    "#y_val_list_scaled = y_val_list[0]\n",
    "#preds_val = preds_val_initial\n",
    "\n",
    "y_val_list_scaled = meta_df_val\n",
    "#y_val_list_scaled = y_val_list[0] #np.divide((y_val_list_scaled), 4)\n",
    "#preds_val = np.divide((preds_val_initial), 4)\n",
    "    \n",
    "    #y_train_list_scaled = np.divide((y_train_list[0]-18),41) - 1\n",
    "    \n",
    "    #preds_val = np.multipy((preds_val_initial), 82) + 18\n",
    "    #y_train_list_scaled = np.multipiy((y_train_list_scaled), 82) +18\n",
    "\n",
    "spearman = stats.spearmanr(preds_val, y_val_list_scaled)\n",
    "#pearson = stats.pearsonr(preds_val, y_val_list_scaled)\n",
    "    \n",
    "mean_ae = np.mean(abs(preds_val - y_val_list_scaled))\n",
    "median_ae = np.median(abs(preds_val - y_val_list_scaled))\n",
    "    \n",
    "print(spearman)\n",
    "print(mean_ae)\n",
    "print(median_ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(filename_base + '_fold_' + str(i) + '.h5', custom_objects={'AdamW':adamw})\n",
    "\n",
    "preds_val = model.predict(X_val_list[0], verbose=1, batch_size=256)\n",
    "#preds_val = preds_val.reshape(preds_val.shape[0])\n",
    "    \n",
    "    #****\n",
    "#y_val_list_scaled = y_val_list[0]\n",
    "#preds_val = preds_val_initial\n",
    "\n",
    "y_val_list_scaled = meta_df_val\n",
    "#y_val_list_scaled = y_val_list[0] #np.divide((y_val_list_scaled), 4)\n",
    "#preds_val = np.divide((preds_val_initial), 4)\n",
    "    \n",
    "    #y_train_list_scaled = np.divide((y_train_list[0]-18),41) - 1\n",
    "    \n",
    "    #preds_val = np.multipy((preds_val_initial), 82) + 18\n",
    "    #y_train_list_scaled = np.multipiy((y_train_list_scaled), 82) +18\n",
    "    \n",
    "\n",
    "for preds_val_single, y_val_single in preds_val, y_val_list_scaled:\n",
    "    spearman = stats.spearmanr(preds_val_single, y_val_list_single)\n",
    "    #pearson = stats.pearsonr(preds_val, y_val_list_scaled)\n",
    "    \n",
    "    mean_ae = np.mean(abs(preds_val_single - y_val_list_scaled))\n",
    "    median_ae = np.median(abs(preds_val_single - y_val_list_scaled))\n",
    "    \n",
    "    print(spearman)\n",
    "    print(mean_ae)\n",
    "    print(median_ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    \n",
    "#for preds_val_single, y_val_single in preds_val, y_val_list_scaled:\n",
    "    spearman = stats.spearmanr(preds_val[:,i], y_val_list_scaled[:,i])\n",
    "    #pearson = stats.pearsonr(preds_val, y_val_list_scaled)\n",
    "    \n",
    "    #mean_ae = np.mean(abs(preds_val_single - y_val_list_scaled))\n",
    "    #median_ae = np.median(abs(preds_val_single - y_val_list_scaled))\n",
    "    \n",
    "    print(spearman)\n",
    "    #print(pearson)\n",
    "    #print(mean_ae)\n",
    "    #print(median_ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['VentricularRate', 'PRInterval', 'QRSDuration', 'QTInterval', 'QTCorrected', 'RAxis', 'TAxis', 'QOnset', 'QOffset', 'TOffset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(preds_val[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_val[1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,12)) \n",
    "plt.scatter(y_val_list_scaled, preds_val, alpha=0.1)\n",
    "    \n",
    "#ax.set_title('Age Using MGH 12-Lead ECGs', fontsize=28)\n",
    "\n",
    "#ax.set(xlim = [0, 105], ylim = [0, 105])\n",
    "#plt.xticks(range(0, 105, 10), fontsize=20)\n",
    "#plt.yticks(range(0, 105, 10), fontsize=20)\n",
    "plt.xlabel('Age', fontsize=26)\n",
    "plt.ylabel('Standard Dev. of R-R Intervals', fontsize=26)\n",
    "\n",
    "#ax.plot([0, 105], [0,105], alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_interval_std = np.std(X_train_list[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_interval_full_std = np.std(X_train_full_list[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,12)) \n",
    "plt.scatter(y_train_list[0], X_interval_full_std, alpha=0.1)\n",
    "    \n",
    "#ax.set_title('Age Using MGH 12-Lead ECGs', fontsize=28)\n",
    "\n",
    "#ax.set(xlim = [0, 105], ylim = [0, 105])\n",
    "#plt.xticks(range(0, 105, 10), fontsize=20)\n",
    "#plt.yticks(range(0, 105, 10), fontsize=20)\n",
    "plt.xlabel('Age', fontsize=26)\n",
    "plt.ylabel('Standard Dev. of R-R Intervals', fontsize=26)\n",
    "\n",
    "#ax.plot([0, 105], [0,105], alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,12)) \n",
    "plt.scatter(y_train_list[0], X_interval_std, alpha=0.1)\n",
    "    \n",
    "#ax.set_title('Age Using MGH 12-Lead ECGs', fontsize=28)\n",
    "\n",
    "#ax.set(xlim = [0, 105], ylim = [0, 105])\n",
    "#plt.xticks(range(0, 105, 10), fontsize=20)\n",
    "#plt.yticks(range(0, 105, 10), fontsize=20)\n",
    "plt.xlabel('Age', fontsize=26)\n",
    "plt.ylabel('Standard Dev. of Rescaled R-R Intervals', fontsize=26)\n",
    "\n",
    "#ax.plot([0, 105], [0,105], alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for age\n",
    "\n",
    "#for PR, range 60-420\n",
    "#for age, range 15-105\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "\n",
    "ax.scatter(y_val_list_scaled, preds_val, color='#732673', alpha = 0.3)\n",
    "lims = [15, 105]\n",
    "    #np.min([60, 60]),  # min of both axes\n",
    "    #np.max([420, 420]),  # max of both axes\n",
    "\n",
    "ax.plot(lims, lims, alpha=0.75, zorder=0, color='#505050', linestyle='--', dashes = [2,2])\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlim(lims)\n",
    "ax.set_ylim(lims)\n",
    "#ax.set_xlim([-0.02, 1.0])\n",
    "#ax.set_ylim([-0.02, 1.05])\n",
    "#ax.set_title('PR Interval Using AMC Single-Lead ECGs', fontsize=34)\n",
    "ax.set_xlabel('Actual Age (years)')\n",
    "ax.set_ylabel('Predicted Age (years)')\n",
    "ax.set_facecolor('#f7f7f7')\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.spines['left'].set_position(('outward', 5))\n",
    "ax.spines['bottom'].set_position(('outward', 5))\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.grid(b=True, which='major', color='white', linestyle='-', linewidth=3.0)\n",
    "#ax.grid(b=True, which='minor', color='white', linestyle='-', linewidth=1)\n",
    "ax.set_axisbelow(True)\n",
    "#ax.legend(loc='best', frameon=False)\n",
    "plt.yticks(np.arange(20, 110, 20))\n",
    "plt.minorticks_on()\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 32})\n",
    "plt.show()\n",
    "#outfile = \"test.png\"\n",
    "#fig.savefig(outfile)\n",
    "#plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_list_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression (binary classification separate)\n",
    "#suffix = 'sex_8k'\n",
    "#suffix = 'pr_12lead_2d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suffix = 'corrected_60k_age_12lead'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "filename_base = 'trained_' + suffix + '_1'\n",
    "print(filename_base)\n",
    "\n",
    "in_neurons=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12-lead model\n",
    "inputs = Input((12, in_neurons, 1))\n",
    "\n",
    "c1 = Conv2D(16,(1,4), padding='same', strides=(1,2), use_bias = False, kernel_initializer= 'he_normal') (inputs)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Activation('relu') (c1)\n",
    "c1 = Dropout(0.1) (c1)\n",
    "\n",
    "c1 = Conv2D(32,(1,8), padding='same', strides=(1,2), use_bias = False, kernel_initializer= 'he_normal') (c1)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Activation('relu') (c1)\n",
    "c1 = Dropout(0.1) (c1)\n",
    "\n",
    "s1 = Conv2D(16, (1,8), padding='same', use_bias=False, kernel_initializer= 'he_normal') (c1)\n",
    "s1 = BatchNormalization() (s1)\n",
    "\n",
    "c2 = WeightNormalization(Conv2D(32,(1,8), padding='same', dilation_rate=(1,2), kernel_initializer= 'he_normal')) (c1)\n",
    "c2 = Activation('relu') (c2)\n",
    "c2 = SpatialDropout2D(0.1) (c2)\n",
    "c2 = WeightNormalization(Conv2D(32,(1,8), padding='same', dilation_rate=(1,4), kernel_initializer= 'he_normal')) (c2)\n",
    "#c2 = Activation('relu') (c2)\n",
    "c2 = Activation('relu') (c1+c2)\n",
    "c2 = SpatialDropout2D(0.1) (c2)\n",
    "#c2 = MaxPooling1D(pool_size=1, strides=4) (c2)\n",
    "\n",
    "c3 = WeightNormalization(Conv2D(16,(1,8), padding='same', dilation_rate=(1,8), kernel_initializer= 'he_normal')) (c2)\n",
    "c3 = Activation('relu') (c3)\n",
    "c3 = SpatialDropout2D(0.1) (c3)\n",
    "c3 = WeightNormalization(Conv2D(16,(1,8), padding='same', dilation_rate=(1,16), kernel_initializer= 'he_normal')) (c3)\n",
    "#c3 = Activation('relu') (c3)\n",
    "c3 = Activation('relu') (s1+c3)\n",
    "c3 = SpatialDropout2D(0.1) (c3)\n",
    "c3 = MaxPooling2D(pool_size=1, strides=(1,8)) (c3)\n",
    "\n",
    "c7 = Conv2D(8, (1,3), use_bias = False, kernel_initializer= 'he_normal') (c3)\n",
    "c7 = BatchNormalization() (c7)\n",
    "c7 = Activation('relu') (c7)\n",
    "c7 = Dropout(0.3) (c7)\n",
    "\n",
    "c7 = Conv2D(16, (12,1), use_bias=False, kernel_initializer='he_normal') (c7)\n",
    "c7 = BatchNormalization() (c7)\n",
    "c7 = Activation('relu') (c7)\n",
    "c7 = Dropout(0.3) (c7)\n",
    "\n",
    "final = Flatten() (c7)\n",
    "\n",
    "outputs = Dense(1) (final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#single-lead model\n",
    "\n",
    "inputs = Input((in_neurons, 1))\n",
    "\n",
    "c1 = Conv1D(16,4, padding='same', strides=2, use_bias = False, kernel_initializer= 'he_normal') (inputs)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Activation('relu') (c1)\n",
    "c1 = Dropout(0.1) (c1)\n",
    "\n",
    "c1 = Conv1D(32,8, padding='same', strides=2, use_bias = False, kernel_initializer= 'he_normal') (inputs)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Activation('relu') (c1)\n",
    "c1 = Dropout(0.1) (c1)\n",
    "\n",
    "s1 = Conv1D(16, 8, padding='same', use_bias=False, kernel_initializer= 'he_normal') (c1)\n",
    "s1 = BatchNormalization() (s1)\n",
    "\n",
    "c2 = WeightNormalization(Conv1D(32,8, padding='same', dilation_rate=2, kernel_initializer= 'he_normal')) (c1)\n",
    "c2 = Activation('relu') (c2)\n",
    "c2 = SpatialDropout1D(0.1) (c2)\n",
    "c2 = WeightNormalization(Conv1D(32,8, padding='same', dilation_rate=4, kernel_initializer= 'he_normal')) (c2)\n",
    "#c2 = Activation('relu') (c2)\n",
    "c2 = Activation('relu') (c1+c2)\n",
    "c2 = SpatialDropout1D(0.1) (c2)\n",
    "#c2 = MaxPooling1D(pool_size=1, strides=4) (c2)\n",
    "\n",
    "c3 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=8, kernel_initializer= 'he_normal')) (c2)\n",
    "c3 = Activation('relu') (c3)\n",
    "c3 = SpatialDropout1D(0.1) (c3)\n",
    "c3 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=16, kernel_initializer= 'he_normal')) (c3)\n",
    "#c3 = Activation('relu') (c3)\n",
    "c3 = Activation('relu') (s1+c3)\n",
    "c3 = SpatialDropout1D(0.1) (c3)\n",
    "c3 = MaxPooling1D(pool_size=1, strides=8) (c3)\n",
    "\n",
    "c7 = Conv1D(1, 3, use_bias = False, kernel_initializer= 'he_normal') (c3)\n",
    "c7 = BatchNormalization() (c7)\n",
    "c7 = Activation('relu') (c7)\n",
    "c7 = Dropout(0.3) (c7)\n",
    "\n",
    "final = Flatten() (c7)\n",
    "\n",
    "outputs = Dense(1) (final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changed to Adam, changed batch size\n",
    "\n",
    "set_loss = 'mean_squared_error' # 'binary_crossentropy'    #'mean_squared_error'\n",
    "set_metrics = ['mean_squared_error']  # ['accuracy']                      #['mean_squared_error']\n",
    "\n",
    "num_batch_size=512   #180\n",
    "num_epochs=100\n",
    "num_patience=30\n",
    "stopping_min_epochs=80\n",
    "weight_d = 0 #0.000001  \n",
    "\n",
    "#adamw = AdamW(weight_decay=weight_d)\n",
    "adam = optimizers.Adam()\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[outputs])\n",
    "model.compile(optimizer= adam, loss = set_loss, metrics= set_metrics)\n",
    "model.save_weights('temp4.h5')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ocp = OneCycle(lr_range=(0.0007, 0.007), momentum_range=(0.94,0.85))\n",
    "#ocp.test_run(128)\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "#y_train_list_scaled = y_train_list[i]\n",
    "#y_val_list_scaled = y_val_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removed age scaling\n",
    "\n",
    "#X_train_list_scaled = np.multiply(np.sign(X_train_list[0]), np.log(np.abs(X_train_list[0])+1)) #log-modulus transform- https://blogs.sas.com/content/iml/2014/07/14/log-transformation-of-pos-neg.html\n",
    "#X_val_list_scaled = np.multiply(np.sign(X_val_list[0]), np.log(np.abs(X_val_list[0])+1))\n",
    "\n",
    "#12-lead- too large\n",
    "#X_train_list_scaled_a = np.divide(X_train_list[i][0:27000],4000)\n",
    "#X_train_list_scaled_b = np.divide(X_train_list[i][27000:],4000)\n",
    "#X_train_list_scaled = np.vstack((X_train_list_scaled_a, X_train_list_scaled_b))\n",
    "\n",
    "\n",
    "X_train_list_scaled = np.divide(X_train_list[i], 4000)\n",
    "X_val_list_scaled = np.divide(X_val_list[i], 4000)\n",
    "\n",
    "y_train_list_scaled = np.multiply((y_train_list[i]), 4)   #for age\n",
    "y_val_list_scaled = np.multiply((y_val_list[i]), 4)\n",
    "\n",
    "#padding to replicate Mayo Clinic paper\n",
    "#X_train_list_scaled = np.pad(X_train_list_scaled, pad_width=((0,0),(0,0),(60,60),(0,0)))\n",
    "#X_val_list_scaled = np.pad(X_val_list_scaled, pad_width=((0,0),(0,0),(60,60),(0,0)))\n",
    "\n",
    "\n",
    "#sns.distplot(X_train_list_scaled)\n",
    "\n",
    "#Normalization\n",
    "#X_train_list_scaled = np.log(X_train_list[0])\n",
    "#print(np.mean(X_train_list_scaled))\n",
    "#print(np.std(X_train_list_scaled))\n",
    "\n",
    "#X_val_list_scaled = (X_val_list[0]-np.mean(X_val_list[0]))/370\n",
    "#print(np.mean(X_val_list_scaled))\n",
    "#print(np.std(X_val_list_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_list_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_list_lr = X_train_list_scaled[0:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train_list_lr = y_train_list_scaled[0:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_finder = LRFinder(min_lr=1e-4, max_lr=1e-1, steps_per_epoch=np.ceil(X_train_list_scaled.shape[0]/num_batch_size), epochs=1)\n",
    "\n",
    "#model.fit(X_train_list_scaled, y_train_list_scaled, callbacks=[lr_finder])\n",
    "#lr_finder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#changed to Adam\n",
    "\n",
    "best_epochs = []\n",
    "mses = []\n",
    "val_mses = []\n",
    "mse_hists = []\n",
    "val_mse_hists = []\n",
    "spearmans = []\n",
    "pearsons = []\n",
    "mean_aes = []\n",
    "median_aes = []\n",
    "\n",
    "#for train_index, val_index in group_kfold.split(ecg_X_k, ecg_y_k, ecg_id_k):  #for full 10-fold cross validation\n",
    "\n",
    "\n",
    "for j in range(i, (i+1)):\n",
    "    \n",
    "    K.clear_session()\n",
    "\n",
    "    adamw = AdamW(weight_decay=weight_d)\n",
    "    adam = optimizers.Adam()  #, clipnorm=1.)\n",
    "    #rmsprop = optimizers.RMSprop(learning_rate= set_learning_rate, clipnorm=1.)\n",
    "    #sgdopt = optimizers.SGD(learning_rate= set_learning_rate, clipnorm=1.)\n",
    "    #reduce_lr = tf.keras.callbacks.ReduceLROnPlateau()\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    #model.compile(optimizer= adamw, loss = set_loss, metrics= set_metrics)\n",
    "    model.compile(optimizer= adam, loss = set_loss, metrics= set_metrics)\n",
    "    model.load_weights('temp4.h5')\n",
    "    #model = load_model(filename_base + '_fold_' + str(i) + '.h5', custom_objects={'AdamW':adamw})\n",
    "    \n",
    "    \n",
    "    #corr_check = corr_checkpointer(X_val_list_scaled, y_val_list_scaled, filepath= (filename_base + '_fold_' + str(i) + '.h5'))\n",
    "    #early_stopper = corr_early_stopping(min_epochs=stopping_min_epochs, mode='max', patience=num_patience, verbose=1)\n",
    "    \n",
    "    onecyc = OneCycle(lr_range=(0.001, 0.01), momentum_range=(0.95,0.85))\n",
    "    \n",
    "    earlystopper = EarlyStopping(monitor = 'val_mean_squared_error', mode='min', patience=num_patience, verbose=1)  #restore_best_weights not available\n",
    "    checkpointer = ModelCheckpoint(filename_base + '_fold_' + str(i) + '.h5', monitor = 'val_mean_squared_error', mode='min', verbose=1, save_best_only = True)\n",
    "\n",
    "\n",
    "    \n",
    "    #reduce_lr = tf.keras.callbacks.ReduceLROnPlateau()\n",
    "\n",
    "\n",
    "    history = model.fit(X_train_list_scaled, y_train_list_scaled, validation_data = (X_val_list_scaled, y_val_list_scaled), batch_size = num_batch_size, epochs=num_epochs, callbacks=[checkpointer, onecyc, earlystopper])   #[corr_check, onecyc, early_stopper]) #tensorboard_callback])\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "current_time2 = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Start time: \", current_time)\n",
    "print(\"End time: \", current_time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#should be in above loop\n",
    "#filename_base = 'trained_' + suffix + '_12'\n",
    "#adamw = AdamW(weight_decay=weight_d)\n",
    "\n",
    "#best_epoch = len(history.history['val_mean_squared_error']) - num_patience\n",
    "#mse_model = history.history['mean_squared_error'][-(num_patience + 1)]\n",
    "#val_mse_model = history.history['val_mean_squared_error'][-(num_patience + 1)]\n",
    "#mse_hist = history.history['mean_squared_error']\n",
    "#val_mse_hist = history.history['val_mean_squared_error']\n",
    "\n",
    "\n",
    "#for round 2 model:\n",
    "#model = load_model(filename_base + '_fold_' + str(i) + '_round2.h5')\n",
    "model = load_model(filename_base + '_fold_' + str(i) + '.h5')  #, custom_objects={'AdamW':adamw})\n",
    "\n",
    "preds_val_initial = model.predict(X_val_list_scaled, verbose=1, batch_size=256)\n",
    "preds_val_initial = preds_val_initial.reshape(preds_val_initial.shape[0])\n",
    "    \n",
    "    #****\n",
    "#y_val_list_scaled = y_val_list[0]\n",
    "#preds_val = preds_val_initial\n",
    "\n",
    "y_val_list_scaled = y_val_list[i] #np.divide((y_val_list_scaled), 4)\n",
    "preds_val = np.divide((preds_val_initial), 4)\n",
    "    \n",
    "    #y_train_list_scaled = np.divide((y_train_list[0]-18),41) - 1\n",
    "    \n",
    "    #preds_val = np.multipy((preds_val_initial), 82) + 18\n",
    "    #y_train_list_scaled = np.multipiy((y_train_list_scaled), 82) +18\n",
    "\n",
    "spearman = stats.spearmanr(preds_val, y_val_list_scaled)\n",
    "pearson = stats.pearsonr(preds_val, y_val_list_scaled)\n",
    "    \n",
    "mean_ae = np.mean(abs(preds_val - y_val_list_scaled))\n",
    "median_ae = np.median(abs(preds_val - y_val_list_scaled))\n",
    "    \n",
    "print(pearson)\n",
    "print(mean_ae)\n",
    "print(median_ae)\n",
    "\n",
    "\n",
    "##best_epochs.append(best_epoch)\n",
    "#mses.append(mse_model)\n",
    "##val_mses.append(val_mse_model)\n",
    "#mse_hists.append(mse_hist)\n",
    "#val_mse_hists.append(val_mse_hist)\n",
    "#spearmans.append(spearman)\n",
    "#pearsons.append(pearson)\n",
    "#mean_aes.append(mean_ae)\n",
    "#median_aes.append(median_ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#y_val_list_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best epoch: ', best_epochs)\n",
    "print('MSE: ', mses)\n",
    "print('Val MSE: ', val_mses)\n",
    "print('Spearman: ', spearmans)\n",
    "print('Pearson: ', pearsons)\n",
    "print('Mean abs error: ', mean_aes)\n",
    "print('Median abs error: ', median_aes)\n",
    "\n",
    "\n",
    "#print('Mean MSE: ', np.mean(mses))\n",
    "#print('Mean val_MSE: ', np.mean(val_mses))\n",
    "#print('Mean Spearman r: ', [np.mean(i) for i in zip(*spearmans)])\n",
    "#print('Mean Pearson r: ', [np.mean(i) for i in zip(*pearsons)])\n",
    "#print('Mean of Mean abs error: ', np.mean(mean_aes))\n",
    "#print('Mean of Median abs error: ', np.mean(median_aes))\n",
    "\n",
    "\n",
    "mean_mse = [np.mean(i) for i in zip(*mse_hists)]\n",
    "mean_val_mse = [np.mean(i) for i in zip(*val_mse_hists)]\n",
    "plt.plot(mean_mse[3:])\n",
    "plt.plot(mean_val_mse[3:])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#for j in range(len(mse_hists)):\n",
    "#    plt.plot(mse_hists[j])\n",
    "#    plt.plot(val_mse_hists[j])\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mean_mse[50:1000])\n",
    "plt.plot(mean_val_mse[50:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bland_altman_plot(preds_val, y_val_list_scaled, scatter_kwds={'s':10}, title='Age Using MGH Single-Lead ECGs')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for PR, max limit 420, for age 105\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,12)) \n",
    "plt.scatter(y_val_list_scaled, preds_val, alpha=0.3)\n",
    "    \n",
    "ax.set_title('Age Using MGH 12-Lead ECGs', fontsize=28)\n",
    "\n",
    "ax.set(xlim = [0, 105], ylim = [0, 105])\n",
    "plt.xticks(range(0, 105, 10), fontsize=20)\n",
    "plt.yticks(range(0, 105, 10), fontsize=20)\n",
    "plt.xlabel('Actual Age', fontsize=26)\n",
    "plt.ylabel('Predicted Age', fontsize=26)\n",
    "\n",
    "ax.plot([0, 105], [0,105], alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_list_scaled = np.multiply((y_val_list[i]), 4)\n",
    "model = load_model(filename_base + '_fold_' + str(i) + '.h5')\n",
    "\n",
    "onecyc = OneCycle(lr_range=(0.0005, 0.005), momentum_range=(0.95,0.85))\n",
    "    \n",
    "earlystopper = EarlyStopping(monitor = 'val_mean_squared_error', mode='min', patience=num_patience, verbose=1)  #restore_best_weights not available\n",
    "checkpointer = ModelCheckpoint(filename_base + '_fold_' + str(i) + '_round2.h5', monitor = 'val_mean_squared_error', mode='min', verbose=1, save_best_only = True)\n",
    "\n",
    "history = model.fit(X_train_list_scaled, y_train_list_scaled, validation_data = (X_val_list_scaled, y_val_list_scaled), batch_size = num_batch_size, epochs=num_epochs, callbacks=[checkpointer, onecyc, earlystopper])   #[corr_check, onecyc, early_stopper]) #tensorboard_callback])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_list_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_val_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import csv\n",
    "from skimage.transform import resize\n",
    "import math\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import statistics \n",
    "from plotnine import *\n",
    "from scipy.stats import spearmanr \n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_list_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#****to check graphs***\n",
    "#y_val_list_scaled = np.multiply((y_val_list[i]), 4)\n",
    "\n",
    "#PR single-lead- 'trained_corrected_60k_pr_1_fold_1_round2.h5'\n",
    "#PR 12-lead 'trained_corrected_60k_pr_12lead_1_fold_1.h5'\n",
    "#age single lead- 'trained_corrected_60k_age_1_fold_0_round2.h5'\n",
    "#age 12-lead- 'trained_corrected_60k_age_12lead_1_fold_0_round2.h5'\n",
    "#agesexmatched age singel lead- ''trained_agesexmatched_60k_age_1_fold_0_round2.h5'\n",
    "\n",
    "\n",
    "i=0\n",
    "X_train_list_scaled = np.divide(X_train_list[i], 4000)\n",
    "X_val_list_scaled = np.divide(X_val_list[i], 4000)\n",
    "\n",
    "y_train_list_scaled = y_train_list[i]      #for PR\n",
    "y_val_list_scaled = y_val_list[i]\n",
    "\n",
    "#y_train_list_scaled = np.multiply((y_train_list[i]), 4)   #for age\n",
    "#y_val_list_scaled = np.multiply((y_val_list[i]), 4)\n",
    "\n",
    "model = load_model('trained_corrected_60k_pr_12lead_1_fold_1.h5') #, custom_objects={'AdamW':adamw})\n",
    "#model = load_model('trained_pr_first_1000_58_fold_' + str(i) + '.h5', custom_objects={'AdamW':adamw})\n",
    "\n",
    "preds_val = model.predict(X_val_list_scaled, verbose=1, batch_size=256)\n",
    "preds_val = preds_val.reshape(preds_val.shape[0])\n",
    "\n",
    "#for age\n",
    "#y_val_list_scaled = np.divide((y_val_list_scaled), 4)\n",
    "#preds_val = np.divide((preds_val), 4)\n",
    "\n",
    "spearman = stats.spearmanr(preds_val, y_val_list_scaled)\n",
    "pearson = stats.pearsonr(preds_val, y_val_list_scaled)\n",
    "    \n",
    "mean_ae = np.mean(abs(preds_val - y_val_list_scaled))\n",
    "median_ae = np.median(abs(preds_val - y_val_list_scaled))\n",
    "    \n",
    "print('Spearman: ', spearman)\n",
    "print('Pearson: ', pearson)\n",
    "print('Mean abs error: ', mean_ae)\n",
    "print('Median abs error: ', median_ae)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,12)) \n",
    "plt.scatter(y_val_list_scaled, preds_val, alpha=0.3)\n",
    "    \n",
    "ax.set_title('Age Using MGH 12-Lead ECGs', fontsize=28)\n",
    "\n",
    "ax.set(xlim = [0, 420], ylim = [0, 420])\n",
    "plt.xticks(range(0, 420, 10), fontsize=20)\n",
    "plt.yticks(range(0, 420, 10), fontsize=20)\n",
    "plt.xlabel('Actual Age', fontsize=26)\n",
    "plt.ylabel('Predicted Age', fontsize=26)\n",
    "\n",
    "ax.plot([0, 420], [0,420], alpha=0.2)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for pr\n",
    "\n",
    "#for PR, range 60-420\n",
    "#for age, range 15-105\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "\n",
    "ax.scatter(y_val_list_scaled, preds_val, color='#732673', alpha = 0.3)\n",
    "lims = [60, 420]\n",
    "    #np.min([60, 60]),  # min of both axes\n",
    "    #np.max([420, 420]),  # max of both axes\n",
    "\n",
    "ax.plot(lims, lims, alpha=0.75, zorder=0, color='#505050', linestyle='--', dashes = [2,2])\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlim(lims)\n",
    "ax.set_ylim(lims)\n",
    "#ax.set_xlim([-0.02, 1.0])\n",
    "#ax.set_ylim([-0.02, 1.05])\n",
    "#ax.set_title('PR Interval Using AMC Single-Lead ECGs', fontsize=34)\n",
    "ax.set_xlabel('Actual PR Interval (msec)')\n",
    "ax.set_ylabel('Predicted PR Interval (msec)')\n",
    "ax.set_facecolor('#f7f7f7')\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.spines['left'].set_position(('outward', 5))\n",
    "ax.spines['bottom'].set_position(('outward', 5))\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.grid(b=True, which='major', color='white', linestyle='-', linewidth=3.0)\n",
    "#ax.grid(b=True, which='minor', color='white', linestyle='-', linewidth=1)\n",
    "ax.set_axisbelow(True)\n",
    "#ax.legend(loc='best', frameon=False)\n",
    "plt.yticks(np.arange(100, 405, 100))\n",
    "plt.minorticks_on()\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 32})\n",
    "plt.show()\n",
    "#outfile = \"test.png\"\n",
    "#fig.savefig(outfile)\n",
    "#plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for age\n",
    "\n",
    "#for PR, range 60-420\n",
    "#for age, range 15-105\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "\n",
    "ax.scatter(y_val_list_scaled, preds_val, color='#732673', alpha = 0.3)\n",
    "lims = [15, 105]\n",
    "    #np.min([60, 60]),  # min of both axes\n",
    "    #np.max([420, 420]),  # max of both axes\n",
    "\n",
    "ax.plot(lims, lims, alpha=0.75, zorder=0, color='#505050', linestyle='--', dashes = [2,2])\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlim(lims)\n",
    "ax.set_ylim(lims)\n",
    "#ax.set_xlim([-0.02, 1.0])\n",
    "#ax.set_ylim([-0.02, 1.05])\n",
    "#ax.set_title('PR Interval Using AMC Single-Lead ECGs', fontsize=34)\n",
    "ax.set_xlabel('Actual Age (years)')\n",
    "ax.set_ylabel('Predicted Age (years)')\n",
    "ax.set_facecolor('#f7f7f7')\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.spines['left'].set_position(('outward', 5))\n",
    "ax.spines['bottom'].set_position(('outward', 5))\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.grid(b=True, which='major', color='white', linestyle='-', linewidth=3.0)\n",
    "#ax.grid(b=True, which='minor', color='white', linestyle='-', linewidth=1)\n",
    "ax.set_axisbelow(True)\n",
    "#ax.legend(loc='best', frameon=False)\n",
    "plt.yticks(np.arange(20, 110, 20))\n",
    "plt.minorticks_on()\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 32})\n",
    "plt.show()\n",
    "#outfile = \"test.png\"\n",
    "#fig.savefig(outfile)\n",
    "#plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grey97='#f7f7f7'\n",
    "steelblue='#4682b4'\n",
    "darkorange3=\"#cd6600\"\n",
    "grey30='#4d4d4d'\n",
    "grey50='#7f7f7f'\n",
    "\n",
    "title='PR Interval Using MGH Single-Lead ECGs'\n",
    "xmax = 410\n",
    "ymax = 410\n",
    "xlabel = 'Observed'\n",
    "alpha = 0.1\n",
    "\n",
    "\n",
    "plot=(ggplot() \\\n",
    "    +aes(x=y_val_list_scaled,y=preds_val) \\\n",
    "    +geom_point(alpha=alpha) \\\n",
    "    +theme(axis_title_x=element_text(size=22),axis_title_y=element_text(size=22),axis_text_x=element_text(size=22,angle=0,hjust=0.5),axis_text_y=element_text(size=22)) \\ \n",
    "    +theme(panel_background=element_rect(fill=grey97)) \\\n",
    "    +geom_abline(intercept=0,slope=1,color=grey50) \\\n",
    "    +labs(title=title) \\\n",
    "    +theme(plot_title=element_text(size=26,hjust=0.5)) \\\n",
    "    +ylim(0,ymax)+xlim(0,xmax) \\\n",
    "      #+xlab(xlabel)+ylab('Deviation') \\\n",
    "    +geom_text(label='some_label',x=18,y=3,size=12) \\\n",
    "    ) \\\n",
    "\n",
    "plot.save('tempplot.pdf', height=8, width=8, dpi=120) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatPlot(data,title,xmax,ymax,xlabel='Observed',pred_key='Predicted',obs_key='Observed',alpha=0.1):\n",
    "    linModel=LinearRegression()\n",
    "    linModel.fit(np.array(data[pred_key]).reshape(-1,1),np.array(data[obs_key]).reshape(-1,1))\n",
    "    print('intercept:',linModel.intercept_,'slope:',linModel.coef_)\n",
    "    rho=spearmanr(list(data[pred_key]),list(data[obs_key]))[0]\n",
    "    print(rho)\n",
    "    label='$\\\\rho$='+str(round(rho,2))\n",
    "    plot=(ggplot(data)\n",
    "     +aes(x=pred_key,y=obs_key)\n",
    "     +geom_point(alpha=alpha)\n",
    "     +theme(axis_title_x='Actual PR'(size=22),axis_title_y='Predicted PR'(size=22),axis_text_x='Actualx'(size=22,angle=0,hjust=0.5),axis_text_y='Actualy'(size=22))\n",
    "     +theme(panel_background=element_rect(fill=grey97))\n",
    "     +geom_abline(intercept=0,slope=1,color=grey50)\n",
    "     +labs(title=title)\n",
    "     +theme(plot_title=element_text(size=26,hjust=0.5))\n",
    "     +ylim(0,ymax)+xlim(0,xmax)\n",
    "     #+xlab(xlabel)+ylab('Deviation')\n",
    "     +geom_text(label=label,x=18,y=3,size=12)\n",
    "    )\n",
    "    \n",
    "    return plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotsp.save(SCATPLOT_FILE_TRUNC+'_video.pdf',height=8,width=8,dpi=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET=\"BWHTest\"\n",
    "LABEL='E'\n",
    "RANGE=300\n",
    "SRC_FILE ='results'+TARGET+'_withBL.txt'\n",
    "BAPLOT_FILE_TRUNC='BA_PLOT_'+TARGET\n",
    "SCATPLOT_FILE_TRUNC='SCAT_PLOT_'+TARGET\n",
    "grey97='#f7f7f7'\n",
    "steelblue='#4682b4'\n",
    "darkorange3=\"#cd6600\"\n",
    "grey30='#4d4d4d'\n",
    "grey50='#7f7f7f'\n",
    "\n",
    "\n",
    "def BAPlot(data,title,xmin,xmax,xlabel='Observed',pred_key='Predicted',obs_key='Observed',alpha=0.1,yscale=10):\n",
    "    ytxt=0.9*yscale\n",
    "    deviation=np.array(list(data[pred_key]))-np.array(list(data[obs_key]))\n",
    "    y025=np.percentile(deviation,2.5)\n",
    "    y125=np.percentile(deviation,12.5)\n",
    "    y250=np.percentile(deviation,25)\n",
    "    y500=np.percentile(deviation,50)\n",
    "    y750=np.percentile(deviation,75)\n",
    "    y875=np.percentile(deviation,87.5)\n",
    "    y975=np.percentile(deviation,97.5)\n",
    "    data['BADeviation']=deviation\n",
    "    \n",
    "    medErr=np.median(np.abs(np.array(list(data[pred_key]))-np.array(list(data[obs_key]))))\n",
    "    label='median error='+str(round(medErr,2))\n",
    "    plot=(ggplot(data)\n",
    "     +aes(x=obs_key,y='BADeviation')\n",
    "     +geom_point(alpha=alpha)\n",
    "     +theme(axis_title_x=element_text(size=22),axis_title_y=element_text(size=22),axis_text_x=element_text(size=22,angle=0,hjust=0.5),axis_text_y=element_text(size=22))\n",
    "     +theme(panel_background=element_rect(fill=grey97))\n",
    "     +geom_hline(yintercept=y025,color=steelblue,linetype='dashed',size=1)\n",
    "     +geom_hline(yintercept=y250,color=darkorange3,linetype='dashed',size=1.5)\n",
    "     +geom_hline(yintercept=y750,color=darkorange3,linetype='dashed',size=1.5)\n",
    "     +geom_hline(yintercept=y975,color=steelblue,linetype='dashed',size=1)\n",
    "     +geom_hline(yintercept=y500,color=grey30,size=1)\n",
    "     +labs(title=title)\n",
    "     +theme(plot_title=element_text(size=26,hjust=0.5))\n",
    "     +ylim(-yscale,yscale)+xlim(xmin,xmax)\n",
    "     +xlab(xlabel)+ylab('Deviation')\n",
    "     +geom_text(label=label,x=18,y=3,size=12)\n",
    "    )\n",
    "    \n",
    "    return plot\n",
    "\n",
    "def scatPlot(data,title,xmax,ymax,xlabel='Observed',pred_key='Predicted',obs_key='Observed',alpha=0.1):\n",
    "    linModel=LinearRegression()\n",
    "    linModel.fit(np.array(data[pred_key]).reshape(-1,1),np.array(data[obs_key]).reshape(-1,1))\n",
    "    print('intercept:',linModel.intercept_,'slope:',linModel.coef_)\n",
    "    rho=spearmanr(list(data[pred_key]),list(data[obs_key]))[0]\n",
    "    print(rho)\n",
    "    label='$\\\\rho$='+str(round(rho,2))\n",
    "    plot=(ggplot(data)\n",
    "     +aes(x=pred_key,y=obs_key)\n",
    "     +geom_point(alpha=alpha)\n",
    "     +theme(axis_title_x=element_text(size=22),axis_title_y=element_text(size=22),axis_text_x=element_text(size=22,angle=0,hjust=0.5),axis_text_y=element_text(size=22))\n",
    "     +theme(panel_background=element_rect(fill=grey97))\n",
    "     +geom_abline(intercept=0,slope=1,color=grey50)\n",
    "     +labs(title=title)\n",
    "     +theme(plot_title=element_text(size=26,hjust=0.5))\n",
    "     +ylim(0,ymax)+xlim(0,xmax)\n",
    "     #+xlab(xlabel)+ylab('Deviation')\n",
    "     +geom_text(label=label,x=18,y=3,size=12)\n",
    "    )\n",
    "    \n",
    "    return plot\n",
    "    \n",
    "srcDF=pd.read_csv(SRC_FILE,sep='\\t')\n",
    "\n",
    "baplot=BAPlot(srcDF,LABEL,0,RANGE,yscale=RANGE/2)#,pred_key='study_median')\n",
    "baplot.save(BAPLOT_FILE_TRUNC+'_video.pdf',height=6,width=12,dpi=120)\n",
    "\n",
    "sp=scatPlot(srcDF,LABEL,RANGE,RANGE)\n",
    "sp.save(SCATPLOT_FILE_TRUNC+'_video.pdf',height=8,width=8,dpi=120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to continue training\n",
    "y_val_list_scaled = np.multiply((y_val_list_scaled), 4)\n",
    "\n",
    "model = load_model(filename_base + '_fold_' + str(i) + '.h5')\n",
    "                   \n",
    "history = model.fit(X_train_list_scaled, y_train_list_scaled, validation_data = (X_val_list_scaled, y_val_list_scaled), batch_size = num_batch_size, epochs=num_epochs, callbacks=[corr_check, onecyc, early_stopper])   #[corr_check, onecyc, early_stopper]) #tensorboard_callback])\n",
    "    \n",
    "best_epoch = len(history.history['val_mean_squared_error']) - num_patience\n",
    "mse_model = history.history['mean_squared_error'][-(num_patience + 1)]\n",
    "val_mse_model = history.history['val_mean_squared_error'][-(num_patience + 1)]\n",
    "mse_hist = history.history['mean_squared_error']\n",
    "val_mse_hist = history.history['val_mean_squared_error']\n",
    "\n",
    "model = load_model(filename_base + '_fold_' + str(i) + '.h5')  #, custom_objects={'AdamW':adamw})\n",
    "preds_val_initial = model.predict(X_val_list_scaled, verbose=1, batch_size=256)\n",
    "preds_val_initial = preds_val_initial.reshape(preds_val_initial.shape[0])\n",
    "    \n",
    "    #****\n",
    "    #y_val_list_scaled = np.divide((y_val_list_scaled), 4)\n",
    "    #preds_val = np.divide((preds_val_initial), 4)\n",
    "    \n",
    "    #y_train_list_scaled = np.divide((y_train_list[0]-18),41) - 1\n",
    "    \n",
    "    #preds_val = np.multipy((preds_val_initial), 82) + 18\n",
    "    #y_train_list_scaled = np.multipiy((y_train_list_scaled), 82) +18\n",
    "\n",
    "spearman = stats.spearmanr(preds_val, y_val_list_scaled)\n",
    "pearson = stats.pearsonr(preds_val, y_val_list_scaled)\n",
    "    \n",
    "mean_ae = np.mean(abs(preds_val - y_val_list_scaled))\n",
    "median_ae = np.median(abs(preds_val - y_val_list_scaled))\n",
    "    \n",
    "    \n",
    "best_epochs.append(best_epoch)\n",
    "mses.append(mse_model)\n",
    "val_mses.append(val_mse_model)\n",
    "mse_hists.append(mse_hist)\n",
    "val_mse_hists.append(val_mse_hist)\n",
    "spearmans.append(spearman)\n",
    "pearsons.append(pearson)\n",
    "mean_aes.append(mean_ae)\n",
    "median_aes.append(median_ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = 'sex_60k'\n",
    "#suffix = 'pr_as_binary'\n",
    "#suffix = 'sex_8k'\n",
    "#suffix = 'pr_12lead_2d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = '100k_diabetes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "filename_base = 'trained_' + suffix + '_2'\n",
    "print(filename_base)\n",
    "\n",
    "in_neurons=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#single-lead model\n",
    "\n",
    "inputs = Input((in_neurons, 1))\n",
    "\n",
    "c1 = Conv1D(16,4, padding='same', strides=2, use_bias = False, kernel_initializer= 'he_normal') (inputs)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Activation('relu') (c1)\n",
    "c1 = Dropout(0.1) (c1)\n",
    "\n",
    "c1 = Conv1D(32,8, padding='same', strides=2, use_bias = False, kernel_initializer= 'he_normal') (inputs)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Activation('relu') (c1)\n",
    "c1 = Dropout(0.1) (c1)\n",
    "\n",
    "s1 = Conv1D(16, 8, padding='same', use_bias=False, kernel_initializer= 'he_normal') (c1)\n",
    "s1 = BatchNormalization() (s1)\n",
    "\n",
    "c2 = WeightNormalization(Conv1D(32,8, padding='same', dilation_rate=2, kernel_initializer= 'he_normal')) (c1)\n",
    "c2 = Activation('relu') (c2)\n",
    "c2 = SpatialDropout1D(0.1) (c2)\n",
    "c2 = WeightNormalization(Conv1D(32,8, padding='same', dilation_rate=4, kernel_initializer= 'he_normal')) (c2)\n",
    "#c2 = Activation('relu') (c2)\n",
    "c2 = Activation('relu') (c1+c2)\n",
    "c2 = SpatialDropout1D(0.1) (c2)\n",
    "#c2 = MaxPooling1D(pool_size=1, strides=4) (c2)\n",
    "\n",
    "c3 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=8, kernel_initializer= 'he_normal')) (c2)\n",
    "c3 = Activation('relu') (c3)\n",
    "c3 = SpatialDropout1D(0.1) (c3)\n",
    "c3 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=16, kernel_initializer= 'he_normal')) (c3)\n",
    "#c3 = Activation('relu') (c3)\n",
    "c3 = Activation('relu') (s1+c3)\n",
    "c3 = SpatialDropout1D(0.1) (c3)\n",
    "c3 = MaxPooling1D(pool_size=1, strides=8) (c3)\n",
    "\n",
    "c7 = Conv1D(1, 3, use_bias = False, kernel_initializer= 'he_normal') (c3)\n",
    "c7 = BatchNormalization() (c7)\n",
    "c7 = Activation('relu') (c7)\n",
    "c7 = Dropout(0.3) (c7)\n",
    "\n",
    "final = Flatten() (c7)\n",
    "\n",
    "outputs = Dense(1, activation='sigmoid') (final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changed to Adam, changed batch size\n",
    "\n",
    "set_loss = 'binary_crossentropy'    #'mean_squared_error'\n",
    "set_metrics = ['accuracy']                      #['mean_squared_error']\n",
    "\n",
    "num_batch_size=64   #180\n",
    "num_epochs=500\n",
    "num_patience=100\n",
    "stopping_min_epochs=100\n",
    "weight_d = 0 #0.000001  \n",
    "\n",
    "#adamw = AdamW(weight_decay=weight_d)\n",
    "adam = optimizers.Adam()\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[outputs])\n",
    "model.compile(optimizer= adam, loss = set_loss, metrics= set_metrics)\n",
    "model.save_weights('temp4.h5')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ocp = OneCycle(lr_range=(0.0007, 0.007), momentum_range=(0.94,0.85))\n",
    "#ocp.test_run(128)\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_list_scaled = y_train_list[0]\n",
    "y_val_list_scaled = y_val_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removed age scaling\n",
    "\n",
    "#X_train_list_scaled = np.multiply(np.sign(X_train_list[0]), np.log(np.abs(X_train_list[0])+1)) #log-modulus transform- https://blogs.sas.com/content/iml/2014/07/14/log-transformation-of-pos-neg.html\n",
    "#X_val_list_scaled = np.multiply(np.sign(X_val_list[0]), np.log(np.abs(X_val_list[0])+1))\n",
    "X_train_list_scaled = np.divide(X_train_list[0],4000)\n",
    "X_val_list_scaled = np.divide(X_val_list[0], 4000)\n",
    "\n",
    "#y_train_list_scaled = np.multiply((y_train_list[0]), 4)\n",
    "#y_val_list_scaled = np.multiply((y_val_list[0]), 4)\n",
    "\n",
    "#padding to replicate Mayo Clinic paper\n",
    "#X_train_list_scaled = np.pad(X_train_list_scaled, pad_width=((0,0),(0,0),(60,60),(0,0)))\n",
    "#X_val_list_scaled = np.pad(X_val_list_scaled, pad_width=((0,0),(0,0),(60,60),(0,0)))\n",
    "\n",
    "\n",
    "#sns.distplot(X_train_list_scaled)\n",
    "\n",
    "#Normalization\n",
    "#X_train_list_scaled = np.log(X_train_list[0])\n",
    "#print(np.mean(X_train_list_scaled))\n",
    "#print(np.std(X_train_list_scaled))\n",
    "\n",
    "#X_val_list_scaled = (X_val_list[0]-np.mean(X_val_list[0]))/370\n",
    "#print(np.mean(X_val_list_scaled))\n",
    "#print(np.std(X_val_list_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_finder = LRFinder(min_lr=1e-4, max_lr=1e-1, steps_per_epoch=np.ceil(X_train_list_scaled.shape[0]/num_batch_size), epochs=1)\n",
    "\n",
    "#model.fit(X_train_list_scaled, y_train_list_scaled, callbacks=[lr_finder])\n",
    "#lr_finder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#changed to Adam\n",
    "#took out one cycle\n",
    "\n",
    "\n",
    "#for train_index, val_index in group_kfold.split(ecg_X_k, ecg_y_k, ecg_id_k):  #for full 10-fold cross validation\n",
    "\n",
    "for i in range(1):\n",
    "    \n",
    "    K.clear_session()\n",
    "\n",
    "    #adamw = AdamW(weight_decay=weight_d)\n",
    "    adam = optimizers.Adam()  #, clipnorm=1.)\n",
    "    #rmsprop = optimizers.RMSprop(learning_rate= set_learning_rate, clipnorm=1.)\n",
    "    #sgdopt = optimizers.SGD(learning_rate= set_learning_rate, clipnorm=1.)\n",
    "    #reduce_lr = tf.keras.callbacks.ReduceLROnPlateau()\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    #model.compile(optimizer= adamw, loss = set_loss, metrics= set_metrics)\n",
    "    model.compile(optimizer= adam, loss = set_loss, metrics= set_metrics)\n",
    "    model.load_weights('temp4.h5')\n",
    "    #model = load_model(filename_base + '_fold_' + str(i) + '.h5', custom_objects={'AdamW':adamw})\n",
    "    \n",
    "    \n",
    "    #corr_check = corr_checkpointer(X_val_list_scaled, y_val_list_scaled, filepath= (filename_base + '_fold_' + str(i) + '.h5'))\n",
    "    #early_stopper = corr_early_stopping(min_epochs=stopping_min_epochs, mode='max', patience=num_patience, verbose=1)\n",
    "    \n",
    "    onecyc = OneCycle(lr_range=(0.001, 0.01), momentum_range=(0.95,0.85))\n",
    "    \n",
    "    earlystopper = EarlyStopping(monitor = 'val_accuracy', mode='max', patience=num_patience, verbose=1)  #restore_best_weights not available\n",
    "    checkpointer = ModelCheckpoint(filename_base + '_fold_' + str(i) + '.h5', monitor = 'val_accuracy', mode='max', verbose=1, save_best_only = True)\n",
    "\n",
    "\n",
    "    \n",
    "    #reduce_lr = tf.keras.callbacks.ReduceLROnPlateau()\n",
    "    #model = load_model(filename_base + '_fold_' + str(i) + '.h5')\n",
    "\n",
    "    history = model.fit(X_train_list_scaled, y_train_list_scaled, validation_data = (X_val_list_scaled, y_val_list_scaled), batch_size = num_batch_size, epochs=num_epochs, callbacks=[checkpointer, onecyc, earlystopper])   #[corr_check, onecyc, early_stopper]) #tensorboard_callback])\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i=0\n",
    "#filename_base = 'trained_' + suffix + '_2'\n",
    "\n",
    "onecyc = OneCycle(lr_range=(0.0005, 0.005), momentum_range=(0.95,0.85))\n",
    "    \n",
    "#earlystopper = EarlyStopping(monitor = 'val_accuracy', mode='max', patience=num_patience, verbose=1)  #restore_best_weights not available\n",
    "checkpointer = ModelCheckpoint(filename_base + '_fold_' + str(i) + '_round2.h5', monitor = 'val_accuracy', mode='max', verbose=1, save_best_only = True)\n",
    "\n",
    "\n",
    "\n",
    "model = load_model(filename_base + '_fold_' + str(i) + '.h5')\n",
    "                   \n",
    "history = model.fit(X_train_list_scaled, y_train_list_scaled, validation_data = (X_val_list_scaled, y_val_list_scaled), batch_size = num_batch_size, epochs=num_epochs, callbacks=[checkpointer, onecyc, earlystopper])   #[corr_check, onecyc, early_stopper]) #tensorboard_callback])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i=0\n",
    "#filename_base = 'trained_' + suffix + '_2'\n",
    "\n",
    "onecyc = OneCycle(lr_range=(0.0001, 0.001), momentum_range=(0.95,0.85))\n",
    "    \n",
    "#earlystopper = EarlyStopping(monitor = 'val_accuracy', mode='max', patience=num_patience, verbose=1)  #restore_best_weights not available\n",
    "checkpointer = ModelCheckpoint(filename_base + '_fold_' + str(i) + '_round3.h5', monitor = 'val_accuracy', mode='max', verbose=1, save_best_only = True)\n",
    "\n",
    "\n",
    "\n",
    "model = load_model(filename_base + '_fold_' + str(i) + '_round2.h5')\n",
    "                   \n",
    "history = model.fit(X_train_list_scaled, y_train_list_scaled, validation_data = (X_val_list_scaled, y_val_list_scaled), batch_size = num_batch_size, epochs=num_epochs, callbacks=[checkpointer, onecyc, earlystopper])   #[corr_check, onecyc, early_stopper]) #tensorboard_callback])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "current_time2 = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Start time: \", current_time)\n",
    "print(\"End time: \", current_time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(filename_base + '_fold_' + str(i) + '.h5')  #, custom_objects={'AdamW':adamw})\n",
    "preds_val = model.predict(X_val_list_scaled, verbose=1, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for test sets\n",
    "\n",
    "y_train_list_scaled = ecg_y_k\n",
    "y_val_list_scaled = ecg_y_k\n",
    "\n",
    "X_train_list_scaled = np.divide(ecg_X_k,4000)\n",
    "X_val_list_scaled = np.divide(ecg_X_k, 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_list_scaled = y_train_list[0]\n",
    "y_val_list_scaled = y_val_list[0]\n",
    "\n",
    "X_train_list_scaled = np.divide(X_train_list[0],4000)\n",
    "X_val_list_scaled = np.divide(X_val_list[0], 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for diabetes- 'trained_100k_diabetes_1_fold_0_round2.h5'\n",
    "#for sex single lead- trained_corrected_60k_sex_2_fold_0_round3.h5\n",
    "#for sex 12-lead- 'trained_corrected_60k_sex_1_12lead_fold_0.h5'\n",
    "#for htn- 'trained_100k_htn_2_fold_0_round2.h5'\n",
    "#for benzo- 'trained_100k_benzo_1_fold_0_round2.h5'\n",
    "#for ssri- 'trained_100k_ssri_1_fold_0_round3.h5'\n",
    "\n",
    "model = load_model('trained_corrected_60k_sex_1_12lead_fold_0.h5')  #, custom_objects={'AdamW':adamw})\n",
    "preds_val = model.predict(X_val_list_scaled, verbose=1, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_val_list_scaled, preds_val)\n",
    "roc_auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lw = 2\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "ax.plot(fpr, tpr, color='#732673',\n",
    "         lw=lw, label='AUROC = %0.2f' % roc_auc)\n",
    "ax.plot([0, 1], [0, 1], color='#505050', lw=lw, linestyle='--')\n",
    "ax.set_xlim([-0.002, 1.0])\n",
    "ax.set_ylim([-0.002, 1.05])\n",
    "#ax.set_title('Diabetes Using AMC Single-Lead ECGs', fontsize=36)\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_facecolor('#f7f7f7')\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.spines['left'].set_position(('outward', 5))\n",
    "ax.spines['bottom'].set_position(('outward', 5))\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.grid(b=True, which='major', color='white', linestyle='-', linewidth=3.0)\n",
    "#ax.grid(b=True, which='minor', color='white', linestyle='-', linewidth=1)\n",
    "ax.set_axisbelow(True)\n",
    "ax.legend(loc='best', frameon=False)\n",
    "plt.minorticks_on()\n",
    "plt.rcParams.update({'font.size': 34})\n",
    "plt.show()\n",
    "outfile = \"test.png\"\n",
    "fig.savefig(outfile)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "lw = 1.5\n",
    "plt.plot(fpr, tpr,\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='black', lw=lw, linestyle='--')\n",
    "plt.xlim([0, 1.0])\n",
    "plt.ylim([0, 1.0])\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel('False Positive Rate', fontsize=26)\n",
    "plt.ylabel('True Positive Rate', fontsize=26)\n",
    "plt.title('Sex Classification Using AMC Single-Lead ECGs', fontsize=28)\n",
    "plt.legend(loc=\"lower right\", fontsize=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i=0\n",
    "#filename_base = 'trained_' + suffix + '_2'\n",
    "\n",
    "onecyc = OneCycle(lr_range=(0.00005, 0.0005), momentum_range=(0.95,0.85))\n",
    "    \n",
    "#earlystopper = EarlyStopping(monitor = 'val_accuracy', mode='max', patience=num_patience, verbose=1)  #restore_best_weights not available\n",
    "#checkpointer = ModelCheckpoint(filename_base + '_fold_' + str(i) + '.h5', monitor = 'val_accuracy', mode='max', verbose=1, save_best_only = True)\n",
    "\n",
    "\n",
    "\n",
    "model = load_model(filename_base + '_fold_' + str(i) + '.h5')\n",
    "                   \n",
    "history = model.fit(X_train_list_scaled, y_train_list_scaled, validation_data = (X_val_list_scaled, y_val_list_scaled), batch_size = num_batch_size, epochs=num_epochs, callbacks=[checkpointer, onecyc, earlystopper])   #[corr_check, onecyc, early_stopper]) #tensorboard_callback])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(filename_base + '_fold_' + str(i) + '.h5')  #, custom_objects={'AdamW':adamw})\n",
    "preds_val = model.predict(X_val_list_scaled, verbose=1, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_val_list_scaled, preds_val)\n",
    "roc_auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr,\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decreasing kernel at end; adding weight norm to last dilation block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also good (3)\n",
    "inputs = Input((in_neurons, 1))\n",
    "\n",
    "c1 = Conv1D(16,4, padding='same', use_bias=False, strides=2, kernel_initializer= 'he_normal') (inputs)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Activation('relu') (c1)\n",
    "c1 = Dropout(0.1) (c1)\n",
    "#c1 = SpatialDropout1D(0.2) (c1)\n",
    "\n",
    "c1 = Conv1D(32,8, padding='same', use_bias=False, strides=2, kernel_initializer= 'he_normal') (c1)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Activation('relu') (c1)\n",
    "c1 = SpatialDropout1D(0.1) (c1)\n",
    "\n",
    "s1 = Conv1D(16, 1, padding='same') (c1)\n",
    "s1 = BatchNormalization() (s1)\n",
    "\n",
    "#c2 = Conv1D(32,8, padding='same', dilation_rate=2, kernel_initializer= 'he_normal') (c1)\n",
    "c2 = WeightNormalization(Conv1D(32,8, padding='same', dilation_rate=2, kernel_initializer= 'he_normal')) (c1)\n",
    "c2 = Activation('relu') (c2)\n",
    "c2 = SpatialDropout1D(0.3) (c2)\n",
    "#c2 = Conv1D(32,8, padding='same', dilation_rate=4, kernel_initializer= 'he_normal') (c2)\n",
    "c2 = WeightNormalization(Conv1D(32,8, padding='same', dilation_rate=4, kernel_initializer= 'he_normal')) (c2)\n",
    "#c2 = Activation('relu') (c2)\n",
    "c2 = Activation('relu') (c1+c2)\n",
    "c2 = SpatialDropout1D(0.3) (c2)\n",
    "\n",
    "#s2 = Conv1D(16, 1, padding='same', use_bias=False) (c2)\n",
    "#s2 = BatchNormalization() (s2)\n",
    "\n",
    "#c3 = Conv1D(16,8, padding='same', dilation_rate=2, kernel_initializer= 'he_normal') (c2)\n",
    "c3 = WeightNormalization(Conv1D(16,6, padding='same', dilation_rate=2, kernel_initializer= 'he_normal')) (c2)\n",
    "c3 = Activation('relu') (c3)\n",
    "c3 = SpatialDropout1D(0.3) (c3)\n",
    "#c3 = Conv1D(16,8, padding='same', dilation_rate=4, kernel_initializer= 'he_normal') (c3)\n",
    "c3 = WeightNormalization(Conv1D(16,6, padding='same', dilation_rate=4, kernel_initializer= 'he_normal')) (c3)\n",
    "c3 = Activation('relu') (s1+c3)\n",
    "c3 = SpatialDropout1D(0.3) (c3)\n",
    "\n",
    "#c3 = Conv1D(16,8, padding='same', dilation_rate=2, kernel_initializer= 'he_normal') (c3)\n",
    "c3 = WeightNormalization(Conv1D(16,4, padding='same', dilation_rate=2, kernel_initializer= 'he_normal')) (c2)\n",
    "c3 = Activation('relu') (c3)\n",
    "c3 = SpatialDropout1D(0.3) (c3)\n",
    "#c3 = Conv1D(16,8, padding='same', dilation_rate=4, kernel_initializer= 'he_normal') (c3)\n",
    "c3 = WeightNormalization(Conv1D(16,4, padding='same', dilation_rate=4, kernel_initializer= 'he_normal')) (c3)\n",
    "c3 = Activation('relu') (s1+c3)\n",
    "c3 = SpatialDropout1D(0.3) (c3)\n",
    "\n",
    "c7 = Conv1D(1, 3, use_bias=False, kernel_initializer= 'he_normal') (c3)\n",
    "c7 = BatchNormalization() (c7)\n",
    "c7 = Activation('relu') (c7)\n",
    "c7 = Dropout(0.5) (c7)\n",
    "\n",
    "#c7 = Conv1D(64, 3, use_bias = False) (c7)\n",
    "#c7 = BatchNormalization() (c7)\n",
    "#c7 = Activation('relu') (c7)\n",
    "\n",
    "#final = GlobalAvgPool1D() (c4)\n",
    "\n",
    "#c6 = Dropout(0.1) (c6)\n",
    "#final = Dense(1, activation='relu') (c7)\n",
    "final = Flatten() (c7)\n",
    "#final = Dropout(0.05) (final)\n",
    "#final = Dense(10, activation='relu') (final)\n",
    "#final = Dropout(0.2) (final)\n",
    "outputs = Dense(1) (final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_loss = 'mean_squared_error'\n",
    "set_metrics = ['mean_squared_error']\n",
    "\n",
    "num_batch_size=1024\n",
    "num_epochs=500\n",
    "num_patience=200\n",
    "stopping_min_epochs=400\n",
    "weight_d = 0 #0.000001  \n",
    "\n",
    "adamw = AdamW(weight_decay=weight_d)\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[outputs])\n",
    "model.compile(optimizer= adamw, loss = set_loss, metrics= ['mean_squared_error'])\n",
    "model.save_weights('temp4.h5')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ocp = OneCycle(lr_range=(0.0007, 0.007), momentum_range=(0.94,0.85))\n",
    "#ocp.test_run(128)\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_list_scaled = np.multiply(np.sign(X_train_list[0]), np.log(np.abs(X_train_list[0])+1)) #log-modulus transform- https://blogs.sas.com/content/iml/2014/07/14/log-transformation-of-pos-neg.html\n",
    "#X_val_list_scaled = np.multiply(np.sign(X_val_list[0]), np.log(np.abs(X_val_list[0])+1))\n",
    "X_train_list_scaled = np.divide(X_train_list[0],4000)\n",
    "X_val_list_scaled = np.divide(X_val_list[0], 4000)\n",
    "y_train_list_scaled = np.multiply((y_train_list[0]), 4)\n",
    "y_val_list_scaled = np.multiply((y_val_list[0]), 4)\n",
    "#sns.distplot(X_train_list_scaled)\n",
    "\n",
    "#Normalization\n",
    "#X_train_list_scaled = np.log(X_train_list[0])\n",
    "#print(np.mean(X_train_list_scaled))\n",
    "#print(np.std(X_train_list_scaled))\n",
    "\n",
    "#X_val_list_scaled = (X_val_list[0]-np.mean(X_val_list[0]))/370\n",
    "#print(np.mean(X_val_list_scaled))\n",
    "#print(np.std(X_val_list_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_finder = LRFinder(min_lr=1e-3, max_lr=0.1, steps_per_epoch=np.ceil(3277/num_batch_size), epochs=3)\n",
    "\n",
    "#model.fit(X_train_list_scaled, y_train_list_scaled, callbacks=[lr_finder])\n",
    "#lr_finder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_epochs = []\n",
    "mses = []\n",
    "val_mses = []\n",
    "mse_hists = []\n",
    "val_mse_hists = []\n",
    "spearmans = []\n",
    "pearsons = []\n",
    "mean_aes = []\n",
    "median_aes = []\n",
    "\n",
    "#for train_index, val_index in group_kfold.split(ecg_X_k, ecg_y_k, ecg_id_k):  #for full 10-fold cross validation\n",
    "\n",
    "for i in range(1):\n",
    "    \n",
    "    K.clear_session()\n",
    "\n",
    "    adamw = AdamW(weight_decay=weight_d)\n",
    "    #adam = optimizers.Adam(learning_rate= set_learning_rate, clipnorm=1.)\n",
    "    #rmsprop = optimizers.RMSprop(learning_rate= set_learning_rate, clipnorm=1.)\n",
    "    #sgdopt = optimizers.SGD(learning_rate= set_learning_rate, clipnorm=1.)\n",
    "    #reduce_lr = tf.keras.callbacks.ReduceLROnPlateau()\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(optimizer= adamw, loss = set_loss, metrics= set_metrics)\n",
    "    model.load_weights('temp4.h5')\n",
    "    \n",
    "    \n",
    "    \n",
    "    corr_check = corr_checkpointer(X_val_list_scaled, y_val_list_scaled, filepath= (filename_base + '_fold_' + str(i) + '.h5'))\n",
    "    early_stopper = corr_early_stopping(min_epochs=stopping_min_epochs, mode='max', patience=num_patience, verbose=1)\n",
    "    \n",
    "    onecyc = OneCycle(lr_range=(0.001, 0.01), momentum_range=(0.95,0.85))\n",
    "    \n",
    "    #earlystopper = EarlyStopping(monitor = 'val_mean_squared_error', mode='min', patience=num_patience, verbose=1)  #restore_best_weights not available\n",
    "    #checkpointer = ModelCheckpoint(filename_base + '_fold_' + str(i) + '.h5', monitor = 'val_mean_squared_error', mode='min', verbose=1, save_best_only = True)\n",
    "\n",
    "\n",
    "    \n",
    "    #reduce_lr = tf.keras.callbacks.ReduceLROnPlateau()\n",
    "\n",
    "\n",
    "    history = model.fit(X_train_list_scaled, y_train_list_scaled, validation_data = (X_val_list_scaled, y_val_list_scaled), batch_size = num_batch_size, epochs=num_epochs, callbacks=[corr_check, onecyc, early_stopper]) #tensorboard_callback])\n",
    "    \n",
    "    best_epoch = len(history.history['val_mean_squared_error']) - num_patience\n",
    "    mse_model = history.history['mean_squared_error'][-(num_patience + 1)]\n",
    "    val_mse_model = history.history['val_mean_squared_error'][-(num_patience + 1)]\n",
    "    mse_hist = history.history['mean_squared_error']\n",
    "    val_mse_hist = history.history['val_mean_squared_error']\n",
    "\n",
    "    model = load_model(filename_base + '_fold_' + str(i) + '.h5', custom_objects={'AdamW':adamw})\n",
    "    preds_val_initial = model.predict(X_val_list_scaled, verbose=1, batch_size=1024)\n",
    "    preds_val_initial = preds_val_initial.reshape(preds_val_initial.shape[0])\n",
    "    \n",
    "    #****\n",
    "    y_val_list_scaled = np.divide((y_val_list_scaled), 4)\n",
    "    preds_val = np.divide((preds_val_initial), 4)\n",
    "    #y_train_list_scaled = np.divide((y_train_list[0]-18),41) - 1\n",
    "    \n",
    "    #preds_val = np.multipy((preds_val_initial), 82) + 18\n",
    "    #y_train_list_scaled = np.multipiy((y_train_list_scaled), 82) +18\n",
    "\n",
    "    spearman = stats.spearmanr(preds_val, y_val_list_scaled)\n",
    "    pearson = stats.pearsonr(preds_val, y_val_list_scaled)\n",
    "    \n",
    "    mean_ae = np.mean(abs(preds_val - y_val_list_scaled))\n",
    "    median_ae = np.median(abs(preds_val - y_val_list_scaled))\n",
    "    \n",
    "    \n",
    "    best_epochs.append(best_epoch)\n",
    "    mses.append(mse_model)\n",
    "    val_mses.append(val_mse_model)\n",
    "    mse_hists.append(mse_hist)\n",
    "    val_mse_hists.append(val_mse_hist)\n",
    "    spearmans.append(spearman)\n",
    "    pearsons.append(pearson)\n",
    "    mean_aes.append(mean_ae)\n",
    "    median_aes.append(median_ae)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best epoch: ', best_epochs)\n",
    "print('MSE: ', mses)\n",
    "print('Val MSE: ', val_mses)\n",
    "print('Spearman: ', spearmans)\n",
    "print('Pearson: ', pearsons)\n",
    "print('Mean abs error: ', mean_aes)\n",
    "print('Median abs error: ', median_aes)\n",
    "\n",
    "\n",
    "#print('Mean MSE: ', np.mean(mses))\n",
    "#print('Mean val_MSE: ', np.mean(val_mses))\n",
    "#print('Mean Spearman r: ', [np.mean(i) for i in zip(*spearmans)])\n",
    "#print('Mean Pearson r: ', [np.mean(i) for i in zip(*pearsons)])\n",
    "#print('Mean of Mean abs error: ', np.mean(mean_aes))\n",
    "#print('Mean of Median abs error: ', np.mean(median_aes))\n",
    "\n",
    "\n",
    "mean_mse = [np.mean(i) for i in zip(*mse_hists)]\n",
    "mean_val_mse = [np.mean(i) for i in zip(*val_mse_hists)]\n",
    "plt.plot(mean_mse[3:])\n",
    "plt.plot(mean_val_mse[3:])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#for j in range(len(mse_hists)):\n",
    "#    plt.plot(mse_hists[j])\n",
    "#    plt.plot(val_mse_hists[j])\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "current_time2 = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Start time: \", current_time)\n",
    "print(\"End time: \", current_time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mean_mse[200:1000])\n",
    "plt.plot(mean_val_mse[200:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bland_altman_plot(preds_val, y_val_list_scaled, scatter_kwds={'s':10}, title='MGH/BWH 12-Lead ECGs (10 secs)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7,7)) #(1, 1, figsize=(12, 12))\n",
    "    \n",
    "ax.set_title('MGH/BWH Single-Lead ECGs (10 secs)', fontsize=14)\n",
    "ax.scatter(y_val_list_scaled, preds_val, s=10)\n",
    "\n",
    "ax.set(xlim = [0, 110], xticks = np.arange(0, 110, 10),\n",
    "       ylim = [0, 110], yticks = np.arange(0, 110, 10),\n",
    "       xlabel = 'Actual Age',\n",
    "       ylabel = 'Predicted Age')\n",
    "ax.plot([0, 110], [0,110], alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keepign weight norm of last diilation block increasing kernel again to 8; increasing filters with a max pool near end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also good (3)\n",
    "inputs = Input((in_neurons, 1))\n",
    "\n",
    "c1 = Conv1D(16,4, padding='same', use_bias=False, strides=2, kernel_initializer= 'he_normal') (inputs)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Activation('relu') (c1)\n",
    "c1 = Dropout(0.1) (c1)\n",
    "#c1 = SpatialDropout1D(0.2) (c1)\n",
    "\n",
    "c1 = Conv1D(32,8, padding='same', use_bias=False, strides=2, kernel_initializer= 'he_normal') (c1)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Activation('relu') (c1)\n",
    "c1 = SpatialDropout1D(0.1) (c1)\n",
    "\n",
    "s1 = Conv1D(32, 1, padding='same') (c1)\n",
    "s1 = BatchNormalization() (s1)\n",
    "\n",
    "#c2 = Conv1D(32,8, padding='same', dilation_rate=2, kernel_initializer= 'he_normal') (c1)\n",
    "c2 = WeightNormalization(Conv1D(32,8, padding='same', dilation_rate=2, kernel_initializer= 'he_normal')) (c1)\n",
    "c2 = Activation('relu') (c2)\n",
    "c2 = SpatialDropout1D(0.3) (c2)\n",
    "#c2 = Conv1D(32,8, padding='same', dilation_rate=4, kernel_initializer= 'he_normal') (c2)\n",
    "c2 = WeightNormalization(Conv1D(32,8, padding='same', dilation_rate=4, kernel_initializer= 'he_normal')) (c2)\n",
    "#c2 = Activation('relu') (c2)\n",
    "c2 = Activation('relu') (c1+c2)\n",
    "c2 = SpatialDropout1D(0.3) (c2)\n",
    "\n",
    "#s2 = Conv1D(16, 1, padding='same', use_bias=False) (c2)\n",
    "#s2 = BatchNormalization() (s2)\n",
    "\n",
    "#c3 = Conv1D(16,8, padding='same', dilation_rate=2, kernel_initializer= 'he_normal') (c2)\n",
    "c3 = WeightNormalization(Conv1D(32,8, padding='same', dilation_rate=2, kernel_initializer= 'he_normal')) (c2)\n",
    "c3 = Activation('relu') (c3)\n",
    "c3 = SpatialDropout1D(0.3) (c3)\n",
    "#c3 = Conv1D(16,8, padding='same', dilation_rate=4, kernel_initializer= 'he_normal') (c3)\n",
    "c3 = WeightNormalization(Conv1D(32,8, padding='same', dilation_rate=4, kernel_initializer= 'he_normal')) (c3)\n",
    "c3 = Activation('relu') (s1+c3)\n",
    "c3 = SpatialDropout1D(0.3) (c3)\n",
    "\n",
    "#c3 = Conv1D(16,8, padding='same', dilation_rate=2, kernel_initializer= 'he_normal') (c3)\n",
    "c3 = WeightNormalization(Conv1D(32,8, padding='same', dilation_rate=2, kernel_initializer= 'he_normal')) (c2)\n",
    "c3 = Activation('relu') (c3)\n",
    "c3 = SpatialDropout1D(0.3) (c3)\n",
    "#c3 = Conv1D(16,8, padding='same', dilation_rate=4, kernel_initializer= 'he_normal') (c3)\n",
    "c3 = WeightNormalization(Conv1D(32,8, padding='same', dilation_rate=4, kernel_initializer= 'he_normal')) (c3)\n",
    "c3 = Activation('relu') (s1+c3)\n",
    "c3 = SpatialDropout1D(0.3) (c3)\n",
    "\n",
    "c7 = Conv1D(16, 3, use_bias=False, kernel_initializer= 'he_normal') (c3)\n",
    "c7 = BatchNormalization() (c7)\n",
    "c7 = Activation('relu') (c7)\n",
    "c7 = Dropout(0.5) (c7)\n",
    "c7 = MaxPooling1D(1, strides=2) (c7)\n",
    "\n",
    "#c7 = Conv1D(64, 3, use_bias = False) (c7)\n",
    "#c7 = BatchNormalization() (c7)\n",
    "#c7 = Activation('relu') (c7)\n",
    "\n",
    "#final = GlobalAvgPool1D() (c4)\n",
    "\n",
    "#c6 = Dropout(0.1) (c6)\n",
    "#final = Dense(1, activation='relu') (c7)\n",
    "final = Flatten() (c7)\n",
    "#final = Dropout(0.05) (final)\n",
    "#final = Dense(10, activation='relu') (final)\n",
    "#final = Dropout(0.2) (final)\n",
    "outputs = Dense(1) (final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_loss = 'mean_squared_error'\n",
    "set_metrics = ['mean_squared_error']\n",
    "\n",
    "num_batch_size=1024\n",
    "num_epochs=500\n",
    "num_patience=200\n",
    "stopping_min_epochs=400\n",
    "weight_d = 0 #0.000001  \n",
    "\n",
    "adamw = AdamW(weight_decay=weight_d)\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[outputs])\n",
    "model.compile(optimizer= adamw, loss = set_loss, metrics= ['mean_squared_error'])\n",
    "model.save_weights('temp4.h5')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ocp = OneCycle(lr_range=(0.0007, 0.007), momentum_range=(0.94,0.85))\n",
    "#ocp.test_run(128)\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_list_scaled = np.multiply(np.sign(X_train_list[0]), np.log(np.abs(X_train_list[0])+1)) #log-modulus transform- https://blogs.sas.com/content/iml/2014/07/14/log-transformation-of-pos-neg.html\n",
    "#X_val_list_scaled = np.multiply(np.sign(X_val_list[0]), np.log(np.abs(X_val_list[0])+1))\n",
    "X_train_list_scaled = np.divide(X_train_list[0],4000)\n",
    "X_val_list_scaled = np.divide(X_val_list[0], 4000)\n",
    "y_train_list_scaled = np.multiply((y_train_list[0]), 4)\n",
    "y_val_list_scaled = np.multiply((y_val_list[0]), 4)\n",
    "#sns.distplot(X_train_list_scaled)\n",
    "\n",
    "#Normalization\n",
    "#X_train_list_scaled = np.log(X_train_list[0])\n",
    "#print(np.mean(X_train_list_scaled))\n",
    "#print(np.std(X_train_list_scaled))\n",
    "\n",
    "#X_val_list_scaled = (X_val_list[0]-np.mean(X_val_list[0]))/370\n",
    "#print(np.mean(X_val_list_scaled))\n",
    "#print(np.std(X_val_list_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_finder = LRFinder(min_lr=1e-3, max_lr=0.1, steps_per_epoch=np.ceil(3277/num_batch_size), epochs=3)\n",
    "\n",
    "#model.fit(X_train_list_scaled, y_train_list_scaled, callbacks=[lr_finder])\n",
    "#lr_finder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_epochs = []\n",
    "mses = []\n",
    "val_mses = []\n",
    "mse_hists = []\n",
    "val_mse_hists = []\n",
    "spearmans = []\n",
    "pearsons = []\n",
    "mean_aes = []\n",
    "median_aes = []\n",
    "\n",
    "#for train_index, val_index in group_kfold.split(ecg_X_k, ecg_y_k, ecg_id_k):  #for full 10-fold cross validation\n",
    "\n",
    "for i in range(1):\n",
    "    \n",
    "    K.clear_session()\n",
    "\n",
    "    adamw = AdamW(weight_decay=weight_d)\n",
    "    #adam = optimizers.Adam(learning_rate= set_learning_rate, clipnorm=1.)\n",
    "    #rmsprop = optimizers.RMSprop(learning_rate= set_learning_rate, clipnorm=1.)\n",
    "    #sgdopt = optimizers.SGD(learning_rate= set_learning_rate, clipnorm=1.)\n",
    "    #reduce_lr = tf.keras.callbacks.ReduceLROnPlateau()\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(optimizer= adamw, loss = set_loss, metrics= set_metrics)\n",
    "    model.load_weights('temp4.h5')\n",
    "    \n",
    "    \n",
    "    \n",
    "    corr_check = corr_checkpointer(X_val_list_scaled, y_val_list_scaled, filepath= (filename_base + '_fold_' + str(i) + '.h5'))\n",
    "    early_stopper = corr_early_stopping(min_epochs=stopping_min_epochs, mode='max', patience=num_patience, verbose=1)\n",
    "    \n",
    "    onecyc = OneCycle(lr_range=(0.001, 0.01), momentum_range=(0.95,0.85))\n",
    "    \n",
    "    #earlystopper = EarlyStopping(monitor = 'val_mean_squared_error', mode='min', patience=num_patience, verbose=1)  #restore_best_weights not available\n",
    "    #checkpointer = ModelCheckpoint(filename_base + '_fold_' + str(i) + '.h5', monitor = 'val_mean_squared_error', mode='min', verbose=1, save_best_only = True)\n",
    "\n",
    "\n",
    "    \n",
    "    #reduce_lr = tf.keras.callbacks.ReduceLROnPlateau()\n",
    "\n",
    "\n",
    "    history = model.fit(X_train_list_scaled, y_train_list_scaled, validation_data = (X_val_list_scaled, y_val_list_scaled), batch_size = num_batch_size, epochs=num_epochs, callbacks=[corr_check, onecyc, early_stopper]) #tensorboard_callback])\n",
    "    \n",
    "    best_epoch = len(history.history['val_mean_squared_error']) - num_patience\n",
    "    mse_model = history.history['mean_squared_error'][-(num_patience + 1)]\n",
    "    val_mse_model = history.history['val_mean_squared_error'][-(num_patience + 1)]\n",
    "    mse_hist = history.history['mean_squared_error']\n",
    "    val_mse_hist = history.history['val_mean_squared_error']\n",
    "\n",
    "    model = load_model(filename_base + '_fold_' + str(i) + '.h5', custom_objects={'AdamW':adamw})\n",
    "    preds_val_initial = model.predict(X_val_list_scaled, verbose=1, batch_size=1024)\n",
    "    preds_val_initial = preds_val_initial.reshape(preds_val_initial.shape[0])\n",
    "    \n",
    "    #****\n",
    "    y_val_list_scaled = np.divide((y_val_list_scaled), 4)\n",
    "    preds_val = np.divide((preds_val_initial), 4)\n",
    "    #y_train_list_scaled = np.divide((y_train_list[0]-18),41) - 1\n",
    "    \n",
    "    #preds_val = np.multipy((preds_val_initial), 82) + 18\n",
    "    #y_train_list_scaled = np.multipiy((y_train_list_scaled), 82) +18\n",
    "\n",
    "    spearman = stats.spearmanr(preds_val, y_val_list_scaled)\n",
    "    pearson = stats.pearsonr(preds_val, y_val_list_scaled)\n",
    "    \n",
    "    mean_ae = np.mean(abs(preds_val - y_val_list_scaled))\n",
    "    median_ae = np.median(abs(preds_val - y_val_list_scaled))\n",
    "    \n",
    "    \n",
    "    best_epochs.append(best_epoch)\n",
    "    mses.append(mse_model)\n",
    "    val_mses.append(val_mse_model)\n",
    "    mse_hists.append(mse_hist)\n",
    "    val_mse_hists.append(val_mse_hist)\n",
    "    spearmans.append(spearman)\n",
    "    pearsons.append(pearson)\n",
    "    mean_aes.append(mean_ae)\n",
    "    median_aes.append(median_ae)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best epoch: ', best_epochs)\n",
    "print('MSE: ', mses)\n",
    "print('Val MSE: ', val_mses)\n",
    "print('Spearman: ', spearmans)\n",
    "print('Pearson: ', pearsons)\n",
    "print('Mean abs error: ', mean_aes)\n",
    "print('Median abs error: ', median_aes)\n",
    "\n",
    "\n",
    "#print('Mean MSE: ', np.mean(mses))\n",
    "#print('Mean val_MSE: ', np.mean(val_mses))\n",
    "#print('Mean Spearman r: ', [np.mean(i) for i in zip(*spearmans)])\n",
    "#print('Mean Pearson r: ', [np.mean(i) for i in zip(*pearsons)])\n",
    "#print('Mean of Mean abs error: ', np.mean(mean_aes))\n",
    "#print('Mean of Median abs error: ', np.mean(median_aes))\n",
    "\n",
    "\n",
    "mean_mse = [np.mean(i) for i in zip(*mse_hists)]\n",
    "mean_val_mse = [np.mean(i) for i in zip(*val_mse_hists)]\n",
    "plt.plot(mean_mse[3:])\n",
    "plt.plot(mean_val_mse[3:])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#for j in range(len(mse_hists)):\n",
    "#    plt.plot(mse_hists[j])\n",
    "#    plt.plot(val_mse_hists[j])\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "current_time2 = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Start time: \", current_time)\n",
    "print(\"End time: \", current_time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mean_mse[200:1000])\n",
    "plt.plot(mean_val_mse[200:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bland_altman_plot(preds_val, y_val_list_scaled, scatter_kwds={'s':10}, title='MGH/BWH 12-Lead ECGs (10 secs)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7,7)) #(1, 1, figsize=(12, 12))\n",
    "    \n",
    "ax.set_title('MGH/BWH Single-Lead ECGs (10 secs)', fontsize=14)\n",
    "ax.scatter(y_val_list_scaled, preds_val, s=10)\n",
    "\n",
    "ax.set(xlim = [0, 110], xticks = np.arange(0, 110, 10),\n",
    "       ylim = [0, 110], yticks = np.arange(0, 110, 10),\n",
    "       xlabel = 'Actual Age',\n",
    "       ylabel = 'Predicted Age')\n",
    "ax.plot([0, 110], [0,110], alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#again weight norm at end, but also avg pool before last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also good (3)\n",
    "inputs = Input((in_neurons, 1))\n",
    "\n",
    "c1 = Conv1D(16,4, padding='same', use_bias=False, strides=2, kernel_initializer= 'he_normal') (inputs)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Activation('relu') (c1)\n",
    "c1 = Dropout(0.1) (c1)\n",
    "#c1 = SpatialDropout1D(0.2) (c1)\n",
    "\n",
    "c1 = Conv1D(32,8, padding='same', use_bias=False, strides=2, kernel_initializer= 'he_normal') (c1)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Activation('relu') (c1)\n",
    "c1 = SpatialDropout1D(0.1) (c1)\n",
    "\n",
    "s1 = Conv1D(16, 1, padding='same') (c1)\n",
    "s1 = BatchNormalization() (s1)\n",
    "\n",
    "#c2 = Conv1D(32,8, padding='same', dilation_rate=2, kernel_initializer= 'he_normal') (c1)\n",
    "c2 = WeightNormalization(Conv1D(32,8, padding='same', dilation_rate=2, kernel_initializer= 'he_normal')) (c1)\n",
    "c2 = Activation('relu') (c2)\n",
    "c2 = SpatialDropout1D(0.3) (c2)\n",
    "#c2 = Conv1D(32,8, padding='same', dilation_rate=4, kernel_initializer= 'he_normal') (c2)\n",
    "c2 = WeightNormalization(Conv1D(32,8, padding='same', dilation_rate=4, kernel_initializer= 'he_normal')) (c2)\n",
    "#c2 = Activation('relu') (c2)\n",
    "c2 = Activation('relu') (c1+c2)\n",
    "c2 = SpatialDropout1D(0.3) (c2)\n",
    "\n",
    "#s2 = Conv1D(16, 1, padding='same', use_bias=False) (c2)\n",
    "#s2 = BatchNormalization() (s2)\n",
    "\n",
    "#c3 = Conv1D(16,8, padding='same', dilation_rate=2, kernel_initializer= 'he_normal') (c2)\n",
    "c3 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=2, kernel_initializer= 'he_normal')) (c2)\n",
    "c3 = Activation('relu') (c3)\n",
    "c3 = SpatialDropout1D(0.3) (c3)\n",
    "#c3 = Conv1D(16,8, padding='same', dilation_rate=4, kernel_initializer= 'he_normal') (c3)\n",
    "c3 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=4, kernel_initializer= 'he_normal')) (c3)\n",
    "c3 = Activation('relu') (s1+c3)\n",
    "c3 = SpatialDropout1D(0.3) (c3)\n",
    "\n",
    "#c3 = Conv1D(16,8, padding='same', dilation_rate=2, kernel_initializer= 'he_normal') (c3)\n",
    "c3 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=2, kernel_initializer= 'he_normal')) (c2)\n",
    "c3 = Activation('relu') (c3)\n",
    "c3 = SpatialDropout1D(0.3) (c3)\n",
    "#c3 = Conv1D(16,8, padding='same', dilation_rate=4, kernel_initializer= 'he_normal') (c3)\n",
    "c3 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=4, kernel_initializer= 'he_normal')) (c3)\n",
    "c3 = Activation('relu') (s1+c3)\n",
    "c3 = SpatialDropout1D(0.3) (c3)\n",
    "\n",
    "c7 = Conv1D(16, 3, use_bias=False, kernel_initializer= 'he_normal') (c3)\n",
    "c7 = BatchNormalization() (c7)\n",
    "c7 = Activation('relu') (c7)\n",
    "c7 = Dropout(0.5) (c7)\n",
    "\n",
    "#c7 = Conv1D(64, 3, use_bias = False) (c7)\n",
    "#c7 = BatchNormalization() (c7)\n",
    "#c7 = Activation('relu') (c7)\n",
    "\n",
    "c7 = GlobalAvgPool1D() (c7)\n",
    "\n",
    "#c6 = Dropout(0.1) (c6)\n",
    "#final = Dense(1, activation='relu') (c7)\n",
    "final = Flatten() (c7)\n",
    "#final = Dropout(0.05) (final)\n",
    "#final = Dense(10, activation='relu') (final)\n",
    "#final = Dropout(0.2) (final)\n",
    "outputs = Dense(1) (final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_loss = 'mean_squared_error'\n",
    "set_metrics = ['mean_squared_error']\n",
    "\n",
    "num_batch_size=1024\n",
    "num_epochs=500\n",
    "num_patience=200\n",
    "stopping_min_epochs=400\n",
    "weight_d = 0 #0.000001  \n",
    "\n",
    "adamw = AdamW(weight_decay=weight_d)\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[outputs])\n",
    "model.compile(optimizer= adamw, loss = set_loss, metrics= ['mean_squared_error'])\n",
    "model.save_weights('temp4.h5')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ocp = OneCycle(lr_range=(0.0007, 0.007), momentum_range=(0.94,0.85))\n",
    "#ocp.test_run(128)\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_list_scaled = np.multiply(np.sign(X_train_list[0]), np.log(np.abs(X_train_list[0])+1)) #log-modulus transform- https://blogs.sas.com/content/iml/2014/07/14/log-transformation-of-pos-neg.html\n",
    "#X_val_list_scaled = np.multiply(np.sign(X_val_list[0]), np.log(np.abs(X_val_list[0])+1))\n",
    "X_train_list_scaled = np.divide(X_train_list[0],4000)\n",
    "X_val_list_scaled = np.divide(X_val_list[0], 4000)\n",
    "y_train_list_scaled = np.multiply((y_train_list[0]), 4)\n",
    "y_val_list_scaled = np.multiply((y_val_list[0]), 4)\n",
    "#sns.distplot(X_train_list_scaled)\n",
    "\n",
    "#Normalization\n",
    "#X_train_list_scaled = np.log(X_train_list[0])\n",
    "#print(np.mean(X_train_list_scaled))\n",
    "#print(np.std(X_train_list_scaled))\n",
    "\n",
    "#X_val_list_scaled = (X_val_list[0]-np.mean(X_val_list[0]))/370\n",
    "#print(np.mean(X_val_list_scaled))\n",
    "#print(np.std(X_val_list_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_finder = LRFinder(min_lr=1e-3, max_lr=0.1, steps_per_epoch=np.ceil(3277/num_batch_size), epochs=3)\n",
    "\n",
    "#model.fit(X_train_list_scaled, y_train_list_scaled, callbacks=[lr_finder])\n",
    "#lr_finder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_epochs = []\n",
    "mses = []\n",
    "val_mses = []\n",
    "mse_hists = []\n",
    "val_mse_hists = []\n",
    "spearmans = []\n",
    "pearsons = []\n",
    "mean_aes = []\n",
    "median_aes = []\n",
    "\n",
    "#for train_index, val_index in group_kfold.split(ecg_X_k, ecg_y_k, ecg_id_k):  #for full 10-fold cross validation\n",
    "\n",
    "for i in range(1):\n",
    "    \n",
    "    K.clear_session()\n",
    "\n",
    "    adamw = AdamW(weight_decay=weight_d)\n",
    "    #adam = optimizers.Adam(learning_rate= set_learning_rate, clipnorm=1.)\n",
    "    #rmsprop = optimizers.RMSprop(learning_rate= set_learning_rate, clipnorm=1.)\n",
    "    #sgdopt = optimizers.SGD(learning_rate= set_learning_rate, clipnorm=1.)\n",
    "    #reduce_lr = tf.keras.callbacks.ReduceLROnPlateau()\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(optimizer= adamw, loss = set_loss, metrics= set_metrics)\n",
    "    model.load_weights('temp4.h5')\n",
    "    \n",
    "    \n",
    "    \n",
    "    corr_check = corr_checkpointer(X_val_list_scaled, y_val_list_scaled, filepath= (filename_base + '_fold_' + str(i) + '.h5'))\n",
    "    early_stopper = corr_early_stopping(min_epochs=stopping_min_epochs, mode='max', patience=num_patience, verbose=1)\n",
    "    \n",
    "    onecyc = OneCycle(lr_range=(0.001, 0.01), momentum_range=(0.95,0.85))\n",
    "    \n",
    "    #earlystopper = EarlyStopping(monitor = 'val_mean_squared_error', mode='min', patience=num_patience, verbose=1)  #restore_best_weights not available\n",
    "    #checkpointer = ModelCheckpoint(filename_base + '_fold_' + str(i) + '.h5', monitor = 'val_mean_squared_error', mode='min', verbose=1, save_best_only = True)\n",
    "\n",
    "\n",
    "    \n",
    "    #reduce_lr = tf.keras.callbacks.ReduceLROnPlateau()\n",
    "\n",
    "\n",
    "    history = model.fit(X_train_list_scaled, y_train_list_scaled, validation_data = (X_val_list_scaled, y_val_list_scaled), batch_size = num_batch_size, epochs=num_epochs, callbacks=[corr_check, onecyc, early_stopper]) #tensorboard_callback])\n",
    "    \n",
    "    best_epoch = len(history.history['val_mean_squared_error']) - num_patience\n",
    "    mse_model = history.history['mean_squared_error'][-(num_patience + 1)]\n",
    "    val_mse_model = history.history['val_mean_squared_error'][-(num_patience + 1)]\n",
    "    mse_hist = history.history['mean_squared_error']\n",
    "    val_mse_hist = history.history['val_mean_squared_error']\n",
    "\n",
    "    model = load_model(filename_base + '_fold_' + str(i) + '.h5', custom_objects={'AdamW':adamw})\n",
    "    preds_val_initial = model.predict(X_val_list_scaled, verbose=1, batch_size=1024)\n",
    "    preds_val_initial = preds_val_initial.reshape(preds_val_initial.shape[0])\n",
    "    \n",
    "    #****\n",
    "    y_val_list_scaled = np.divide((y_val_list_scaled), 4)\n",
    "    preds_val = np.divide((preds_val_initial), 4)\n",
    "    #y_train_list_scaled = np.divide((y_train_list[0]-18),41) - 1\n",
    "    \n",
    "    #preds_val = np.multipy((preds_val_initial), 82) + 18\n",
    "    #y_train_list_scaled = np.multipiy((y_train_list_scaled), 82) +18\n",
    "\n",
    "    spearman = stats.spearmanr(preds_val, y_val_list_scaled)\n",
    "    pearson = stats.pearsonr(preds_val, y_val_list_scaled)\n",
    "    \n",
    "    mean_ae = np.mean(abs(preds_val - y_val_list_scaled))\n",
    "    median_ae = np.median(abs(preds_val - y_val_list_scaled))\n",
    "    \n",
    "    \n",
    "    best_epochs.append(best_epoch)\n",
    "    mses.append(mse_model)\n",
    "    val_mses.append(val_mse_model)\n",
    "    mse_hists.append(mse_hist)\n",
    "    val_mse_hists.append(val_mse_hist)\n",
    "    spearmans.append(spearman)\n",
    "    pearsons.append(pearson)\n",
    "    mean_aes.append(mean_ae)\n",
    "    median_aes.append(median_ae)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best epoch: ', best_epochs)\n",
    "print('MSE: ', mses)\n",
    "print('Val MSE: ', val_mses)\n",
    "print('Spearman: ', spearmans)\n",
    "print('Pearson: ', pearsons)\n",
    "print('Mean abs error: ', mean_aes)\n",
    "print('Median abs error: ', median_aes)\n",
    "\n",
    "\n",
    "#print('Mean MSE: ', np.mean(mses))\n",
    "#print('Mean val_MSE: ', np.mean(val_mses))\n",
    "#print('Mean Spearman r: ', [np.mean(i) for i in zip(*spearmans)])\n",
    "#print('Mean Pearson r: ', [np.mean(i) for i in zip(*pearsons)])\n",
    "#print('Mean of Mean abs error: ', np.mean(mean_aes))\n",
    "#print('Mean of Median abs error: ', np.mean(median_aes))\n",
    "\n",
    "\n",
    "mean_mse = [np.mean(i) for i in zip(*mse_hists)]\n",
    "mean_val_mse = [np.mean(i) for i in zip(*val_mse_hists)]\n",
    "plt.plot(mean_mse[3:])\n",
    "plt.plot(mean_val_mse[3:])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#for j in range(len(mse_hists)):\n",
    "#    plt.plot(mse_hists[j])\n",
    "#    plt.plot(val_mse_hists[j])\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "current_time2 = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Start time: \", current_time)\n",
    "print(\"End time: \", current_time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mean_mse[200:1000])\n",
    "plt.plot(mean_val_mse[200:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bland_altman_plot(preds_val, y_val_list_scaled, scatter_kwds={'s':10}, title='MGH/BWH 12-Lead ECGs (10 secs)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7,7)) #(1, 1, figsize=(12, 12))\n",
    "    \n",
    "ax.set_title('MGH/BWH Single-Lead ECGs (10 secs)', fontsize=14)\n",
    "ax.scatter(y_val_list_scaled, preds_val, s=10)\n",
    "\n",
    "ax.set(xlim = [0, 110], xticks = np.arange(0, 110, 10),\n",
    "       ylim = [0, 110], yticks = np.arange(0, 110, 10),\n",
    "       xlabel = 'Actual Age',\n",
    "       ylabel = 'Predicted Age')\n",
    "ax.plot([0, 110], [0,110], alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also good (3)\n",
    "inputs = Input((in_neurons, 1))\n",
    "\n",
    "c1 = Conv1D(16,4, padding='same', use_bias=False, strides=2, kernel_initializer= 'he_normal') (inputs)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Activation('relu') (c1)\n",
    "c1 = Dropout(0.1) (c1)\n",
    "#c1 = SpatialDropout1D(0.2) (c1)\n",
    "\n",
    "c1 = Conv1D(32,8, padding='same', use_bias=False, strides=2, kernel_initializer= 'he_normal') (c1)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Activation('relu') (c1)\n",
    "c1 = SpatialDropout1D(0.1) (c1)\n",
    "\n",
    "s1 = Conv1D(16, 1, padding='same') (c1)\n",
    "s1 = BatchNormalization() (s1)\n",
    "\n",
    "#c2 = Conv1D(32,8, padding='same', dilation_rate=2, kernel_initializer= 'he_normal') (c1)\n",
    "c2 = WeightNormalization(Conv1D(32,8, padding='same', dilation_rate=2, kernel_initializer= 'he_normal')) (c1)\n",
    "c2 = Activation('relu') (c2)\n",
    "c2 = SpatialDropout1D(0.3) (c2)\n",
    "#c2 = Conv1D(32,8, padding='same', dilation_rate=4, kernel_initializer= 'he_normal') (c2)\n",
    "c2 = WeightNormalization(Conv1D(32,8, padding='same', dilation_rate=4, kernel_initializer= 'he_normal')) (c2)\n",
    "#c2 = Activation('relu') (c2)\n",
    "c2 = Activation('relu') (c1+c2)\n",
    "c2 = SpatialDropout1D(0.3) (c2)\n",
    "\n",
    "#s2 = Conv1D(16, 1, padding='same', use_bias=False) (c2)\n",
    "#s2 = BatchNormalization() (s2)\n",
    "\n",
    "#c3 = Conv1D(16,8, padding='same', dilation_rate=2, kernel_initializer= 'he_normal') (c2)\n",
    "c3 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=2, kernel_initializer= 'he_normal')) (c2)\n",
    "c3 = Activation('relu') (c3)\n",
    "c3 = SpatialDropout1D(0.3) (c3)\n",
    "#c3 = Conv1D(16,8, padding='same', dilation_rate=4, kernel_initializer= 'he_normal') (c3)\n",
    "c3 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=4, kernel_initializer= 'he_normal')) (c3)\n",
    "c3 = Activation('relu') (s1+c3)\n",
    "c3 = SpatialDropout1D(0.3) (c3)\n",
    "\n",
    "c3 = Conv1D(16,8, padding='same', dilation_rate=2, kernel_initializer= 'he_normal') (c3)\n",
    "#c3 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=8, kernel_initializer= 'he_normal')) (c2)\n",
    "c3 = Activation('relu') (c3)\n",
    "c3 = SpatialDropout1D(0.3) (c3)\n",
    "c3 = Conv1D(16,8, padding='same', dilation_rate=4, kernel_initializer= 'he_normal') (c3)\n",
    "#c3 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=16, kernel_initializer= 'he_normal')) (c3)\n",
    "c3 = Activation('relu') (s1+c3)\n",
    "c3 = SpatialDropout1D(0.3) (c3)\n",
    "\n",
    "c7 = Conv1D(1, 3, use_bias=False, kernel_initializer= 'he_normal') (c3)\n",
    "c7 = BatchNormalization() (c7)\n",
    "c7 = Activation('relu') (c7)\n",
    "c7 = Dropout(0.5) (c7)\n",
    "\n",
    "#c7 = Conv1D(64, 3, use_bias = False) (c7)\n",
    "#c7 = BatchNormalization() (c7)\n",
    "#c7 = Activation('relu') (c7)\n",
    "\n",
    "#final = GlobalAvgPool1D() (c4)\n",
    "\n",
    "#c6 = Dropout(0.1) (c6)\n",
    "#final = Dense(1, activation='relu') (c7)\n",
    "final = Flatten() (c7)\n",
    "#final = Dropout(0.05) (final)\n",
    "#final = Dense(10, activation='relu') (final)\n",
    "#final = Dropout(0.2) (final)\n",
    "outputs = Dense(1) (final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_loss = 'mean_squared_error'\n",
    "set_metrics = ['mean_squared_error']\n",
    "\n",
    "num_batch_size=1024\n",
    "num_epochs=500\n",
    "num_patience=200\n",
    "stopping_min_epochs=400\n",
    "weight_d = 0 #0.000001  \n",
    "\n",
    "adamw = AdamW(weight_decay=weight_d)\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[outputs])\n",
    "model.compile(optimizer= adamw, loss = set_loss, metrics= ['mean_squared_error'])\n",
    "model.save_weights('temp4.h5')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ocp = OneCycle(lr_range=(0.0007, 0.007), momentum_range=(0.94,0.85))\n",
    "#ocp.test_run(128)\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_list_scaled = np.multiply(np.sign(X_train_list[0]), np.log(np.abs(X_train_list[0])+1)) #log-modulus transform- https://blogs.sas.com/content/iml/2014/07/14/log-transformation-of-pos-neg.html\n",
    "#X_val_list_scaled = np.multiply(np.sign(X_val_list[0]), np.log(np.abs(X_val_list[0])+1))\n",
    "X_train_list_scaled = np.divide(X_train_list[0],4000)\n",
    "X_val_list_scaled = np.divide(X_val_list[0], 4000)\n",
    "y_train_list_scaled = np.multiply((y_train_list[0]), 4)\n",
    "y_val_list_scaled = np.multiply((y_val_list[0]), 4)\n",
    "#sns.distplot(X_train_list_scaled)\n",
    "\n",
    "#Normalization\n",
    "#X_train_list_scaled = np.log(X_train_list[0])\n",
    "#print(np.mean(X_train_list_scaled))\n",
    "#print(np.std(X_train_list_scaled))\n",
    "\n",
    "#X_val_list_scaled = (X_val_list[0]-np.mean(X_val_list[0]))/370\n",
    "#print(np.mean(X_val_list_scaled))\n",
    "#print(np.std(X_val_list_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_finder = LRFinder(min_lr=1e-3, max_lr=0.1, steps_per_epoch=np.ceil(3277/num_batch_size), epochs=3)\n",
    "\n",
    "#model.fit(X_train_list_scaled, y_train_list_scaled, callbacks=[lr_finder])\n",
    "#lr_finder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_epochs = []\n",
    "mses = []\n",
    "val_mses = []\n",
    "mse_hists = []\n",
    "val_mse_hists = []\n",
    "spearmans = []\n",
    "pearsons = []\n",
    "mean_aes = []\n",
    "median_aes = []\n",
    "\n",
    "#for train_index, val_index in group_kfold.split(ecg_X_k, ecg_y_k, ecg_id_k):  #for full 10-fold cross validation\n",
    "\n",
    "for i in range(1):\n",
    "    \n",
    "    K.clear_session()\n",
    "\n",
    "    adamw = AdamW(weight_decay=weight_d)\n",
    "    #adam = optimizers.Adam(learning_rate= set_learning_rate, clipnorm=1.)\n",
    "    #rmsprop = optimizers.RMSprop(learning_rate= set_learning_rate, clipnorm=1.)\n",
    "    #sgdopt = optimizers.SGD(learning_rate= set_learning_rate, clipnorm=1.)\n",
    "    #reduce_lr = tf.keras.callbacks.ReduceLROnPlateau()\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(optimizer= adamw, loss = set_loss, metrics= set_metrics)\n",
    "    model.load_weights('temp4.h5')\n",
    "    \n",
    "    \n",
    "    \n",
    "    corr_check = corr_checkpointer(X_val_list_scaled, y_val_list_scaled, filepath= (filename_base + '_fold_' + str(i) + '.h5'))\n",
    "    early_stopper = corr_early_stopping(min_epochs=stopping_min_epochs, mode='max', patience=num_patience, verbose=1)\n",
    "    \n",
    "    onecyc = OneCycle(lr_range=(0.001, 0.01), momentum_range=(0.95,0.85))\n",
    "    \n",
    "    #earlystopper = EarlyStopping(monitor = 'val_mean_squared_error', mode='min', patience=num_patience, verbose=1)  #restore_best_weights not available\n",
    "    #checkpointer = ModelCheckpoint(filename_base + '_fold_' + str(i) + '.h5', monitor = 'val_mean_squared_error', mode='min', verbose=1, save_best_only = True)\n",
    "\n",
    "\n",
    "    \n",
    "    #reduce_lr = tf.keras.callbacks.ReduceLROnPlateau()\n",
    "\n",
    "\n",
    "    history = model.fit(X_train_list_scaled, y_train_list_scaled, validation_data = (X_val_list_scaled, y_val_list_scaled), batch_size = num_batch_size, epochs=num_epochs, callbacks=[corr_check, onecyc, early_stopper]) #tensorboard_callback])\n",
    "    \n",
    "    best_epoch = len(history.history['val_mean_squared_error']) - num_patience\n",
    "    mse_model = history.history['mean_squared_error'][-(num_patience + 1)]\n",
    "    val_mse_model = history.history['val_mean_squared_error'][-(num_patience + 1)]\n",
    "    mse_hist = history.history['mean_squared_error']\n",
    "    val_mse_hist = history.history['val_mean_squared_error']\n",
    "\n",
    "    model = load_model(filename_base + '_fold_' + str(i) + '.h5', custom_objects={'AdamW':adamw})\n",
    "    preds_val_initial = model.predict(X_val_list_scaled, verbose=1, batch_size=1024)\n",
    "    preds_val_initial = preds_val_initial.reshape(preds_val_initial.shape[0])\n",
    "    \n",
    "    #****\n",
    "    y_val_list_scaled = np.divide((y_val_list_scaled), 4)\n",
    "    preds_val = np.divide((preds_val_initial), 4)\n",
    "    #y_train_list_scaled = np.divide((y_train_list[0]-18),41) - 1\n",
    "    \n",
    "    #preds_val = np.multipy((preds_val_initial), 82) + 18\n",
    "    #y_train_list_scaled = np.multipiy((y_train_list_scaled), 82) +18\n",
    "\n",
    "    spearman = stats.spearmanr(preds_val, y_val_list_scaled)\n",
    "    pearson = stats.pearsonr(preds_val, y_val_list_scaled)\n",
    "    \n",
    "    mean_ae = np.mean(abs(preds_val - y_val_list_scaled))\n",
    "    median_ae = np.median(abs(preds_val - y_val_list_scaled))\n",
    "    \n",
    "    \n",
    "    best_epochs.append(best_epoch)\n",
    "    mses.append(mse_model)\n",
    "    val_mses.append(val_mse_model)\n",
    "    mse_hists.append(mse_hist)\n",
    "    val_mse_hists.append(val_mse_hist)\n",
    "    spearmans.append(spearman)\n",
    "    pearsons.append(pearson)\n",
    "    mean_aes.append(mean_ae)\n",
    "    median_aes.append(median_ae)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best epoch: ', best_epochs)\n",
    "print('MSE: ', mses)\n",
    "print('Val MSE: ', val_mses)\n",
    "print('Spearman: ', spearmans)\n",
    "print('Pearson: ', pearsons)\n",
    "print('Mean abs error: ', mean_aes)\n",
    "print('Median abs error: ', median_aes)\n",
    "\n",
    "\n",
    "#print('Mean MSE: ', np.mean(mses))\n",
    "#print('Mean val_MSE: ', np.mean(val_mses))\n",
    "#print('Mean Spearman r: ', [np.mean(i) for i in zip(*spearmans)])\n",
    "#print('Mean Pearson r: ', [np.mean(i) for i in zip(*pearsons)])\n",
    "#print('Mean of Mean abs error: ', np.mean(mean_aes))\n",
    "#print('Mean of Median abs error: ', np.mean(median_aes))\n",
    "\n",
    "\n",
    "mean_mse = [np.mean(i) for i in zip(*mse_hists)]\n",
    "mean_val_mse = [np.mean(i) for i in zip(*val_mse_hists)]\n",
    "plt.plot(mean_mse[3:])\n",
    "plt.plot(mean_val_mse[3:])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#for j in range(len(mse_hists)):\n",
    "#    plt.plot(mse_hists[j])\n",
    "#    plt.plot(val_mse_hists[j])\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "current_time2 = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Start time: \", current_time)\n",
    "print(\"End time: \", current_time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mean_mse[200:1000])\n",
    "plt.plot(mean_val_mse[200:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bland_altman_plot(preds_val, y_val_list_scaled, scatter_kwds={'s':10}, title='MGH/BWH 12-Lead ECGs (10 secs)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7,7)) #(1, 1, figsize=(12, 12))\n",
    "    \n",
    "ax.set_title('MGH/BWH Single-Lead ECGs (10 secs)', fontsize=14)\n",
    "ax.scatter(y_val_list_scaled, preds_val, s=10)\n",
    "\n",
    "ax.set(xlim = [0, 110], xticks = np.arange(0, 110, 10),\n",
    "       ylim = [0, 110], yticks = np.arange(0, 110, 10),\n",
    "       xlabel = 'Actual Age',\n",
    "       ylabel = 'Predicted Age')\n",
    "ax.plot([0, 110], [0,110], alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_list_scaled = np.multiply((y_train_list[0]), 4)\n",
    "y_val_list_scaled = np.multiply((y_val_list[0]), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onecyc2 = OneCycle(lr_range=(0.0005, 0.005), momentum_range=(0.95,0.85))\n",
    "history2 = model.fit(X_train_list_scaled, y_train_list_scaled, validation_data = (X_val_list_scaled, y_val_list_scaled), batch_size = num_batch_size, epochs=num_epochs, callbacks=[corr_check, onecyc, early_stopper]) #tensorboard_callback])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i=0\n",
    "model = load_model(filename_base + '_fold_' + str(i) + '.h5', custom_objects={'AdamW':adamw})\n",
    "#model = load_model('trained_pr_first_1000_58_fold_' + str(i) + '.h5', custom_objects={'AdamW':adamw})\n",
    "\n",
    "preds_val = model.predict(X_val_list_scaled, verbose=1, batch_size=1024)\n",
    "preds_val = preds_val.reshape(preds_val.shape[0])\n",
    "\n",
    "y_val_list_scaled = np.divide((y_val_list_scaled), 4)\n",
    "preds_val = np.divide((preds_val), 1)\n",
    "\n",
    "spearman = stats.spearmanr(preds_val, y_val_list_scaled)\n",
    "pearson = stats.pearsonr(preds_val, y_val_list_scaled)\n",
    "    \n",
    "mean_ae = np.mean(abs(preds_val - y_val_list_scaled))\n",
    "median_ae = np.median(abs(preds_val - y_val_list_scaled))\n",
    "    \n",
    "print('Spearman: ', spearman)\n",
    "print('Pearson: ', pearson)\n",
    "print('Mean abs error: ', mean_ae)\n",
    "print('Median abs error: ', median_ae)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,7)) #(1, 1, figsize=(12, 12))\n",
    "    \n",
    "ax.set_title('MGH/BWH Single-Lead ECGs (10 secs)', fontsize=14)\n",
    "ax.scatter(y_val_list_scaled, preds_val, s=10)\n",
    "\n",
    "ax.set(xlim = [0, 110], xticks = np.arange(0, 110, 10),\n",
    "       ylim = [0, 110], yticks = np.arange(0, 110, 10),\n",
    "       xlabel = 'Actual Age',\n",
    "       ylabel = 'Predicted Age')\n",
    "ax.plot([0, 110], [0,110], alpha=0.2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_list_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(y_val_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_list[i].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "far_preds = np.where(abs(preds_val - y_val_list[i])>25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "far_preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for far_pred in far_preds[0]:\n",
    "    print('Index: ', far_pred, ', Actual: ', y_val_list[i][far_pred], ', Predicted: ', preds_val[far_pred].round(1), \\\n",
    "          ', Difference: ', (preds_val[far_pred] - y_val_list[i][far_pred]).round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.widgets import Cursor\n",
    "from matplotlib.widgets import RectangleSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineBuilder:\n",
    "    def __init__(self, line):\n",
    "        self.line = line\n",
    "        self.xs = list(line.get_xdata())\n",
    "        self.ys = list(line.get_ydata())\n",
    "        self.cid = line.figure.canvas.mpl_connect('button_press_event', self)\n",
    "    \n",
    "    def __call__(self, event):\n",
    "        print('click', event)\n",
    "        if event.inaxes!=self.line.axes: return\n",
    "        self.xs.append(event.xdata)\n",
    "        self.ys.append(event.ydata)\n",
    "        self.line.set_data(self.xs, self.ys)\n",
    "        self.line.figure.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.random.rand(10))\n",
    "\n",
    "def onclick(event):\n",
    "    print('%s click: button=%d, x=%d, y=%d, xdata=%f, ydata=%f' %\n",
    "          ('double' if event.dblclick else 'single', event.button,\n",
    "           event.x, event.y, event.xdata, event.ydata))\n",
    "\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "class LineDrawer(object):\n",
    "    lines = []\n",
    "    def draw_line(self):\n",
    "        ax = plt.gca()\n",
    "        xy = plt.ginput(2)\n",
    "\n",
    "        x = [p[0] for p in xy]\n",
    "        y = [p[1] for p in xy]\n",
    "        line = plt.plot(x,y)\n",
    "        ax.figure.canvas.draw()\n",
    "\n",
    "        self.lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.random.rand(10))\n",
    "\n",
    "def onclick(event):\n",
    "    a='xdata=%f, ydata=%f' % (event.xdata, event.ydata)\n",
    "    x = event.xdata\n",
    "    temp.append(x)\n",
    "    L =  ax.axvline(x=x)\n",
    "    ax.set_title(a)\n",
    "    ax.fig.canvas.draw()\n",
    "\n",
    "def onrelease(event):\n",
    "    x2= event.xdata\n",
    "    b = 'Start: %.1f, End: %.1f, PR interval: %.1f' % (temp[0], x2, (x2 - temp[0]))\n",
    "    L =  ax.axvline(x=x2)\n",
    "    ax.set_title(b)\n",
    "    ax.fig.canvas.draw()\n",
    "    \n",
    "temp = []\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick)\n",
    "cid2 = fig.canvas.mpl_connect('button_release_event', onrelease)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "indx = 9\n",
    "j= 20    #starting point within first 5000 points\n",
    "length= 1000\n",
    "\n",
    "for k in range(1):\n",
    "    #plt.figure(figsize=(20,5))\n",
    "    fig, ax = plt.subplots(figsize=(10,5))\n",
    "    #figure(figsize=(20,5))\n",
    "    plt.plot(X_val_list[i][indx][(5000*k)+j:(5000*k)+j+length])\n",
    "    #line, = ax.plot([0], [0])\n",
    "    #linebuilder = LineBuilder(line)\n",
    "    cid = fig.canvas.mpl_connect('button_press_event', onclick)\n",
    "    #cursor = Cursor(ax, horizOn=True, vertOn = True, color = 'green', linewidth=2.0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_list[0][0][0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(X_val_list[0][20][0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2d array\n",
    "inputs = Input((12, in_neurons, 1))\n",
    "\n",
    "c1 = Conv2D(16,(1,4), padding='valid', use_bias = False, kernel_initializer= 'he_normal') (inputs)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Activation('relu') (c1)\n",
    "c1 = MaxPooling2D((1, 2)) (c1)\n",
    "c1 = Dropout(0.1) (c1)\n",
    "\n",
    "c1 = Conv2D(16,(1,8), padding='valid', use_bias = False, kernel_initializer= 'he_normal') (c1)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Activation('relu') (c1)\n",
    "c1 = MaxPooling2D((1, 4)) (c1)\n",
    "c1 = Dropout(0.2) (c1)\n",
    "\n",
    "\n",
    "#s1 = Conv2D(16, (12,4), padding='valid', use_bias=False, kernel_initializer= 'he_normal') (c1)\n",
    "#s1 = BatchNormalization() (s1)\n",
    "#s1 = MaxPooling2D((1, 4)) (c1)\n",
    "\n",
    "c2 = WeightNormalization(Conv2D(16,(1,8), padding='same', dilation_rate=2, kernel_initializer= 'he_normal')) (c1)\n",
    "c2 = Activation('relu') (c2)\n",
    "c2 = SpatialDropout2D(0.3) (c2)\n",
    "c2 = WeightNormalization(Conv2D(16,(1,8), padding='same', dilation_rate=4, kernel_initializer= 'he_normal')) (c2)\n",
    "c2 = Activation('relu') (c1+c2)\n",
    "c2 = SpatialDropout2D(0.3) (c2)\n",
    "#c2 = MaxPooling2D((1, 4)) (c2)\n",
    "\n",
    "\n",
    "c2 = Conv2D(16,(12,8), padding='same', use_bias = False, kernel_initializer= 'he_normal') (c2)\n",
    "c2 = BatchNormalization() (c2)\n",
    "c2 = Activation('relu') (c2)\n",
    "c2 = Dropout(0.4) (c2)\n",
    "c2 = MaxPooling2D((12, 1)) (c2)\n",
    "\n",
    "#c2 = Conv2D(16, (4, 1), use_bias = False, kernel_initializer= 'he_normal') (c2)\n",
    "#c2 = BatchNormalization() (c2)\n",
    "#c2 = Activation('relu') (c2)\n",
    "#c2 = Dropout(0.2) (c2)\n",
    "\n",
    "c3 = WeightNormalization(Conv2D(16,(1,8), padding='same', dilation_rate=2, kernel_initializer= 'he_normal')) (c2)\n",
    "c3 = Activation('relu') (c3)\n",
    "c3 = SpatialDropout2D(0.4) (c3)\n",
    "c3 = WeightNormalization(Conv2D(16,(1,8), padding='same', dilation_rate=4, kernel_initializer= 'he_normal')) (c3)\n",
    "c3 = Activation('relu') (c1+c3)\n",
    "c3 = SpatialDropout2D(0.4) (c3)\n",
    "c3 = MaxPooling2D((1, 2)) (c3)\n",
    "\n",
    "\n",
    "#c3 = Conv2D(16,(12,1), padding='valid', use_bias = False, kernel_initializer= 'he_normal') (c3)\n",
    "#c3 = BatchNormalization() (c3)\n",
    "#c3 = Activation('relu') (c3)\n",
    "##c2 = MaxPooling2D((1, 2)) (c3)\n",
    "#c3 = Dropout(0.3) (c3)\n",
    "#c3 = MaxPooling2D((1, 8)) (c3)\n",
    "\n",
    "\n",
    "c7 = Conv2D(16, (1,8), use_bias = False, kernel_initializer= 'he_normal') (c2)\n",
    "c7 = BatchNormalization() (c7)\n",
    "c7 = Activation('relu') (c7)\n",
    "c7 = Dropout(0.5) (c7)\n",
    "c7 = MaxPooling2D((1, 4)) (c7)\n",
    "\n",
    "final = Flatten() (c7)\n",
    "\n",
    "outputs = Dense(1) (final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not going below ~400\n",
    "inputs = Input((12, in_neurons, 1))\n",
    "\n",
    "c1 = Conv2D(16,(1,4), padding='same', use_bias = False, kernel_initializer= 'he_normal') (inputs)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Activation('relu') (c1)\n",
    "c1 = MaxPooling2D((1, 2)) (c1)\n",
    "c1 = Dropout(0.1) (c1)\n",
    "\n",
    "s1 = Conv2D(16, (4,1), padding='valid', use_bias=False, kernel_initializer= 'he_normal') (c1)\n",
    "s1 = BatchNormalization() (s1)\n",
    "s1 = MaxPooling2D((1, 4)) (s1)\n",
    "\n",
    "c2 = WeightNormalization(Conv2D(16,(1,8), padding='same', dilation_rate=2, kernel_initializer= 'he_normal')) (c1)\n",
    "c2 = Activation('relu') (c2)\n",
    "c2 = SpatialDropout2D(0.2) (c2)\n",
    "c2 = WeightNormalization(Conv2D(16,(1,8), padding='same', dilation_rate=4, kernel_initializer= 'he_normal')) (c2)\n",
    "c2 = Activation('relu') (c1+c2)\n",
    "c2 = SpatialDropout2D(0.2) (c2)\n",
    "c2 = MaxPooling2D((1, 4)) (c2)\n",
    "\n",
    "c2 = Conv2D(16, (4, 1), use_bias = False, kernel_initializer= 'he_normal') (c2)\n",
    "c2 = BatchNormalization() (c2)\n",
    "c2 = Activation('relu') (c2)\n",
    "c2 = Dropout(0.2) (c2)\n",
    "\n",
    "c3 = WeightNormalization(Conv2D(16,(1,8), padding='same', dilation_rate=2, kernel_initializer= 'he_normal')) (c2)\n",
    "c3 = Activation('relu') (c3)\n",
    "c3 = SpatialDropout2D(0.3) (c3)\n",
    "c3 = WeightNormalization(Conv2D(16,(1,8), padding='same', dilation_rate=4, kernel_initializer= 'he_normal')) (c3)\n",
    "c3 = Activation('relu') (s1+c3)\n",
    "c3 = SpatialDropout2D(0.3) (c3)\n",
    "c3 = MaxPooling2D((1, 8)) (c3)\n",
    "\n",
    "c7 = Conv2D(16, (9,3), use_bias = False, kernel_initializer= 'he_normal') (c3)\n",
    "c7 = BatchNormalization() (c7)\n",
    "c7 = Activation('relu') (c7)\n",
    "c7 = Dropout(0.5) (c7)\n",
    "\n",
    "final = Flatten() (c7)\n",
    "\n",
    "outputs = Dense(1) (final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "filename_base = 'trained_' + suffix + '_73'\n",
    "print(filename_base)\n",
    "\n",
    "in_neurons=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input((12, in_neurons, 1))\n",
    "\n",
    "c1 = Conv2D(16,(1,4), padding='same', use_bias = False, kernel_initializer= 'he_normal') (inputs)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Activation('relu') (c1)\n",
    "c1 = MaxPooling2D((1, 2)) (c1)\n",
    "c1 = Dropout(0.1) (c1)\n",
    "\n",
    "s1 = Conv2D(16, (4,1), padding='valid', use_bias=False, kernel_initializer= 'he_normal') (c1)\n",
    "s1 = BatchNormalization() (s1)\n",
    "s1 = MaxPooling2D((1, 4)) (s1)\n",
    "\n",
    "c2 = WeightNormalization(Conv2D(16,(1,8), padding='same', dilation_rate=2, kernel_initializer= 'he_normal')) (c1)\n",
    "c2 = Activation('relu') (c2)\n",
    "c2 = SpatialDropout2D(0.2) (c2)\n",
    "c2 = WeightNormalization(Conv2D(16,(1,8), padding='same', dilation_rate=4, kernel_initializer= 'he_normal')) (c2)\n",
    "c2 = Activation('relu') (c1+c2)\n",
    "c2 = SpatialDropout2D(0.2) (c2)\n",
    "c2 = MaxPooling2D((1, 4)) (c2)\n",
    "\n",
    "c2 = Conv2D(16, (4, 1), use_bias = False, kernel_initializer= 'he_normal') (c2)\n",
    "c2 = BatchNormalization() (c2)\n",
    "c2 = Activation('relu') (c2)\n",
    "c2 = Dropout(0.2) (c2)\n",
    "\n",
    "c3 = WeightNormalization(Conv2D(16,(1,8), padding='same', dilation_rate=2, kernel_initializer= 'he_normal')) (c2)\n",
    "c3 = Activation('relu') (c3)\n",
    "c3 = SpatialDropout2D(0.3) (c3)\n",
    "c3 = WeightNormalization(Conv2D(16,(1,8), padding='same', dilation_rate=4, kernel_initializer= 'he_normal')) (c3)\n",
    "c3 = Activation('relu') (s1+c3)\n",
    "c3 = SpatialDropout2D(0.3) (c3)\n",
    "c3 = MaxPooling2D((1, 8)) (c3)\n",
    "\n",
    "c7 = Conv2D(16, (9,3), use_bias = False, kernel_initializer= 'he_normal') (c3)\n",
    "c7 = BatchNormalization() (c7)\n",
    "c7 = Activation('relu') (c7)\n",
    "c7 = Dropout(0.5) (c7)\n",
    "\n",
    "final = Flatten() (c7)\n",
    "\n",
    "outputs = Dense(1) (final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_loss = 'mean_squared_error'\n",
    "set_metrics = ['mean_squared_error']\n",
    "#set_learning_rate = 0.007\n",
    "\n",
    "num_batch_size=210\n",
    "num_epochs=500\n",
    "num_patience=200\n",
    "stopping_min_epochs=400\n",
    "weight_d = 0  # 0.0000001\n",
    "\n",
    "adamw = AdamW(weight_decay=weight_d)\n",
    "#adam = optimizers.Adam(learning_rate= set_learning_rate)\n",
    "#reduce_lr = tf.keras.callbacks.ReduceLROnPlateau()\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[outputs])\n",
    "model.compile(optimizer= adamw, loss = set_loss, metrics= ['mean_squared_error'])\n",
    "model.save_weights('temp4.h5')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ocp = OneCycle(lr_range=(0.0007, 0.007), momentum_range=(0.94,0.85))\n",
    "#ocp.test_run(128)\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_finder = LRFinder(start_lr= 0.001, end_lr= 1, max_steps=20)\n",
    "\n",
    "#model.fit(X_train_list[0], y_train_list[0], callbacks=[lr_finder])\n",
    "#lr_finder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_epochs = []\n",
    "mses = []\n",
    "val_mses = []\n",
    "mse_hists = []\n",
    "val_mse_hists = []\n",
    "spearmans = []\n",
    "pearsons = []\n",
    "mean_aes = []\n",
    "median_aes = []\n",
    "\n",
    "#for train_index, val_index in group_kfold.split(ecg_X_k, ecg_y_k, ecg_id_k):  #for full 10-fold cross validation\n",
    "\n",
    "for i in range(1):\n",
    "    \n",
    "    K.clear_session()\n",
    "\n",
    "    adamw = AdamW(weight_decay=weight_d)\n",
    "    #adam = optimizers.Adam(learning_rate= set_learning_rate, clipnorm=1.)\n",
    "    #rmsprop = optimizers.RMSprop(learning_rate= set_learning_rate, clipnorm=1.)\n",
    "    #sgdopt = optimizers.SGD(learning_rate= set_learning_rate, clipnorm=1.)\n",
    "    #reduce_lr = tf.keras.callbacks.ReduceLROnPlateau()\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(optimizer= adamw, loss = set_loss, metrics= set_metrics)\n",
    "    model.load_weights('temp4.h5')\n",
    "    \n",
    "    \n",
    "    \n",
    "    corr_check = corr_checkpointer(X_val_list[i], y_val_list[i], filepath= (filename_base + '_fold_' + str(i) + '.h5'))\n",
    "    early_stopper = corr_early_stopping(min_epochs=stopping_min_epochs, mode='max', patience=num_patience, verbose=1)\n",
    "    \n",
    "    onecyc = OneCycle(lr_range=(0.0005, 0.005), momentum_range=(0.95,0.85))\n",
    "    \n",
    "    #restore_best_weights not available\n",
    "    #checkpointer = ModelCheckpoint(filename, monitor = corr_check, mode='min', verbose=1, save_best_only = True)\n",
    "    #reduce_lr = tf.keras.callbacks.ReduceLROnPlateau()\n",
    "\n",
    "\n",
    "    history = model.fit(X_train_list[i], y_train_list[i], validation_data = (X_val_list[i], y_val_list[i]), batch_size = num_batch_size, epochs=num_epochs, callbacks=[corr_check, onecyc, early_stopper]) #tensorboard_callback])\n",
    "    \n",
    "    best_epoch = len(history.history['val_mean_squared_error']) - num_patience\n",
    "    mse_model = history.history['mean_squared_error'][-(num_patience + 1)]\n",
    "    val_mse_model = history.history['val_mean_squared_error'][-(num_patience + 1)]\n",
    "    mse_hist = history.history['mean_squared_error']\n",
    "    val_mse_hist = history.history['val_mean_squared_error']\n",
    "\n",
    "    model = load_model(filename_base + '_fold_' + str(i) + '.h5', custom_objects={'AdamW':adamw})\n",
    "    preds_val = model.predict(X_val_list[i], verbose=1, batch_size=1024)\n",
    "    preds_val = preds_val.reshape(preds_val.shape[0])\n",
    "\n",
    "    spearman = stats.spearmanr(preds_val, y_val_list[i])\n",
    "    pearson = stats.pearsonr(preds_val, y_val_list[i])\n",
    "    \n",
    "    mean_ae = np.mean(abs(preds_val - y_val_list[i]))\n",
    "    median_ae = np.median(abs(preds_val - y_val_list[i]))\n",
    "    \n",
    "    \n",
    "    best_epochs.append(best_epoch)\n",
    "    mses.append(mse_model)\n",
    "    val_mses.append(val_mse_model)\n",
    "    mse_hists.append(mse_hist)\n",
    "    val_mse_hists.append(val_mse_hist)\n",
    "    spearmans.append(spearman)\n",
    "    pearsons.append(pearson)\n",
    "    mean_aes.append(mean_ae)\n",
    "    median_aes.append(median_ae)\n",
    "\n",
    "print('Best epoch: ', best_epochs)\n",
    "print('MSE: ', mses)\n",
    "print('Val MSE: ', val_mses)\n",
    "print('Spearman: ', spearmans)\n",
    "print('Pearson: ', pearsons)\n",
    "print('Mean abs error: ', mean_aes)\n",
    "print('Median abs error: ', median_aes)\n",
    "\n",
    "\n",
    "#print('Mean MSE: ', np.mean(mses))\n",
    "#print('Mean val_MSE: ', np.mean(val_mses))\n",
    "#print('Mean Spearman r: ', [np.mean(i) for i in zip(*spearmans)])\n",
    "#print('Mean Pearson r: ', [np.mean(i) for i in zip(*pearsons)])\n",
    "#print('Mean of Mean abs error: ', np.mean(mean_aes))\n",
    "#print('Mean of Median abs error: ', np.mean(median_aes))\n",
    "\n",
    "\n",
    "mean_mse = [np.mean(i) for i in zip(*mse_hists)]\n",
    "mean_val_mse = [np.mean(i) for i in zip(*val_mse_hists)]\n",
    "plt.plot(mean_mse[3:])\n",
    "plt.plot(mean_val_mse[3:])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#for j in range(len(mse_hists)):\n",
    "#    plt.plot(mse_hists[j])\n",
    "#    plt.plot(val_mse_hists[j])\n",
    "#    plt.show()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "current_time2 = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Start time: \", current_time)\n",
    "print(\"End time: \", current_time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mean_mse[60:1000])\n",
    "plt.plot(mean_val_mse[60:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bland_altman_plot(preds_val, y_val_list[i], scatter_kwds={'s':10}, title='MGH/BWH 12-Lead ECGs (10 secs)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7,7)) #(1, 1, figsize=(12, 12))\n",
    "    \n",
    "ax.set_title('MGH/BWH 12-Lead ECGs (10 secs)', fontsize=14)\n",
    "ax.scatter(y_val_list[i], preds_val, s=10)\n",
    "\n",
    "ax.set(xlim = [0, 420], xticks = np.arange(0, 420, 50),\n",
    "       ylim = [0, 420], yticks = np.arange(0, 420, 50),\n",
    "       xlabel = 'ECG PR Interval',\n",
    "       ylabel = 'Predicted PR Interval')\n",
    "ax.plot([0, 450], [0,450], alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onecyc2 = OneCycle(lr_range=(0.001, 0.01), momentum_range=(0.95,0.85))\n",
    "history2 = model.fit(X_train_list[i], y_train_list[i], validation_data = (X_val_list[i], y_val_list[i]), batch_size = num_batch_size, epochs=num_epochs, callbacks=[corr_check, onecyc, early_stopper]) #tensorboard_callback])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "model = load_model('trained_' + suffix + '_58' + '_fold_' + str(i) + '.h5', custom_objects={'AdamW':adamw})\n",
    "preds_val = model.predict(X_val_list[i], verbose=1, batch_size=1024)\n",
    "preds_val = preds_val.reshape(preds_val.shape[0])\n",
    "\n",
    "spearman = stats.spearmanr(preds_val, y_val_list[i])\n",
    "pearson = stats.pearsonr(preds_val, y_val_list[i])\n",
    "    \n",
    "mean_ae = np.mean(abs(preds_val - y_val_list[i]))\n",
    "median_ae = np.median(abs(preds_val - y_val_list[i]))\n",
    "    \n",
    "print('Spearman: ', spearman)\n",
    "print('Pearson: ', pearson)\n",
    "print('Mean abs error: ', mean_ae)\n",
    "print('Median abs error: ', median_ae)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6)) #(1, 1, figsize=(12, 12))\n",
    "    \n",
    "fig.suptitle('12-Lead ECGs (47)')\n",
    "ax.scatter(y_val_list[i], preds_val)\n",
    "ax.set(xlim = [0, 350], xticks = np.arange(0, 350, 50),\n",
    "       ylim = [0, 350], yticks = np.arange(0, 350, 50),\n",
    "       xlabel = 'ECG PR Interval',\n",
    "       ylabel = 'Predicted PR Interval')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "filename_base = 'trained_' + suffix + '_57'\n",
    "print(filename_base)\n",
    "\n",
    "in_neurons=60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input((in_neurons, 1))\n",
    "\n",
    "c1 = Conv1D(16,4, padding='same', strides=2, use_bias = False, kernel_initializer= 'he_normal') (inputs)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Activation('relu') (c1)\n",
    "c1 = Dropout(0.1) (c1)\n",
    "\n",
    "s1 = Conv1D(16, 8, padding='same', strides=4, use_bias=False, kernel_initializer= 'he_normal') (c1)\n",
    "s1 = BatchNormalization() (s1)\n",
    "\n",
    "\n",
    "c2 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=2, kernel_initializer= 'he_normal')) (c1)\n",
    "c2 = Activation('relu') (c2)\n",
    "c2 = SpatialDropout1D(0.1) (c2)\n",
    "c2 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=4, kernel_initializer= 'he_normal')) (c2)\n",
    "#c2 = Activation('relu') (c2)\n",
    "c2 = Activation('relu') (c1+c2)\n",
    "c2 = SpatialDropout1D(0.1) (c2)\n",
    "c2 = MaxPooling1D(pool_size=1, strides=4) (c2)\n",
    "\n",
    "\n",
    "\n",
    "c3 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=2, kernel_initializer= 'he_normal')) (c2)\n",
    "c3 = Activation('relu') (c3)\n",
    "c3 = SpatialDropout1D(0.2) (c3)\n",
    "c3 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=4, kernel_initializer= 'he_normal')) (c3)\n",
    "#c3 = Activation('relu') (c3)\n",
    "c3 = Activation('relu') (s1+c3)\n",
    "c3 = SpatialDropout1D(0.2) (c3)\n",
    "c3 = MaxPooling1D(pool_size=1, strides=8) (c3)\n",
    "\n",
    "c7 = Conv1D(1, 3, use_bias = False, kernel_initializer= 'he_normal') (c3)\n",
    "c7 = BatchNormalization() (c7)\n",
    "c7 = Activation('relu') (c7)\n",
    "c7 = Dropout(0.2) (c7)\n",
    "\n",
    "final = Flatten() (c7)\n",
    "\n",
    "outputs = Dense(1) (final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_loss = 'mean_squared_error'\n",
    "set_metrics = ['mean_squared_error']\n",
    "#set_learning_rate = 0.007\n",
    "\n",
    "num_batch_size=245\n",
    "num_epochs=500\n",
    "num_patience=200\n",
    "stopping_min_epochs=400\n",
    "weight_d = 0  # 0.0000001\n",
    "\n",
    "adamw = AdamW(weight_decay=weight_d)\n",
    "#adam = optimizers.Adam(learning_rate= set_learning_rate)\n",
    "#reduce_lr = tf.keras.callbacks.ReduceLROnPlateau()\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[outputs])\n",
    "model.compile(optimizer= adamw, loss = set_loss, metrics= ['mean_squared_error'])\n",
    "model.save_weights('temp4.h5')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ocp = OneCycle(lr_range=(0.0007, 0.007), momentum_range=(0.94,0.85))\n",
    "#ocp.test_run(128)\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_finder = LRFinder(start_lr= 0.0001, end_lr= 1, max_steps=20)\n",
    "\n",
    "#model.fit(X_train_list[0], y_train_list[0], callbacks=[lr_finder])\n",
    "#lr_finder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_epochs = []\n",
    "mses = []\n",
    "val_mses = []\n",
    "mse_hists = []\n",
    "val_mse_hists = []\n",
    "spearmans = []\n",
    "pearsons = []\n",
    "mean_aes = []\n",
    "median_aes = []\n",
    "\n",
    "#for train_index, val_index in group_kfold.split(ecg_X_k, ecg_y_k, ecg_id_k):  #for full 10-fold cross validation\n",
    "\n",
    "for i in range(1):\n",
    "    \n",
    "    K.clear_session()\n",
    "\n",
    "    adamw = AdamW(weight_decay=weight_d)\n",
    "    #adam = optimizers.Adam(learning_rate= set_learning_rate, clipnorm=1.)\n",
    "    #rmsprop = optimizers.RMSprop(learning_rate= set_learning_rate, clipnorm=1.)\n",
    "    #sgdopt = optimizers.SGD(learning_rate= set_learning_rate, clipnorm=1.)\n",
    "    #reduce_lr = tf.keras.callbacks.ReduceLROnPlateau()\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(optimizer= adamw, loss = set_loss, metrics= set_metrics)\n",
    "    model.load_weights('temp4.h5')\n",
    "    \n",
    "    \n",
    "    \n",
    "    corr_check = corr_checkpointer(X_val_list[i], y_val_list[i], filepath= (filename_base + '_fold_' + str(i) + '.h5'))\n",
    "    early_stopper = corr_early_stopping(min_epochs=stopping_min_epochs, mode='max', patience=num_patience, verbose=1)\n",
    "    \n",
    "    onecyc = OneCycle(lr_range=(0.0005, 0.005), momentum_range=(0.95,0.85))\n",
    "    \n",
    "    #restore_best_weights not available\n",
    "    #checkpointer = ModelCheckpoint(filename, monitor = corr_check, mode='min', verbose=1, save_best_only = True)\n",
    "    #reduce_lr = tf.keras.callbacks.ReduceLROnPlateau()\n",
    "\n",
    "\n",
    "    history = model.fit(X_train_list[i], y_train_list[i], validation_data = (X_val_list[i], y_val_list[i]), batch_size = num_batch_size, epochs=num_epochs, callbacks=[corr_check, onecyc, early_stopper]) #tensorboard_callback])\n",
    "    \n",
    "    best_epoch = len(history.history['val_mean_squared_error']) - num_patience\n",
    "    mse_model = history.history['mean_squared_error'][-(num_patience + 1)]\n",
    "    val_mse_model = history.history['val_mean_squared_error'][-(num_patience + 1)]\n",
    "    mse_hist = history.history['mean_squared_error']\n",
    "    val_mse_hist = history.history['val_mean_squared_error']\n",
    "\n",
    "    model = load_model(filename_base + '_fold_' + str(i) + '.h5', custom_objects={'AdamW':adamw})\n",
    "    preds_val = model.predict(X_val_list[i], verbose=1, batch_size=1024)\n",
    "    preds_val = preds_val.reshape(preds_val.shape[0])\n",
    "\n",
    "    spearman = stats.spearmanr(preds_val, y_val_list[i])\n",
    "    pearson = stats.pearsonr(preds_val, y_val_list[i])\n",
    "    \n",
    "    mean_ae = np.mean(abs(preds_val - y_val_list[i]))\n",
    "    median_ae = np.median(abs(preds_val - y_val_list[i]))\n",
    "    \n",
    "    \n",
    "    best_epochs.append(best_epoch)\n",
    "    mses.append(mse_model)\n",
    "    val_mses.append(val_mse_model)\n",
    "    mse_hists.append(mse_hist)\n",
    "    val_mse_hists.append(val_mse_hist)\n",
    "    spearmans.append(spearman)\n",
    "    pearsons.append(pearson)\n",
    "    mean_aes.append(mean_ae)\n",
    "    median_aes.append(median_ae)\n",
    "\n",
    "print('Best epoch: ', best_epochs)\n",
    "print('MSE: ', mses)\n",
    "print('Val MSE: ', val_mses)\n",
    "print('Spearman: ', spearmans)\n",
    "print('Pearson: ', pearsons)\n",
    "print('Mean abs error: ', mean_aes)\n",
    "print('Median abs error: ', median_aes)\n",
    "\n",
    "\n",
    "#print('Mean MSE: ', np.mean(mses))\n",
    "#print('Mean val_MSE: ', np.mean(val_mses))\n",
    "#print('Mean Spearman r: ', [np.mean(i) for i in zip(*spearmans)])\n",
    "#print('Mean Pearson r: ', [np.mean(i) for i in zip(*pearsons)])\n",
    "#print('Mean of Mean abs error: ', np.mean(mean_aes))\n",
    "#print('Mean of Median abs error: ', np.mean(median_aes))\n",
    "\n",
    "\n",
    "mean_mse = [np.mean(i) for i in zip(*mse_hists)]\n",
    "mean_val_mse = [np.mean(i) for i in zip(*val_mse_hists)]\n",
    "plt.plot(mean_mse[3:])\n",
    "plt.plot(mean_val_mse[3:])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#for j in range(len(mse_hists)):\n",
    "#    plt.plot(mse_hists[j])\n",
    "#    plt.plot(val_mse_hists[j])\n",
    "#    plt.show()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "current_time2 = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Start time: \", current_time)\n",
    "print(\"End time: \", current_time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mean_mse[60:1000])\n",
    "plt.plot(mean_val_mse[60:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bland_altman_plot(preds_val, y_val_list[i], scatter_kwds={'s':10}, title='MGH/BWH 12-Lead ECGs (10 secs)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7,7)) #(1, 1, figsize=(12, 12))\n",
    "    \n",
    "ax.set_title('MGH/BWH 12-Lead ECGs (10 secs)', fontsize=14)\n",
    "ax.scatter(y_val_list[i], preds_val, s=10)\n",
    "\n",
    "ax.set(xlim = [0, 420], xticks = np.arange(0, 420, 50),\n",
    "       ylim = [0, 420], yticks = np.arange(0, 420, 50),\n",
    "       xlabel = 'ECG PR Interval',\n",
    "       ylabel = 'Predicted PR Interval')\n",
    "ax.plot([0, 450], [0,450], alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onecyc2 = OneCycle(lr_range=(0.001, 0.01), momentum_range=(0.95,0.85))\n",
    "history2 = model.fit(X_train_list[i], y_train_list[i], validation_data = (X_val_list[i], y_val_list[i]), batch_size = num_batch_size, epochs=num_epochs, callbacks=[corr_check, onecyc, early_stopper]) #tensorboard_callback])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "model = load_model(filename_base + '_fold_' + str(i) + '.h5', custom_objects={'AdamW':adamw})\n",
    "preds_val = model.predict(X_val_list[i], verbose=1, batch_size=1024)\n",
    "preds_val = preds_val.reshape(preds_val.shape[0])\n",
    "\n",
    "spearman = stats.spearmanr(preds_val, y_val_list[i])\n",
    "pearson = stats.pearsonr(preds_val, y_val_list[i])\n",
    "    \n",
    "mean_ae = np.mean(abs(preds_val - y_val_list[i]))\n",
    "median_ae = np.median(abs(preds_val - y_val_list[i]))\n",
    "    \n",
    "print('Spearman: ', spearman)\n",
    "print('Pearson: ', pearson)\n",
    "print('Mean abs error: ', mean_ae)\n",
    "print('Median abs error: ', median_ae)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6)) #(1, 1, figsize=(12, 12))\n",
    "    \n",
    "fig.suptitle('12-Lead ECGs (47)')\n",
    "ax.scatter(y_val_list[i], preds_val)\n",
    "ax.set(xlim = [0, 350], xticks = np.arange(0, 350, 50),\n",
    "       ylim = [0, 350], yticks = np.arange(0, 350, 50),\n",
    "       xlabel = 'ECG PR Interval',\n",
    "       ylabel = 'Predicted PR Interval')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "filename_base = 'trained_' + suffix + '_58'\n",
    "print(filename_base)\n",
    "\n",
    "in_neurons=60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input((in_neurons, 1))\n",
    "\n",
    "c1 = Conv1D(16,4, padding='same', strides=2, use_bias = False, kernel_initializer= 'he_normal') (inputs)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Activation('relu') (c1)\n",
    "c1 = Dropout(0.1) (c1)\n",
    "\n",
    "s1 = Conv1D(16, 8, padding='same', strides=4, use_bias=False, kernel_initializer= 'he_normal') (c1)\n",
    "s1 = BatchNormalization() (s1)\n",
    "\n",
    "\n",
    "c2 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=2, kernel_initializer= 'he_normal')) (c1)\n",
    "c2 = Activation('relu') (c2)\n",
    "c2 = SpatialDropout1D(0.1) (c2)\n",
    "c2 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=4, kernel_initializer= 'he_normal')) (c2)\n",
    "#c2 = Activation('relu') (c2)\n",
    "c2 = Activation('relu') (c1+c2)\n",
    "c2 = SpatialDropout1D(0.1) (c2)\n",
    "c2 = MaxPooling1D(pool_size=1, strides=4) (c2)\n",
    "\n",
    "\n",
    "\n",
    "c3 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=2, kernel_initializer= 'he_normal')) (c2)\n",
    "c3 = Activation('relu') (c3)\n",
    "c3 = SpatialDropout1D(0.1) (c3)\n",
    "c3 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=4, kernel_initializer= 'he_normal')) (c3)\n",
    "#c3 = Activation('relu') (c3)\n",
    "c3 = Activation('relu') (s1+c3)\n",
    "c3 = SpatialDropout1D(0.1) (c3)\n",
    "c3 = MaxPooling1D(pool_size=1, strides=8) (c3)\n",
    "\n",
    "c7 = Conv1D(1, 3, use_bias = False, kernel_initializer= 'he_normal') (c3)\n",
    "c7 = BatchNormalization() (c7)\n",
    "c7 = Activation('relu') (c7)\n",
    "c7 = Dropout(0.3) (c7)\n",
    "\n",
    "final = Flatten() (c7)\n",
    "\n",
    "outputs = Dense(1) (final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_loss = 'mean_squared_error'\n",
    "set_metrics = ['mean_squared_error']\n",
    "#set_learning_rate = 0.007\n",
    "\n",
    "num_batch_size=245\n",
    "num_epochs=500\n",
    "num_patience=200\n",
    "stopping_min_epochs=400\n",
    "weight_d = 0  # 0.0000001\n",
    "\n",
    "adamw = AdamW(weight_decay=weight_d)\n",
    "#adam = optimizers.Adam(learning_rate= set_learning_rate)\n",
    "#reduce_lr = tf.keras.callbacks.ReduceLROnPlateau()\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[outputs])\n",
    "model.compile(optimizer= adamw, loss = set_loss, metrics= ['mean_squared_error'])\n",
    "model.save_weights('temp4.h5')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ocp = OneCycle(lr_range=(0.0007, 0.007), momentum_range=(0.94,0.85))\n",
    "#ocp.test_run(128)\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_finder = LRFinder(start_lr= 0.0001, end_lr= 1, max_steps=20)\n",
    "\n",
    "#model.fit(X_train_list[0], y_train_list[0], callbacks=[lr_finder])\n",
    "#lr_finder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_epochs = []\n",
    "mses = []\n",
    "val_mses = []\n",
    "mse_hists = []\n",
    "val_mse_hists = []\n",
    "spearmans = []\n",
    "pearsons = []\n",
    "mean_aes = []\n",
    "median_aes = []\n",
    "\n",
    "#for train_index, val_index in group_kfold.split(ecg_X_k, ecg_y_k, ecg_id_k):  #for full 10-fold cross validation\n",
    "\n",
    "for i in range(1):\n",
    "    \n",
    "    K.clear_session()\n",
    "\n",
    "    adamw = AdamW(weight_decay=weight_d)\n",
    "    #adam = optimizers.Adam(learning_rate= set_learning_rate, clipnorm=1.)\n",
    "    #rmsprop = optimizers.RMSprop(learning_rate= set_learning_rate, clipnorm=1.)\n",
    "    #sgdopt = optimizers.SGD(learning_rate= set_learning_rate, clipnorm=1.)\n",
    "    #reduce_lr = tf.keras.callbacks.ReduceLROnPlateau()\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(optimizer= adamw, loss = set_loss, metrics= set_metrics)\n",
    "    model.load_weights('temp4.h5')\n",
    "    \n",
    "    \n",
    "    \n",
    "    corr_check = corr_checkpointer(X_val_list[i], y_val_list[i], filepath= (filename_base + '_fold_' + str(i) + '.h5'))\n",
    "    early_stopper = corr_early_stopping(min_epochs=stopping_min_epochs, mode='max', patience=num_patience, verbose=1)\n",
    "    \n",
    "    onecyc = OneCycle(lr_range=(0.0005, 0.005), momentum_range=(0.95,0.85))\n",
    "    \n",
    "    #restore_best_weights not available\n",
    "    #checkpointer = ModelCheckpoint(filename, monitor = corr_check, mode='min', verbose=1, save_best_only = True)\n",
    "    #reduce_lr = tf.keras.callbacks.ReduceLROnPlateau()\n",
    "\n",
    "\n",
    "    history = model.fit(X_train_list[i], y_train_list[i], validation_data = (X_val_list[i], y_val_list[i]), batch_size = num_batch_size, epochs=num_epochs, callbacks=[corr_check, onecyc, early_stopper]) #tensorboard_callback])\n",
    "    \n",
    "    best_epoch = len(history.history['val_mean_squared_error']) - num_patience\n",
    "    mse_model = history.history['mean_squared_error'][-(num_patience + 1)]\n",
    "    val_mse_model = history.history['val_mean_squared_error'][-(num_patience + 1)]\n",
    "    mse_hist = history.history['mean_squared_error']\n",
    "    val_mse_hist = history.history['val_mean_squared_error']\n",
    "\n",
    "    model = load_model(filename_base + '_fold_' + str(i) + '.h5', custom_objects={'AdamW':adamw})\n",
    "    preds_val = model.predict(X_val_list[i], verbose=1, batch_size=1024)\n",
    "    preds_val = preds_val.reshape(preds_val.shape[0])\n",
    "\n",
    "    spearman = stats.spearmanr(preds_val, y_val_list[i])\n",
    "    pearson = stats.pearsonr(preds_val, y_val_list[i])\n",
    "    \n",
    "    mean_ae = np.mean(abs(preds_val - y_val_list[i]))\n",
    "    median_ae = np.median(abs(preds_val - y_val_list[i]))\n",
    "    \n",
    "    \n",
    "    best_epochs.append(best_epoch)\n",
    "    mses.append(mse_model)\n",
    "    val_mses.append(val_mse_model)\n",
    "    mse_hists.append(mse_hist)\n",
    "    val_mse_hists.append(val_mse_hist)\n",
    "    spearmans.append(spearman)\n",
    "    pearsons.append(pearson)\n",
    "    mean_aes.append(mean_ae)\n",
    "    median_aes.append(median_ae)\n",
    "\n",
    "print('Best epoch: ', best_epochs)\n",
    "print('MSE: ', mses)\n",
    "print('Val MSE: ', val_mses)\n",
    "print('Spearman: ', spearmans)\n",
    "print('Pearson: ', pearsons)\n",
    "print('Mean abs error: ', mean_aes)\n",
    "print('Median abs error: ', median_aes)\n",
    "\n",
    "\n",
    "#print('Mean MSE: ', np.mean(mses))\n",
    "#print('Mean val_MSE: ', np.mean(val_mses))\n",
    "#print('Mean Spearman r: ', [np.mean(i) for i in zip(*spearmans)])\n",
    "#print('Mean Pearson r: ', [np.mean(i) for i in zip(*pearsons)])\n",
    "#print('Mean of Mean abs error: ', np.mean(mean_aes))\n",
    "#print('Mean of Median abs error: ', np.mean(median_aes))\n",
    "\n",
    "\n",
    "mean_mse = [np.mean(i) for i in zip(*mse_hists)]\n",
    "mean_val_mse = [np.mean(i) for i in zip(*val_mse_hists)]\n",
    "plt.plot(mean_mse[3:])\n",
    "plt.plot(mean_val_mse[3:])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#for j in range(len(mse_hists)):\n",
    "#    plt.plot(mse_hists[j])\n",
    "#    plt.plot(val_mse_hists[j])\n",
    "#    plt.show()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "current_time2 = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Start time: \", current_time)\n",
    "print(\"End time: \", current_time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mean_mse[60:1000])\n",
    "plt.plot(mean_val_mse[60:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bland_altman_plot(preds_val, y_val_list[i], scatter_kwds={'s':10}, title='MGH/BWH 12-Lead ECGs (10 secs)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7,7)) #(1, 1, figsize=(12, 12))\n",
    "    \n",
    "ax.set_title('MGH/BWH 12-Lead ECGs (10 secs)', fontsize=14)\n",
    "ax.scatter(y_val_list[i], preds_val, s=10)\n",
    "\n",
    "ax.set(xlim = [0, 420], xticks = np.arange(0, 420, 50),\n",
    "       ylim = [0, 420], yticks = np.arange(0, 420, 50),\n",
    "       xlabel = 'ECG PR Interval',\n",
    "       ylabel = 'Predicted PR Interval')\n",
    "ax.plot([0, 450], [0,450], alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onecyc2 = OneCycle(lr_range=(0.001, 0.01), momentum_range=(0.95,0.85))\n",
    "history2 = model.fit(X_train_list[i], y_train_list[i], validation_data = (X_val_list[i], y_val_list[i]), batch_size = num_batch_size, epochs=num_epochs, callbacks=[corr_check, onecyc, early_stopper]) #tensorboard_callback])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "model = load_model(filename_base + '_fold_' + str(i) + '.h5', custom_objects={'AdamW':adamw})\n",
    "preds_val = model.predict(X_val_list[i], verbose=1, batch_size=1024)\n",
    "preds_val = preds_val.reshape(preds_val.shape[0])\n",
    "\n",
    "spearman = stats.spearmanr(preds_val, y_val_list[i])\n",
    "pearson = stats.pearsonr(preds_val, y_val_list[i])\n",
    "    \n",
    "mean_ae = np.mean(abs(preds_val - y_val_list[i]))\n",
    "median_ae = np.median(abs(preds_val - y_val_list[i]))\n",
    "    \n",
    "print('Spearman: ', spearman)\n",
    "print('Pearson: ', pearson)\n",
    "print('Mean abs error: ', mean_ae)\n",
    "print('Median abs error: ', median_ae)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6)) #(1, 1, figsize=(12, 12))\n",
    "    \n",
    "fig.suptitle('12-Lead ECGs (47)')\n",
    "ax.scatter(y_val_list[i], preds_val)\n",
    "ax.set(xlim = [0, 350], xticks = np.arange(0, 350, 50),\n",
    "       ylim = [0, 350], yticks = np.arange(0, 350, 50),\n",
    "       xlabel = 'ECG PR Interval',\n",
    "       ylabel = 'Predicted PR Interval')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "filename_base = 'trained_' + suffix + '_59'\n",
    "print(filename_base)\n",
    "\n",
    "in_neurons=60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input((in_neurons, 1))\n",
    "\n",
    "c1 = Conv1D(16,4, padding='same', strides=2, use_bias = False, kernel_initializer= 'he_normal') (inputs)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Activation('relu') (c1)\n",
    "#c1 = Dropout(0.1) (c1)\n",
    "\n",
    "s1 = Conv1D(16, 8, padding='same', strides=4, use_bias=False, kernel_initializer= 'he_normal') (c1)\n",
    "s1 = BatchNormalization() (s1)\n",
    "\n",
    "\n",
    "c2 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=2, kernel_initializer= 'he_normal')) (c1)\n",
    "c2 = Activation('relu') (c2)\n",
    "#c2 = SpatialDropout1D(0.1) (c2)\n",
    "c2 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=4, kernel_initializer= 'he_normal')) (c2)\n",
    "#c2 = Activation('relu') (c2)\n",
    "c2 = Activation('relu') (c1+c2)\n",
    "c2 = SpatialDropout1D(0.1) (c2)\n",
    "c2 = MaxPooling1D(pool_size=1, strides=4) (c2)\n",
    "\n",
    "\n",
    "\n",
    "c3 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=2, kernel_initializer= 'he_normal')) (c2)\n",
    "c3 = Activation('relu') (c3)\n",
    "#c3 = SpatialDropout1D(0.2) (c3)\n",
    "c3 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=4, kernel_initializer= 'he_normal')) (c3)\n",
    "#c3 = Activation('relu') (c3)\n",
    "c3 = Activation('relu') (s1+c3)\n",
    "c3 = SpatialDropout1D(0.1) (c3)\n",
    "c3 = MaxPooling1D(pool_size=1, strides=8) (c3)\n",
    "\n",
    "c7 = Conv1D(1, 3, use_bias = False, kernel_initializer= 'he_normal') (c3)\n",
    "c7 = BatchNormalization() (c7)\n",
    "c7 = Activation('relu') (c7)\n",
    "c7 = Dropout(0.3) (c7)\n",
    "\n",
    "final = Flatten() (c7)\n",
    "\n",
    "outputs = Dense(1) (final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_loss = 'mean_squared_error'\n",
    "set_metrics = ['mean_squared_error']\n",
    "#set_learning_rate = 0.007\n",
    "\n",
    "num_batch_size=245\n",
    "num_epochs=500\n",
    "num_patience=200\n",
    "stopping_min_epochs=400\n",
    "weight_d = 0  # 0.0000001\n",
    "\n",
    "adamw = AdamW(weight_decay=weight_d)\n",
    "#adam = optimizers.Adam(learning_rate= set_learning_rate)\n",
    "#reduce_lr = tf.keras.callbacks.ReduceLROnPlateau()\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[outputs])\n",
    "model.compile(optimizer= adamw, loss = set_loss, metrics= ['mean_squared_error'])\n",
    "model.save_weights('temp4.h5')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ocp = OneCycle(lr_range=(0.0007, 0.007), momentum_range=(0.94,0.85))\n",
    "#ocp.test_run(128)\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_finder = LRFinder(start_lr= 0.0001, end_lr= 1, max_steps=20)\n",
    "\n",
    "#model.fit(X_train_list[0], y_train_list[0], callbacks=[lr_finder])\n",
    "#lr_finder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_epochs = []\n",
    "mses = []\n",
    "val_mses = []\n",
    "mse_hists = []\n",
    "val_mse_hists = []\n",
    "spearmans = []\n",
    "pearsons = []\n",
    "mean_aes = []\n",
    "median_aes = []\n",
    "\n",
    "#for train_index, val_index in group_kfold.split(ecg_X_k, ecg_y_k, ecg_id_k):  #for full 10-fold cross validation\n",
    "\n",
    "for i in range(1):\n",
    "    \n",
    "    K.clear_session()\n",
    "\n",
    "    adamw = AdamW(weight_decay=weight_d)\n",
    "    #adam = optimizers.Adam(learning_rate= set_learning_rate, clipnorm=1.)\n",
    "    #rmsprop = optimizers.RMSprop(learning_rate= set_learning_rate, clipnorm=1.)\n",
    "    #sgdopt = optimizers.SGD(learning_rate= set_learning_rate, clipnorm=1.)\n",
    "    #reduce_lr = tf.keras.callbacks.ReduceLROnPlateau()\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(optimizer= adamw, loss = set_loss, metrics= set_metrics)\n",
    "    model.load_weights('temp4.h5')\n",
    "    \n",
    "    \n",
    "    \n",
    "    corr_check = corr_checkpointer(X_val_list[i], y_val_list[i], filepath= (filename_base + '_fold_' + str(i) + '.h5'))\n",
    "    early_stopper = corr_early_stopping(min_epochs=stopping_min_epochs, mode='max', patience=num_patience, verbose=1)\n",
    "    \n",
    "    onecyc = OneCycle(lr_range=(0.0005, 0.005), momentum_range=(0.95,0.85))\n",
    "    \n",
    "    #restore_best_weights not available\n",
    "    #checkpointer = ModelCheckpoint(filename, monitor = corr_check, mode='min', verbose=1, save_best_only = True)\n",
    "    #reduce_lr = tf.keras.callbacks.ReduceLROnPlateau()\n",
    "\n",
    "\n",
    "    history = model.fit(X_train_list[i], y_train_list[i], validation_data = (X_val_list[i], y_val_list[i]), batch_size = num_batch_size, epochs=num_epochs, callbacks=[corr_check, onecyc, early_stopper]) #tensorboard_callback])\n",
    "    \n",
    "    best_epoch = len(history.history['val_mean_squared_error']) - num_patience\n",
    "    mse_model = history.history['mean_squared_error'][-(num_patience + 1)]\n",
    "    val_mse_model = history.history['val_mean_squared_error'][-(num_patience + 1)]\n",
    "    mse_hist = history.history['mean_squared_error']\n",
    "    val_mse_hist = history.history['val_mean_squared_error']\n",
    "\n",
    "    model = load_model(filename_base + '_fold_' + str(i) + '.h5', custom_objects={'AdamW':adamw})\n",
    "    preds_val = model.predict(X_val_list[i], verbose=1, batch_size=1024)\n",
    "    preds_val = preds_val.reshape(preds_val.shape[0])\n",
    "\n",
    "    spearman = stats.spearmanr(preds_val, y_val_list[i])\n",
    "    pearson = stats.pearsonr(preds_val, y_val_list[i])\n",
    "    \n",
    "    mean_ae = np.mean(abs(preds_val - y_val_list[i]))\n",
    "    median_ae = np.median(abs(preds_val - y_val_list[i]))\n",
    "    \n",
    "    \n",
    "    best_epochs.append(best_epoch)\n",
    "    mses.append(mse_model)\n",
    "    val_mses.append(val_mse_model)\n",
    "    mse_hists.append(mse_hist)\n",
    "    val_mse_hists.append(val_mse_hist)\n",
    "    spearmans.append(spearman)\n",
    "    pearsons.append(pearson)\n",
    "    mean_aes.append(mean_ae)\n",
    "    median_aes.append(median_ae)\n",
    "\n",
    "print('Best epoch: ', best_epochs)\n",
    "print('MSE: ', mses)\n",
    "print('Val MSE: ', val_mses)\n",
    "print('Spearman: ', spearmans)\n",
    "print('Pearson: ', pearsons)\n",
    "print('Mean abs error: ', mean_aes)\n",
    "print('Median abs error: ', median_aes)\n",
    "\n",
    "\n",
    "#print('Mean MSE: ', np.mean(mses))\n",
    "#print('Mean val_MSE: ', np.mean(val_mses))\n",
    "#print('Mean Spearman r: ', [np.mean(i) for i in zip(*spearmans)])\n",
    "#print('Mean Pearson r: ', [np.mean(i) for i in zip(*pearsons)])\n",
    "#print('Mean of Mean abs error: ', np.mean(mean_aes))\n",
    "#print('Mean of Median abs error: ', np.mean(median_aes))\n",
    "\n",
    "\n",
    "mean_mse = [np.mean(i) for i in zip(*mse_hists)]\n",
    "mean_val_mse = [np.mean(i) for i in zip(*val_mse_hists)]\n",
    "plt.plot(mean_mse[3:])\n",
    "plt.plot(mean_val_mse[3:])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#for j in range(len(mse_hists)):\n",
    "#    plt.plot(mse_hists[j])\n",
    "#    plt.plot(val_mse_hists[j])\n",
    "#    plt.show()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "current_time2 = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Start time: \", current_time)\n",
    "print(\"End time: \", current_time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mean_mse[60:1000])\n",
    "plt.plot(mean_val_mse[60:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bland_altman_plot(preds_val, y_val_list[i], scatter_kwds={'s':10}, title='MGH/BWH 12-Lead ECGs (10 secs)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7,7)) #(1, 1, figsize=(12, 12))\n",
    "    \n",
    "ax.set_title('MGH/BWH 12-Lead ECGs (10 secs)', fontsize=14)\n",
    "ax.scatter(y_val_list[i], preds_val, s=10)\n",
    "\n",
    "ax.set(xlim = [0, 420], xticks = np.arange(0, 420, 50),\n",
    "       ylim = [0, 420], yticks = np.arange(0, 420, 50),\n",
    "       xlabel = 'ECG PR Interval',\n",
    "       ylabel = 'Predicted PR Interval')\n",
    "ax.plot([0, 450], [0,450], alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onecyc2 = OneCycle(lr_range=(0.001, 0.01), momentum_range=(0.95,0.85))\n",
    "history2 = model.fit(X_train_list[i], y_train_list[i], validation_data = (X_val_list[i], y_val_list[i]), batch_size = num_batch_size, epochs=num_epochs, callbacks=[corr_check, onecyc, early_stopper]) #tensorboard_callback])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "model = load_model(filename_base + '_fold_' + str(i) + '.h5', custom_objects={'AdamW':adamw})\n",
    "preds_val = model.predict(X_val_list[i], verbose=1, batch_size=1024)\n",
    "preds_val = preds_val.reshape(preds_val.shape[0])\n",
    "\n",
    "spearman = stats.spearmanr(preds_val, y_val_list[i])\n",
    "pearson = stats.pearsonr(preds_val, y_val_list[i])\n",
    "    \n",
    "mean_ae = np.mean(abs(preds_val - y_val_list[i]))\n",
    "median_ae = np.median(abs(preds_val - y_val_list[i]))\n",
    "    \n",
    "print('Spearman: ', spearman)\n",
    "print('Pearson: ', pearson)\n",
    "print('Mean abs error: ', mean_ae)\n",
    "print('Median abs error: ', median_ae)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6)) #(1, 1, figsize=(12, 12))\n",
    "    \n",
    "fig.suptitle('12-Lead ECGs (47)')\n",
    "ax.scatter(y_val_list[i], preds_val)\n",
    "ax.set(xlim = [0, 350], xticks = np.arange(0, 350, 50),\n",
    "       ylim = [0, 350], yticks = np.arange(0, 350, 50),\n",
    "       xlabel = 'ECG PR Interval',\n",
    "       ylabel = 'Predicted PR Interval')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "filename_base = 'trained_' + suffix + '_60'\n",
    "print(filename_base)\n",
    "\n",
    "in_neurons=60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input((in_neurons, 1))\n",
    "\n",
    "c1 = Conv1D(16,4, padding='same', strides=2, use_bias = False, kernel_initializer= 'he_normal') (inputs)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Activation('relu') (c1)\n",
    "#c1 = Dropout(0.1) (c1)\n",
    "\n",
    "s1 = Conv1D(16, 8, padding='same', strides=4, use_bias=False, kernel_initializer= 'he_normal') (c1)\n",
    "s1 = BatchNormalization() (s1)\n",
    "\n",
    "\n",
    "c2 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=2, kernel_initializer= 'he_normal')) (c1)\n",
    "c2 = Activation('relu') (c2)\n",
    "#c2 = SpatialDropout1D(0.2) (c2)\n",
    "c2 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=4, kernel_initializer= 'he_normal')) (c2)\n",
    "#c2 = Activation('relu') (c2)\n",
    "c2 = Activation('relu') (c1+c2)\n",
    "#c2 = SpatialDropout1D(0.2) (c2)\n",
    "c2 = MaxPooling1D(pool_size=1, strides=4) (c2)\n",
    "\n",
    "\n",
    "\n",
    "c3 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=2, kernel_initializer= 'he_normal')) (c2)\n",
    "c3 = Activation('relu') (c3)\n",
    "#c3 = SpatialDropout1D(0.2) (c3)\n",
    "c3 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=4, kernel_initializer= 'he_normal')) (c3)\n",
    "#c3 = Activation('relu') (c3)\n",
    "c3 = Activation('relu') (s1+c3)\n",
    "#c3 = SpatialDropout1D(0.2) (c3)\n",
    "c3 = MaxPooling1D(pool_size=1, strides=8) (c3)\n",
    "\n",
    "c7 = Conv1D(1, 3, use_bias = False, kernel_initializer= 'he_normal') (c3)\n",
    "c7 = BatchNormalization() (c7)\n",
    "c7 = Activation('relu') (c7)\n",
    "c7 = Dropout(0.4) (c7)\n",
    "\n",
    "final = Flatten() (c7)\n",
    "\n",
    "outputs = Dense(1) (final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_loss = 'mean_squared_error'\n",
    "set_metrics = ['mean_squared_error']\n",
    "#set_learning_rate = 0.007\n",
    "\n",
    "num_batch_size=245\n",
    "num_epochs=500\n",
    "num_patience=200\n",
    "stopping_min_epochs=400\n",
    "weight_d = 0  # 0.0000001\n",
    "\n",
    "adamw = AdamW(weight_decay=weight_d)\n",
    "#adam = optimizers.Adam(learning_rate= set_learning_rate)\n",
    "#reduce_lr = tf.keras.callbacks.ReduceLROnPlateau()\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[outputs])\n",
    "model.compile(optimizer= adamw, loss = set_loss, metrics= ['mean_squared_error'])\n",
    "model.save_weights('temp4.h5')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ocp = OneCycle(lr_range=(0.0007, 0.007), momentum_range=(0.94,0.85))\n",
    "#ocp.test_run(128)\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_finder = LRFinder(start_lr= 0.0001, end_lr= 1, max_steps=20)\n",
    "\n",
    "#model.fit(X_train_list[0], y_train_list[0], callbacks=[lr_finder])\n",
    "#lr_finder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_epochs = []\n",
    "mses = []\n",
    "val_mses = []\n",
    "mse_hists = []\n",
    "val_mse_hists = []\n",
    "spearmans = []\n",
    "pearsons = []\n",
    "mean_aes = []\n",
    "median_aes = []\n",
    "\n",
    "#for train_index, val_index in group_kfold.split(ecg_X_k, ecg_y_k, ecg_id_k):  #for full 10-fold cross validation\n",
    "\n",
    "for i in range(1):\n",
    "    \n",
    "    K.clear_session()\n",
    "\n",
    "    adamw = AdamW(weight_decay=weight_d)\n",
    "    #adam = optimizers.Adam(learning_rate= set_learning_rate, clipnorm=1.)\n",
    "    #rmsprop = optimizers.RMSprop(learning_rate= set_learning_rate, clipnorm=1.)\n",
    "    #sgdopt = optimizers.SGD(learning_rate= set_learning_rate, clipnorm=1.)\n",
    "    #reduce_lr = tf.keras.callbacks.ReduceLROnPlateau()\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(optimizer= adamw, loss = set_loss, metrics= set_metrics)\n",
    "    model.load_weights('temp4.h5')\n",
    "    \n",
    "    \n",
    "    \n",
    "    corr_check = corr_checkpointer(X_val_list[i], y_val_list[i], filepath= (filename_base + '_fold_' + str(i) + '.h5'))\n",
    "    early_stopper = corr_early_stopping(min_epochs=stopping_min_epochs, mode='max', patience=num_patience, verbose=1)\n",
    "    \n",
    "    onecyc = OneCycle(lr_range=(0.0005, 0.005), momentum_range=(0.95,0.85))\n",
    "    \n",
    "    #restore_best_weights not available\n",
    "    #checkpointer = ModelCheckpoint(filename, monitor = corr_check, mode='min', verbose=1, save_best_only = True)\n",
    "    #reduce_lr = tf.keras.callbacks.ReduceLROnPlateau()\n",
    "\n",
    "\n",
    "    history = model.fit(X_train_list[i], y_train_list[i], validation_data = (X_val_list[i], y_val_list[i]), batch_size = num_batch_size, epochs=num_epochs, callbacks=[corr_check, onecyc, early_stopper]) #tensorboard_callback])\n",
    "    \n",
    "    best_epoch = len(history.history['val_mean_squared_error']) - num_patience\n",
    "    mse_model = history.history['mean_squared_error'][-(num_patience + 1)]\n",
    "    val_mse_model = history.history['val_mean_squared_error'][-(num_patience + 1)]\n",
    "    mse_hist = history.history['mean_squared_error']\n",
    "    val_mse_hist = history.history['val_mean_squared_error']\n",
    "\n",
    "    model = load_model(filename_base + '_fold_' + str(i) + '.h5', custom_objects={'AdamW':adamw})\n",
    "    preds_val = model.predict(X_val_list[i], verbose=1, batch_size=1024)\n",
    "    preds_val = preds_val.reshape(preds_val.shape[0])\n",
    "\n",
    "    spearman = stats.spearmanr(preds_val, y_val_list[i])\n",
    "    pearson = stats.pearsonr(preds_val, y_val_list[i])\n",
    "    \n",
    "    mean_ae = np.mean(abs(preds_val - y_val_list[i]))\n",
    "    median_ae = np.median(abs(preds_val - y_val_list[i]))\n",
    "    \n",
    "    \n",
    "    best_epochs.append(best_epoch)\n",
    "    mses.append(mse_model)\n",
    "    val_mses.append(val_mse_model)\n",
    "    mse_hists.append(mse_hist)\n",
    "    val_mse_hists.append(val_mse_hist)\n",
    "    spearmans.append(spearman)\n",
    "    pearsons.append(pearson)\n",
    "    mean_aes.append(mean_ae)\n",
    "    median_aes.append(median_ae)\n",
    "\n",
    "print('Best epoch: ', best_epochs)\n",
    "print('MSE: ', mses)\n",
    "print('Val MSE: ', val_mses)\n",
    "print('Spearman: ', spearmans)\n",
    "print('Pearson: ', pearsons)\n",
    "print('Mean abs error: ', mean_aes)\n",
    "print('Median abs error: ', median_aes)\n",
    "\n",
    "\n",
    "#print('Mean MSE: ', np.mean(mses))\n",
    "#print('Mean val_MSE: ', np.mean(val_mses))\n",
    "#print('Mean Spearman r: ', [np.mean(i) for i in zip(*spearmans)])\n",
    "#print('Mean Pearson r: ', [np.mean(i) for i in zip(*pearsons)])\n",
    "#print('Mean of Mean abs error: ', np.mean(mean_aes))\n",
    "#print('Mean of Median abs error: ', np.mean(median_aes))\n",
    "\n",
    "\n",
    "mean_mse = [np.mean(i) for i in zip(*mse_hists)]\n",
    "mean_val_mse = [np.mean(i) for i in zip(*val_mse_hists)]\n",
    "plt.plot(mean_mse[3:])\n",
    "plt.plot(mean_val_mse[3:])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#for j in range(len(mse_hists)):\n",
    "#    plt.plot(mse_hists[j])\n",
    "#    plt.plot(val_mse_hists[j])\n",
    "#    plt.show()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "current_time2 = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Start time: \", current_time)\n",
    "print(\"End time: \", current_time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mean_mse[60:1000])\n",
    "plt.plot(mean_val_mse[60:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bland_altman_plot(preds_val, y_val_list[i], scatter_kwds={'s':10}, title='MGH/BWH 12-Lead ECGs (10 secs)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7,7)) #(1, 1, figsize=(12, 12))\n",
    "    \n",
    "ax.set_title('MGH/BWH 12-Lead ECGs (10 secs)', fontsize=14)\n",
    "ax.scatter(y_val_list[i], preds_val, s=10)\n",
    "\n",
    "ax.set(xlim = [0, 420], xticks = np.arange(0, 420, 50),\n",
    "       ylim = [0, 420], yticks = np.arange(0, 420, 50),\n",
    "       xlabel = 'ECG PR Interval',\n",
    "       ylabel = 'Predicted PR Interval')\n",
    "ax.plot([0, 450], [0,450], alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onecyc2 = OneCycle(lr_range=(0.001, 0.01), momentum_range=(0.95,0.85))\n",
    "history2 = model.fit(X_train_list[i], y_train_list[i], validation_data = (X_val_list[i], y_val_list[i]), batch_size = num_batch_size, epochs=num_epochs, callbacks=[corr_check, onecyc, early_stopper]) #tensorboard_callback])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "model = load_model(filename_base + '_fold_' + str(i) + '.h5', custom_objects={'AdamW':adamw})\n",
    "preds_val = model.predict(X_val_list[i], verbose=1, batch_size=1024)\n",
    "preds_val = preds_val.reshape(preds_val.shape[0])\n",
    "\n",
    "spearman = stats.spearmanr(preds_val, y_val_list[i])\n",
    "pearson = stats.pearsonr(preds_val, y_val_list[i])\n",
    "    \n",
    "mean_ae = np.mean(abs(preds_val - y_val_list[i]))\n",
    "median_ae = np.median(abs(preds_val - y_val_list[i]))\n",
    "    \n",
    "print('Spearman: ', spearman)\n",
    "print('Pearson: ', pearson)\n",
    "print('Mean abs error: ', mean_ae)\n",
    "print('Median abs error: ', median_ae)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6)) #(1, 1, figsize=(12, 12))\n",
    "    \n",
    "fig.suptitle('12-Lead ECGs (47)')\n",
    "ax.scatter(y_val_list[i], preds_val)\n",
    "ax.set(xlim = [0, 350], xticks = np.arange(0, 350, 50),\n",
    "       ylim = [0, 350], yticks = np.arange(0, 350, 50),\n",
    "       xlabel = 'ECG PR Interval',\n",
    "       ylabel = 'Predicted PR Interval')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "filename_base = 'trained_' + suffix + '_61'\n",
    "print(filename_base)\n",
    "\n",
    "in_neurons=60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input((in_neurons, 1))\n",
    "\n",
    "c1 = Conv1D(16,4, padding='same', strides=2, use_bias = False, kernel_initializer= 'he_normal') (inputs)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Activation('relu') (c1)\n",
    "#c1 = Dropout(0.1) (c1)\n",
    "\n",
    "s1 = Conv1D(16, 8, padding='same', strides=4, use_bias=False, kernel_initializer= 'he_normal') (c1)\n",
    "s1 = BatchNormalization() (s1)\n",
    "\n",
    "\n",
    "c2 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=2, kernel_initializer= 'he_normal')) (c1)\n",
    "c2 = Activation('relu') (c2)\n",
    "#c2 = SpatialDropout1D(0.2) (c2)\n",
    "c2 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=4, kernel_initializer= 'he_normal')) (c2)\n",
    "#c2 = Activation('relu') (c2)\n",
    "c2 = Activation('relu') (c1+c2)\n",
    "#c2 = SpatialDropout1D(0.2) (c2)\n",
    "c2 = MaxPooling1D(pool_size=1, strides=4) (c2)\n",
    "\n",
    "\n",
    "\n",
    "c3 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=2, kernel_initializer= 'he_normal')) (c2)\n",
    "c3 = Activation('relu') (c3)\n",
    "#c3 = SpatialDropout1D(0.2) (c3)\n",
    "c3 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=4, kernel_initializer= 'he_normal')) (c3)\n",
    "#c3 = Activation('relu') (c3)\n",
    "c3 = Activation('relu') (s1+c3)\n",
    "#c3 = SpatialDropout1D(0.2) (c3)\n",
    "c3 = MaxPooling1D(pool_size=1, strides=8) (c3)\n",
    "\n",
    "c7 = Conv1D(1, 3, use_bias = False, kernel_initializer= 'he_normal') (c3)\n",
    "c7 = BatchNormalization() (c7)\n",
    "c7 = Activation('relu') (c7)\n",
    "c7 = Dropout(0.1) (c7)\n",
    "\n",
    "final = Flatten() (c7)\n",
    "\n",
    "outputs = Dense(1) (final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_loss = 'mean_squared_error'\n",
    "set_metrics = ['mean_squared_error']\n",
    "#set_learning_rate = 0.007\n",
    "\n",
    "num_batch_size=245\n",
    "num_epochs=500\n",
    "num_patience=200\n",
    "stopping_min_epochs=400\n",
    "weight_d = 0  # 0.0000001\n",
    "\n",
    "adamw = AdamW(weight_decay=weight_d)\n",
    "#adam = optimizers.Adam(learning_rate= set_learning_rate)\n",
    "#reduce_lr = tf.keras.callbacks.ReduceLROnPlateau()\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[outputs])\n",
    "model.compile(optimizer= adamw, loss = set_loss, metrics= ['mean_squared_error'])\n",
    "model.save_weights('temp4.h5')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ocp = OneCycle(lr_range=(0.0007, 0.007), momentum_range=(0.94,0.85))\n",
    "#ocp.test_run(128)\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_finder = LRFinder(start_lr= 0.0001, end_lr= 1, max_steps=20)\n",
    "\n",
    "#model.fit(X_train_list[0], y_train_list[0], callbacks=[lr_finder])\n",
    "#lr_finder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_epochs = []\n",
    "mses = []\n",
    "val_mses = []\n",
    "mse_hists = []\n",
    "val_mse_hists = []\n",
    "spearmans = []\n",
    "pearsons = []\n",
    "mean_aes = []\n",
    "median_aes = []\n",
    "\n",
    "#for train_index, val_index in group_kfold.split(ecg_X_k, ecg_y_k, ecg_id_k):  #for full 10-fold cross validation\n",
    "\n",
    "for i in range(1):\n",
    "    \n",
    "    K.clear_session()\n",
    "\n",
    "    adamw = AdamW(weight_decay=weight_d)\n",
    "    #adam = optimizers.Adam(learning_rate= set_learning_rate, clipnorm=1.)\n",
    "    #rmsprop = optimizers.RMSprop(learning_rate= set_learning_rate, clipnorm=1.)\n",
    "    #sgdopt = optimizers.SGD(learning_rate= set_learning_rate, clipnorm=1.)\n",
    "    #reduce_lr = tf.keras.callbacks.ReduceLROnPlateau()\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(optimizer= adamw, loss = set_loss, metrics= set_metrics)\n",
    "    model.load_weights('temp4.h5')\n",
    "    \n",
    "    \n",
    "    \n",
    "    corr_check = corr_checkpointer(X_val_list[i], y_val_list[i], filepath= (filename_base + '_fold_' + str(i) + '.h5'))\n",
    "    early_stopper = corr_early_stopping(min_epochs=stopping_min_epochs, mode='max', patience=num_patience, verbose=1)\n",
    "    \n",
    "    onecyc = OneCycle(lr_range=(0.0005, 0.005), momentum_range=(0.95,0.85))\n",
    "    \n",
    "    #restore_best_weights not available\n",
    "    #checkpointer = ModelCheckpoint(filename, monitor = corr_check, mode='min', verbose=1, save_best_only = True)\n",
    "    #reduce_lr = tf.keras.callbacks.ReduceLROnPlateau()\n",
    "\n",
    "\n",
    "    history = model.fit(X_train_list[i], y_train_list[i], validation_data = (X_val_list[i], y_val_list[i]), batch_size = num_batch_size, epochs=num_epochs, callbacks=[corr_check, onecyc, early_stopper]) #tensorboard_callback])\n",
    "    \n",
    "    best_epoch = len(history.history['val_mean_squared_error']) - num_patience\n",
    "    mse_model = history.history['mean_squared_error'][-(num_patience + 1)]\n",
    "    val_mse_model = history.history['val_mean_squared_error'][-(num_patience + 1)]\n",
    "    mse_hist = history.history['mean_squared_error']\n",
    "    val_mse_hist = history.history['val_mean_squared_error']\n",
    "\n",
    "    model = load_model(filename_base + '_fold_' + str(i) + '.h5', custom_objects={'AdamW':adamw})\n",
    "    preds_val = model.predict(X_val_list[i], verbose=1, batch_size=1024)\n",
    "    preds_val = preds_val.reshape(preds_val.shape[0])\n",
    "\n",
    "    spearman = stats.spearmanr(preds_val, y_val_list[i])\n",
    "    pearson = stats.pearsonr(preds_val, y_val_list[i])\n",
    "    \n",
    "    mean_ae = np.mean(abs(preds_val - y_val_list[i]))\n",
    "    median_ae = np.median(abs(preds_val - y_val_list[i]))\n",
    "    \n",
    "    \n",
    "    best_epochs.append(best_epoch)\n",
    "    mses.append(mse_model)\n",
    "    val_mses.append(val_mse_model)\n",
    "    mse_hists.append(mse_hist)\n",
    "    val_mse_hists.append(val_mse_hist)\n",
    "    spearmans.append(spearman)\n",
    "    pearsons.append(pearson)\n",
    "    mean_aes.append(mean_ae)\n",
    "    median_aes.append(median_ae)\n",
    "\n",
    "print('Best epoch: ', best_epochs)\n",
    "print('MSE: ', mses)\n",
    "print('Val MSE: ', val_mses)\n",
    "print('Spearman: ', spearmans)\n",
    "print('Pearson: ', pearsons)\n",
    "print('Mean abs error: ', mean_aes)\n",
    "print('Median abs error: ', median_aes)\n",
    "\n",
    "\n",
    "#print('Mean MSE: ', np.mean(mses))\n",
    "#print('Mean val_MSE: ', np.mean(val_mses))\n",
    "#print('Mean Spearman r: ', [np.mean(i) for i in zip(*spearmans)])\n",
    "#print('Mean Pearson r: ', [np.mean(i) for i in zip(*pearsons)])\n",
    "#print('Mean of Mean abs error: ', np.mean(mean_aes))\n",
    "#print('Mean of Median abs error: ', np.mean(median_aes))\n",
    "\n",
    "\n",
    "mean_mse = [np.mean(i) for i in zip(*mse_hists)]\n",
    "mean_val_mse = [np.mean(i) for i in zip(*val_mse_hists)]\n",
    "plt.plot(mean_mse[3:])\n",
    "plt.plot(mean_val_mse[3:])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#for j in range(len(mse_hists)):\n",
    "#    plt.plot(mse_hists[j])\n",
    "#    plt.plot(val_mse_hists[j])\n",
    "#    plt.show()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "current_time2 = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Start time: \", current_time)\n",
    "print(\"End time: \", current_time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mean_mse[60:1000])\n",
    "plt.plot(mean_val_mse[60:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bland_altman_plot(preds_val, y_val_list[i], scatter_kwds={'s':10}, title='MGH/BWH 12-Lead ECGs (10 secs)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7,7)) #(1, 1, figsize=(12, 12))\n",
    "    \n",
    "ax.set_title('MGH/BWH 12-Lead ECGs (10 secs)', fontsize=14)\n",
    "ax.scatter(y_val_list[i], preds_val, s=10)\n",
    "\n",
    "ax.set(xlim = [0, 420], xticks = np.arange(0, 420, 50),\n",
    "       ylim = [0, 420], yticks = np.arange(0, 420, 50),\n",
    "       xlabel = 'ECG PR Interval',\n",
    "       ylabel = 'Predicted PR Interval')\n",
    "ax.plot([0, 450], [0,450], alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onecyc2 = OneCycle(lr_range=(0.001, 0.01), momentum_range=(0.95,0.85))\n",
    "history2 = model.fit(X_train_list[i], y_train_list[i], validation_data = (X_val_list[i], y_val_list[i]), batch_size = num_batch_size, epochs=num_epochs, callbacks=[corr_check, onecyc, early_stopper]) #tensorboard_callback])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "model = load_model(filename_base + '_fold_' + str(i) + '.h5', custom_objects={'AdamW':adamw})\n",
    "preds_val = model.predict(X_val_list[i], verbose=1, batch_size=1024)\n",
    "preds_val = preds_val.reshape(preds_val.shape[0])\n",
    "\n",
    "spearman = stats.spearmanr(preds_val, y_val_list[i])\n",
    "pearson = stats.pearsonr(preds_val, y_val_list[i])\n",
    "    \n",
    "mean_ae = np.mean(abs(preds_val - y_val_list[i]))\n",
    "median_ae = np.median(abs(preds_val - y_val_list[i]))\n",
    "    \n",
    "print('Spearman: ', spearman)\n",
    "print('Pearson: ', pearson)\n",
    "print('Mean abs error: ', mean_ae)\n",
    "print('Median abs error: ', median_ae)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6)) #(1, 1, figsize=(12, 12))\n",
    "    \n",
    "fig.suptitle('12-Lead ECGs (47)')\n",
    "ax.scatter(y_val_list[i], preds_val)\n",
    "ax.set(xlim = [0, 350], xticks = np.arange(0, 350, 50),\n",
    "       ylim = [0, 350], yticks = np.arange(0, 350, 50),\n",
    "       xlabel = 'ECG PR Interval',\n",
    "       ylabel = 'Predicted PR Interval')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_pr_first_1000_40_fold_0.h5 new_model = tf.keras.models.load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = load_model('trained_pr_first_1000_40_fold_0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_value(new_model.optimizer.lr, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epochs = []\n",
    "mses = []\n",
    "val_mses = []\n",
    "mse_hists = []\n",
    "val_mse_hists = []\n",
    "spearmans = []\n",
    "pearsons = []\n",
    "mean_aes = []\n",
    "median_aes = []\n",
    "\n",
    "#for train_index, val_index in group_kfold.split(ecg_X_k, ecg_y_k, ecg_id_k):  #for full 10-fold cross validation\n",
    "\n",
    "for i in range(0,1):\n",
    "    \n",
    "\n",
    "    corr_check = corr_checkpointer(X_val_list[i], y_val_list[i], filepath= ('temp_fold_' + str(i) + '.h5'))\n",
    "    early_stopper = corr_early_stopping(min_epochs=stopping_min_epochs, mode='max', patience=num_patience, verbose=1)  #restore_best_weights not available\n",
    "    #checkpointer = ModelCheckpoint(filename, monitor = corr_check, mode='min', verbose=1, save_best_only = True)\n",
    "    \n",
    "    #reduce_lr = tf.keras.callbacks.ReduceLROnPlateau()\n",
    "    #X_train, X_val = ecg_X_k[train_index], ecg_X_k[val_index]\n",
    "    #y_train, y_val = ecg_y_k[train_index], ecg_y_k[val_index]\n",
    "    \n",
    "    history = model.fit(X_train_list[i], y_train_list[i], validation_data = (X_val_list[i], y_val_list[i]), batch_size = num_batch_size, epochs=num_epochs, callbacks=[corr_check, early_stopper]) #tensorboard_callback])\n",
    "    \n",
    "    best_epoch = len(history.history['val_mean_squared_error']) - num_patience\n",
    "    mse_model = history.history['mean_squared_error'][-(num_patience + 1)]\n",
    "    val_mse_model = history.history['val_mean_squared_error'][-(num_patience + 1)]\n",
    "    mse_hist = history.history['mean_squared_error']\n",
    "    val_mse_hist = history.history['val_mean_squared_error']\n",
    "\n",
    "    model = load_model('temp_fold_' + str(i) + '.h5')\n",
    "    preds_val = model.predict(X_val_list[i], verbose=1, batch_size=1024)\n",
    "    preds_val = preds_val.reshape(preds_val.shape[0])\n",
    "\n",
    "    spearman = stats.spearmanr(preds_val, y_val_list[i])\n",
    "    pearson = stats.pearsonr(preds_val, y_val_list[i])\n",
    "    \n",
    "    mean_ae = np.mean(abs(preds_val - y_val_list[i]))\n",
    "    median_ae = np.median(abs(preds_val - y_val_list[i]))\n",
    "    \n",
    "    \n",
    "    best_epochs.append(best_epoch)\n",
    "    mses.append(mse_model)\n",
    "    val_mses.append(val_mse_model)\n",
    "    mse_hists.append(mse_hist)\n",
    "    val_mse_hists.append(val_mse_hist)\n",
    "    spearmans.append(spearman)\n",
    "    pearsons.append(pearson)\n",
    "    mean_aes.append(mean_ae)\n",
    "    median_aes.append(median_ae)\n",
    "\n",
    "print('MSE: ', mses)\n",
    "print('Val MSE: ', val_mses)\n",
    "print('Spearman: ', spearmans)\n",
    "print('Pearson: ', pearsons)\n",
    "print('Mean abs error: ', mean_aes)\n",
    "print('Median abs error: ', median_aes)\n",
    "\n",
    "\n",
    "print('Mean MSE: ', np.mean(mses))\n",
    "print('Mean val_MSE: ', np.mean(val_mses))\n",
    "print('Mean Spearman r: ', [np.mean(i) for i in zip(*spearmans)])\n",
    "print('Mean Pearson r: ', [np.mean(i) for i in zip(*pearsons)])\n",
    "print('Mean of Mean abs error: ', np.mean(mean_aes))\n",
    "print('Mean of Median abs error: ', np.mean(median_aes))\n",
    "\n",
    "\n",
    "mean_mse = [np.mean(i) for i in zip(*mse_hists)]\n",
    "mean_val_mse = [np.mean(i) for i in zip(*val_mse_hists)]\n",
    "plt.plot(mean_mse[3:])\n",
    "plt.plot(mean_val_mse[3:])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "for j in range(len(mse_hists)):\n",
    "    plt.plot(mse_hists[j])\n",
    "    plt.plot(val_mse_hists[j])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(filename_base + '_fold_' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_value(model.optimizer.lr, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_check = corr_checkpointer(X_val_list[i], y_val_list[i], filepath= (filename_base + '_contfold_' + str(i) + '.h5))\n",
    "early_stopper = corr_early_stopping(min_epochs=stopping_min_epochs, mode='max', patience=num_patience, verbose=1)  #restore_best_weights not available\n",
    "  \n",
    "history = model.fit(X_train_list[i], y_train_list[i], validation_data = (X_val_list[i], y_val_list[i]), batch_size = num_batch_size, epochs=num_epochs, callbacks=[corr_check, early_stopper]) #tensorboard_callback])\n",
    "    \n",
    "best_epoch = len(history.history['val_mean_squared_error']) - num_patience\n",
    "mse_model = history.history['mean_squared_error'][-(num_patience + 1)]\n",
    "val_mse_model = history.history['val_mean_squared_error'][-(num_patience + 1)]\n",
    "mse_hist = history.history['mean_squared_error']\n",
    "val_mse_hist = history.history['val_mean_squared_error']\n",
    "\n",
    "model = load_model(filename_base + '_contfold_' + str(i) + '.h5')\n",
    "preds_val = model.predict(X_val_list[i], verbose=1, batch_size=1024)\n",
    "preds_val = preds_val.reshape(preds_val.shape[0])\n",
    "\n",
    "spearman = stats.spearmanr(preds_val, y_val_list[i])\n",
    "pearson = stats.pearsonr(preds_val, y_val_list[i])\n",
    "    \n",
    "mean_ae = np.mean(abs(preds_val - y_val_list[i]))\n",
    "median_ae = np.median(abs(preds_val - y_val_list[i]))\n",
    "\n",
    "print('MSE: ', mses)\n",
    "print('Val MSE: ', val_mses)\n",
    "print('Spearman: ', spearmans)\n",
    "print('Pearson: ', pearsons)\n",
    "print('Mean abs error: ', mean_aes)\n",
    "print('Median abs error: ', median_aes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also good (3)\n",
    "inputs = Input((in_neurons, 1))\n",
    "\n",
    "#c1 = BatchNormalization() (inputs)\n",
    "\n",
    "c1 = Conv1D(16,3, padding='same', strides=2, use_bias = False, kernel_initializer= 'he_normal') (inputs)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Activation('relu') (c1)\n",
    "c1 = Dropout(0.1) (c1)\n",
    "#c1 = SpatialDropout1D(0.2) (c1)\n",
    "\n",
    "\n",
    "c1 = Conv1D(32,7, padding='same', strides=2, use_bias = False, kernel_initializer= 'he_normal') (c1)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Activation('relu') (c1)\n",
    "c1 = SpatialDropout1D(0.2) (c1)\n",
    "\n",
    "\n",
    "s1 = Conv1D(64, 1, padding='same', strides=2, use_bias=False) (c1)\n",
    "s1 = BatchNormalization() (s1)\n",
    "\n",
    "c2 = WeightNormalization(Conv1D(32,7, padding='same', dilation_rate=2, kernel_initializer= 'he_normal')) (c1)\n",
    "c2 = Activation('relu') (c2)\n",
    "c2 = SpatialDropout1D(0.3) (c2)\n",
    "c2 = WeightNormalization(Conv1D(32,7, padding='same', dilation_rate=4, kernel_initializer= 'he_normal')) (c2)\n",
    "#c2 = Activation('relu') (c2)\n",
    "c2 = Activation('relu') (c1+c2)\n",
    "c2 = SpatialDropout1D(0.3) (c2)\n",
    "c2 = MaxPooling1D(1, strides=2) (c2)\n",
    "\n",
    "#s2 = Conv1D(16, 1, padding='same', use_bias=False) (c2)\n",
    "#s2 = BatchNormalization() (s2)\n",
    "\n",
    "c3 = WeightNormalization(Conv1D(64,7, padding='same', dilation_rate=8, kernel_initializer= 'he_normal')) (c2)\n",
    "c3 = Activation('relu') (c3)\n",
    "c3 = SpatialDropout1D(0.4) (c3)\n",
    "c3 = WeightNormalization(Conv1D(64,7, padding='same', dilation_rate=16, kernel_initializer= 'he_normal')) (c3)\n",
    "c3 = Activation('relu') (s1+c3)\n",
    "c3 = SpatialDropout1D(0.4) (c3)\n",
    "#c3 = MaxPooling1D(1, strides=2) (c3)\n",
    "\n",
    "\n",
    "c3 = WeightNormalization(Conv1D(64,7, padding='same', dilation_rate=2, kernel_initializer= 'he_normal')) (c3)\n",
    "c3 = Activation('relu') (c3)\n",
    "c3 = SpatialDropout1D(0.4) (c3)\n",
    "c3 = WeightNormalization(Conv1D(64,7, padding='same', dilation_rate=4, kernel_initializer= 'he_normal')) (c3)\n",
    "c3 = Activation('relu') (s1+c3)\n",
    "c3 = SpatialDropout1D(0.4) (c3)\n",
    "c3 = MaxPooling1D(1, strides=2) (c3)\n",
    "\n",
    "#c3 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=2, kernel_initializer= 'he_normal')) (c3)\n",
    "#c3 = Activation('relu') (c3)\n",
    "#c3 = SpatialDropout1D(0.3) (c3)\n",
    "#c3 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=4, kernel_initializer= 'he_normal')) (c3)\n",
    "#c3 = Activation('relu') (s1+c3)\n",
    "#c3 = SpatialDropout1D(0.3) (c3)\n",
    "\n",
    "c7 = Conv1D(32, 3, use_bias = False, kernel_initializer= 'he_normal') (c3)\n",
    "c7 = BatchNormalization() (c7)\n",
    "c7 = Activation('relu') (c7)\n",
    "c7 = Dropout(0.5) (c7)\n",
    "c7 = MaxPooling1D(1, strides=2) (c7)\n",
    "\n",
    "#c7 = Conv1D(64, 3, use_bias = False) (c7)\n",
    "#c7 = BatchNormalization() (c7)\n",
    "#c7 = Activation('relu') (c7)\n",
    "\n",
    "#final = GlobalAvgPool1D() (c4)\n",
    "\n",
    "#c6 = Dropout(0.1) (c6)\n",
    "#final = Dense(1, activation='relu') (c7)\n",
    "final = Flatten() (c7)\n",
    "#final = Dropout(0.05) (final)\n",
    "#final = Dense(10, activation='relu') (final)\n",
    "#final = Dropout(0.2) (final)\n",
    "outputs = Dense(1) (final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''inputs = Input((in_neurons, 1))\n",
    "\n",
    "c1 = Conv1D(16,4, padding='same', strides=2, use_bias = False, kernel_initializer= 'he_normal') (inputs)\n",
    "c1 = BatchNormalization() (c1)\n",
    "c1 = Activation('relu') (c1)\n",
    "c1 = Dropout(0.1) (c1)\n",
    "\n",
    "s1 = Conv1D(16, 8, padding='same', strides=4, use_bias=False, kernel_initializer= 'he_normal') (c1)\n",
    "s1 = BatchNormalization() (s1)\n",
    "\n",
    "c2 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=2, kernel_initializer= 'he_normal')) (c1)\n",
    "c2 = Activation('relu') (c2)\n",
    "c2 = SpatialDropout1D(0.1) (c2)\n",
    "c2 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=4, kernel_initializer= 'he_normal')) (c2)\n",
    "#c2 = Activation('relu') (c2)\n",
    "c2 = Activation('relu') (c1+c2)\n",
    "c2 = SpatialDropout1D(0.1) (c2)\n",
    "c2 = MaxPooling1D(pool_size=1, strides=4) (c2)\n",
    "\n",
    "c3 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=2, kernel_initializer= 'he_normal')) (c2)\n",
    "c3 = Activation('relu') (c3)\n",
    "c3 = SpatialDropout1D(0.1) (c3)\n",
    "c3 = WeightNormalization(Conv1D(16,8, padding='same', dilation_rate=4, kernel_initializer= 'he_normal')) (c3)\n",
    "#c3 = Activation('relu') (c3)\n",
    "c3 = Activation('relu') (s1+c3)\n",
    "c3 = SpatialDropout1D(0.1) (c3)\n",
    "c3 = MaxPooling1D(pool_size=1, strides=8) (c3)\n",
    "\n",
    "c7 = Conv1D(1, 3, use_bias = False, kernel_initializer= 'he_normal') (c3)\n",
    "c7 = BatchNormalization() (c7)\n",
    "c7 = Activation('relu') (c7)\n",
    "c7 = Dropout(0.3) (c7)\n",
    "\n",
    "final = Flatten() (c7)\n",
    "\n",
    "outputs = Dense(1) (final)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
